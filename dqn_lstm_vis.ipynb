{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "import argparse\n",
    "import time\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten, Convolution2D, Permute\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "from keras.layers.recurrent import LSTM, GRU\n",
    "import keras.backend as K\n",
    "\n",
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.policy import LinearAnnealedPolicy, BoltzmannQPolicy, EpsGreedyQPolicy\n",
    "from rl.memory import SequentialMemory\n",
    "from rl.core import Processor\n",
    "from rl.callbacks import FileLogger, ModelIntervalCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version:\n",
      "3.6.7 |Anaconda, Inc.| (default, Oct 23 2018, 14:01:38) \n",
      "[GCC 4.2.1 Compatible Clang 4.0.1 (tags/RELEASE_401/final)]\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "from gym_unity.envs import UnityEnv\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"Python version:\")\n",
    "print(sys.version)\n",
    "\n",
    "# check Python version\n",
    "if (sys.version_info[0] < 3):\n",
    "    raise Exception(\"ERROR: ML-Agents Toolkit (v0.3 onwards) requires Python 3\")\n",
    "    \n",
    "INPUT_SHAPE = (80, 80, 3)\n",
    "WINDOW_LENGTH = 4\n",
    "mode = 'train'\n",
    "weights = 'None'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BallVecProcessor(Processor):\n",
    "    def process_observation(self, observation):\n",
    "        assert observation.ndim == 3  # (height, width, channel)\n",
    "        img = np.array(observation)\n",
    "        img = img[24:104,24:104]    \n",
    "        processed_observation = img\n",
    "        return processed_observation.astype('uint8')  # saves storage in experience memory\n",
    "\n",
    "    def process_action(self, action):\n",
    "        action = [np.floor(action/21)*((action-20)/10-1.1), (1-np.floor(action/21))*((action-20)/10+1)]\n",
    "        return action\n",
    "    \n",
    "    def process_info(self, info):\n",
    "        key, value = info.items()\n",
    "        key = value[0]\n",
    "        value = value[1].rewards\n",
    "        info = {key: value}\n",
    "        \n",
    "        \"\"\"Processes the info as obtained from the environment for use in an agent and\n",
    "        returns it.\n",
    "\n",
    "        # Arguments\n",
    "            info (dict): An info as obtained by the environment\n",
    "\n",
    "        # Returns\n",
    "            Info obtained by the environment processed\n",
    "        \"\"\"\n",
    "        return info\n",
    "\n",
    "    def process_reward(self, reward):\n",
    "        return np.clip(reward, -1., 1.)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:mlagents.envs:\n",
      "'Ball3DAcademy' started successfully!\n",
      "Unity Academy name: Ball3DAcademy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: Ball3DBrain\n",
      "        Number of Visual Observations (per agent): 1\n",
      "        Vector Observation space size (per agent): 8\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): [2]\n",
      "        Vector Action descriptions: , \n",
      "INFO:gym_unity:1 agents within environment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<UnityEnv instance>\n"
     ]
    }
   ],
   "source": [
    "env_name = \"mlagents/envs/3DBall_128\"  # Name of the Unity environment binary to launch\n",
    "env = UnityEnv(env_name, worker_id=0, use_visual=True)\n",
    "\n",
    "nb_actions = 42\n",
    "print(str(env))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xinhuili/anaconda3/envs/RLPJ/lib/python3.6/site-packages/ipykernel_launcher.py:13: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (8, 8), activation=\"relu\", strides=(4, 4))`\n",
      "  del sys.path[0]\n",
      "/Users/xinhuili/anaconda3/envs/RLPJ/lib/python3.6/site-packages/ipykernel_launcher.py:14: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (4, 4), activation=\"relu\", strides=(2, 2))`\n",
      "  \n",
      "/Users/xinhuili/anaconda3/envs/RLPJ/lib/python3.6/site-packages/ipykernel_launcher.py:15: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), activation=\"relu\")`\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "permute_1 (Permute)          (None, 4, 80, 80, 3)      0         \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 4, 19, 19, 32)     6176      \n",
      "_________________________________________________________________\n",
      "time_distributed_2 (TimeDist (None, 4, 8, 8, 64)       32832     \n",
      "_________________________________________________________________\n",
      "time_distributed_3 (TimeDist (None, 4, 6, 6, 64)       36928     \n",
      "_________________________________________________________________\n",
      "time_distributed_4 (TimeDist (None, 4, 2304)           0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 512)               5769216   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 42)                21546     \n",
      "=================================================================\n",
      "Total params: 5,866,698\n",
      "Trainable params: 5,866,698\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xinhuili/anaconda3/envs/RLPJ/lib/python3.6/site-packages/ipykernel_launcher.py:18: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"linear\", units=42)`\n"
     ]
    }
   ],
   "source": [
    "# Next, we build a very simple model.\n",
    "input_shape = (WINDOW_LENGTH,) + INPUT_SHAPE\n",
    "model = Sequential()\n",
    "if K.image_dim_ordering() == 'tf':\n",
    "    # (width, height, channels)\n",
    "    model.add(Permute((1, 2, 3, 4), input_shape=input_shape))\n",
    "elif K.image_dim_ordering() == 'th':\n",
    "    # (channels, width, height)\n",
    "    model.add(Permute((1, 2, 3, 4), input_shape=input_shape))\n",
    "else:\n",
    "    raise RuntimeError('Unknown image_dim_ordering.')\n",
    "\n",
    "model.add(TimeDistributed(Convolution2D(32, 8, 8, subsample=(4,4), activation='relu'), input_shape=(input_shape)))\n",
    "model.add(TimeDistributed(Convolution2D(64, 4, 4, subsample=(2,2), activation='relu')))\n",
    "model.add(TimeDistributed(Convolution2D(64, 3, 3, activation='relu')))\n",
    "model.add(TimeDistributed(Flatten()))\n",
    "model.add(LSTM(512,  activation='tanh'))\n",
    "model.add(Dense(output_dim=nb_actions, activation='linear'))\n",
    "    \n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finally, we configure and compile our agent. You can use every built-in Keras optimizer and\n",
    "# even the metrics!\n",
    "processor = BallVecProcessor()\n",
    "memory = SequentialMemory(limit=1000000, window_length=WINDOW_LENGTH)\n",
    "policy = LinearAnnealedPolicy(EpsGreedyQPolicy(), attr='eps', value_max=1., value_min=.01, value_test=.005,\n",
    "                              nb_steps=1000000)\n",
    "\n",
    "dqn = DQNAgent(model=model, nb_actions=nb_actions, policy=policy, memory=memory,\n",
    "               processor=processor, nb_steps_warmup=5000, gamma=.99, target_model_update=10000,\n",
    "               train_interval=1, delta_clip=1.)\n",
    "dqn.compile(Adam(lr=1e-6), metrics=['mae'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = BallVecProcessor()\n",
    "memory = SequentialMemory(limit=1000000, window_length=WINDOW_LENGTH)\n",
    "\n",
    "policy = BoltzmannQPolicy()\n",
    "dqn = DQNAgent(model=model, nb_actions=42, memory=memory, nb_steps_warmup=10000,\n",
    "               target_model_update=1e-2, policy=policy, processor = processor)\n",
    "dqn.compile(Adam(lr=1e-6), metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 1750000 steps ...\n",
      "      20/1750000: episode: 1, duration: 0.735s, episode steps: 20, steps per second: 27, episode reward: 0.900, mean reward: 0.045 [-1.000, 0.100], mean action: -0.090 [-1.000, 0.800], mean observation: 0.002 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "      41/1750000: episode: 2, duration: 0.366s, episode steps: 21, steps per second: 57, episode reward: 1.000, mean reward: 0.048 [-1.000, 0.100], mean action: -0.124 [-1.000, 1.000], mean observation: 0.002 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "      60/1750000: episode: 3, duration: 0.333s, episode steps: 19, steps per second: 57, episode reward: 0.800, mean reward: 0.042 [-1.000, 0.100], mean action: 0.018 [-0.900, 1.000], mean observation: 0.002 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "      80/1750000: episode: 4, duration: 0.400s, episode steps: 20, steps per second: 50, episode reward: 0.900, mean reward: 0.045 [-1.000, 0.100], mean action: -0.033 [-1.000, 0.900], mean observation: 0.002 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "      95/1750000: episode: 5, duration: 0.318s, episode steps: 15, steps per second: 47, episode reward: 0.400, mean reward: 0.027 [-1.000, 0.100], mean action: -0.120 [-1.000, 0.900], mean observation: 0.002 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "     118/1750000: episode: 6, duration: 0.445s, episode steps: 23, steps per second: 52, episode reward: 1.200, mean reward: 0.052 [-1.000, 0.100], mean action: -0.011 [-1.000, 0.700], mean observation: 0.002 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "     159/1750000: episode: 7, duration: 0.844s, episode steps: 41, steps per second: 49, episode reward: 3.000, mean reward: 0.073 [-1.000, 0.100], mean action: 0.012 [-1.000, 1.000], mean observation: 0.003 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "     179/1750000: episode: 8, duration: 0.387s, episode steps: 20, steps per second: 52, episode reward: 0.900, mean reward: 0.045 [-1.000, 0.100], mean action: -0.008 [-1.000, 1.000], mean observation: 0.002 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "     196/1750000: episode: 9, duration: 0.306s, episode steps: 17, steps per second: 56, episode reward: 0.600, mean reward: 0.035 [-1.000, 0.100], mean action: -0.082 [-0.800, 0.800], mean observation: 0.002 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "     223/1750000: episode: 10, duration: 0.551s, episode steps: 27, steps per second: 49, episode reward: 1.600, mean reward: 0.059 [-1.000, 0.100], mean action: -0.009 [-1.000, 0.900], mean observation: 0.004 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "     248/1750000: episode: 11, duration: 0.533s, episode steps: 25, steps per second: 47, episode reward: 1.400, mean reward: 0.056 [-1.000, 0.100], mean action: 0.082 [-0.800, 0.900], mean observation: 0.002 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "     272/1750000: episode: 12, duration: 0.398s, episode steps: 24, steps per second: 60, episode reward: 1.300, mean reward: 0.054 [-1.000, 0.100], mean action: -0.035 [-1.000, 1.000], mean observation: 0.002 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "     303/1750000: episode: 13, duration: 0.510s, episode steps: 31, steps per second: 61, episode reward: 2.000, mean reward: 0.065 [-1.000, 0.100], mean action: -0.031 [-1.000, 0.900], mean observation: 0.002 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "     325/1750000: episode: 14, duration: 0.383s, episode steps: 22, steps per second: 57, episode reward: 1.100, mean reward: 0.050 [-1.000, 0.100], mean action: -0.080 [-1.000, 0.800], mean observation: 0.002 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "     364/1750000: episode: 15, duration: 0.668s, episode steps: 39, steps per second: 58, episode reward: 2.800, mean reward: 0.072 [-1.000, 0.100], mean action: 0.023 [-0.900, 1.000], mean observation: 0.002 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "     382/1750000: episode: 16, duration: 0.291s, episode steps: 18, steps per second: 62, episode reward: 0.700, mean reward: 0.039 [-1.000, 0.100], mean action: -0.061 [-1.000, 0.700], mean observation: 0.002 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "     416/1750000: episode: 17, duration: 0.590s, episode steps: 34, steps per second: 58, episode reward: 2.300, mean reward: 0.068 [-1.000, 0.100], mean action: 0.026 [-1.000, 1.000], mean observation: 0.002 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "     445/1750000: episode: 18, duration: 0.509s, episode steps: 29, steps per second: 57, episode reward: 1.800, mean reward: 0.062 [-1.000, 0.100], mean action: 0.031 [-1.000, 1.000], mean observation: 0.002 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "     479/1750000: episode: 19, duration: 0.557s, episode steps: 34, steps per second: 61, episode reward: 2.300, mean reward: 0.068 [-1.000, 0.100], mean action: 0.022 [-0.800, 1.000], mean observation: 0.002 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "     497/1750000: episode: 20, duration: 0.345s, episode steps: 18, steps per second: 52, episode reward: 0.700, mean reward: 0.039 [-1.000, 0.100], mean action: 0.122 [-1.000, 1.000], mean observation: 0.002 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "     515/1750000: episode: 21, duration: 0.304s, episode steps: 18, steps per second: 59, episode reward: 0.700, mean reward: 0.039 [-1.000, 0.100], mean action: -0.094 [-1.000, 1.000], mean observation: 0.002 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "     562/1750000: episode: 22, duration: 0.793s, episode steps: 47, steps per second: 59, episode reward: 3.600, mean reward: 0.077 [-1.000, 0.100], mean action: 0.010 [-0.900, 1.000], mean observation: 0.002 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "     579/1750000: episode: 23, duration: 0.297s, episode steps: 17, steps per second: 57, episode reward: 0.600, mean reward: 0.035 [-1.000, 0.100], mean action: 0.071 [-0.800, 1.000], mean observation: 0.002 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "     598/1750000: episode: 24, duration: 0.330s, episode steps: 19, steps per second: 58, episode reward: 0.800, mean reward: 0.042 [-1.000, 0.100], mean action: -0.087 [-1.000, 0.900], mean observation: 0.002 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "     616/1750000: episode: 25, duration: 0.322s, episode steps: 18, steps per second: 56, episode reward: 0.700, mean reward: 0.039 [-1.000, 0.100], mean action: -0.019 [-0.900, 0.900], mean observation: 0.002 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "     669/1750000: episode: 26, duration: 0.875s, episode steps: 53, steps per second: 61, episode reward: 4.200, mean reward: 0.079 [-1.000, 0.100], mean action: 0.031 [-1.000, 1.000], mean observation: 0.002 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "     684/1750000: episode: 27, duration: 0.245s, episode steps: 15, steps per second: 61, episode reward: 0.400, mean reward: 0.027 [-1.000, 0.100], mean action: 0.037 [-0.900, 1.000], mean observation: 0.002 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "     711/1750000: episode: 28, duration: 0.490s, episode steps: 27, steps per second: 55, episode reward: 1.600, mean reward: 0.059 [-1.000, 0.100], mean action: -0.035 [-1.000, 0.900], mean observation: 0.002 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "     732/1750000: episode: 29, duration: 0.401s, episode steps: 21, steps per second: 52, episode reward: 1.000, mean reward: 0.048 [-1.000, 0.100], mean action: 0.060 [-1.000, 1.000], mean observation: 0.002 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "     767/1750000: episode: 30, duration: 0.639s, episode steps: 35, steps per second: 55, episode reward: 2.400, mean reward: 0.069 [-1.000, 0.100], mean action: 0.044 [-1.000, 1.000], mean observation: 0.002 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "     791/1750000: episode: 31, duration: 0.458s, episode steps: 24, steps per second: 52, episode reward: 1.300, mean reward: 0.054 [-1.000, 0.100], mean action: 0.002 [-1.000, 0.900], mean observation: 0.002 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     818/1750000: episode: 32, duration: 0.452s, episode steps: 27, steps per second: 60, episode reward: 1.600, mean reward: 0.059 [-1.000, 0.100], mean action: -0.007 [-0.900, 0.900], mean observation: 0.002 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "     835/1750000: episode: 33, duration: 0.286s, episode steps: 17, steps per second: 60, episode reward: 0.600, mean reward: 0.035 [-1.000, 0.100], mean action: -0.091 [-1.000, 0.900], mean observation: 0.002 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "     862/1750000: episode: 34, duration: 0.517s, episode steps: 27, steps per second: 52, episode reward: 1.600, mean reward: 0.059 [-1.000, 0.100], mean action: -0.020 [-0.900, 0.900], mean observation: 0.002 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "     890/1750000: episode: 35, duration: 0.488s, episode steps: 28, steps per second: 57, episode reward: 1.700, mean reward: 0.061 [-1.000, 0.100], mean action: -0.066 [-1.000, 1.000], mean observation: 0.002 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "     912/1750000: episode: 36, duration: 0.387s, episode steps: 22, steps per second: 57, episode reward: 1.100, mean reward: 0.050 [-1.000, 0.100], mean action: -0.014 [-1.000, 0.900], mean observation: 0.002 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "     929/1750000: episode: 37, duration: 0.295s, episode steps: 17, steps per second: 58, episode reward: 0.600, mean reward: 0.035 [-1.000, 0.100], mean action: -0.141 [-1.000, 1.000], mean observation: 0.002 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "     955/1750000: episode: 38, duration: 0.530s, episode steps: 26, steps per second: 49, episode reward: 1.500, mean reward: 0.058 [-1.000, 0.100], mean action: 0.031 [-0.900, 1.000], mean observation: 0.002 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "     971/1750000: episode: 39, duration: 0.325s, episode steps: 16, steps per second: 49, episode reward: 0.500, mean reward: 0.031 [-1.000, 0.100], mean action: 0.069 [-1.000, 1.000], mean observation: 0.002 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "     989/1750000: episode: 40, duration: 0.366s, episode steps: 18, steps per second: 49, episode reward: 0.700, mean reward: 0.039 [-1.000, 0.100], mean action: 0.003 [-0.900, 1.000], mean observation: 0.003 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "    1008/1750000: episode: 41, duration: 0.408s, episode steps: 19, steps per second: 47, episode reward: 0.800, mean reward: 0.042 [-1.000, 0.100], mean action: 0.066 [-1.000, 1.000], mean observation: 0.002 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "    1041/1750000: episode: 42, duration: 0.700s, episode steps: 33, steps per second: 47, episode reward: 2.200, mean reward: 0.067 [-1.000, 0.100], mean action: -0.003 [-1.000, 1.000], mean observation: 0.002 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "    1062/1750000: episode: 43, duration: 0.425s, episode steps: 21, steps per second: 49, episode reward: 1.000, mean reward: 0.048 [-1.000, 0.100], mean action: -0.019 [-0.900, 1.000], mean observation: 0.002 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "    1084/1750000: episode: 44, duration: 0.484s, episode steps: 22, steps per second: 45, episode reward: 1.100, mean reward: 0.050 [-1.000, 0.100], mean action: 0.107 [-0.800, 1.000], mean observation: 0.002 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "    1103/1750000: episode: 45, duration: 0.356s, episode steps: 19, steps per second: 53, episode reward: 0.800, mean reward: 0.042 [-1.000, 0.100], mean action: -0.087 [-0.900, 1.000], mean observation: 0.002 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "    1123/1750000: episode: 46, duration: 0.404s, episode steps: 20, steps per second: 49, episode reward: 0.900, mean reward: 0.045 [-1.000, 0.100], mean action: -0.070 [-1.000, 1.000], mean observation: 0.003 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "    1141/1750000: episode: 47, duration: 0.319s, episode steps: 18, steps per second: 56, episode reward: 0.700, mean reward: 0.039 [-1.000, 0.100], mean action: -0.019 [-0.900, 1.000], mean observation: 0.002 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "    1162/1750000: episode: 48, duration: 0.392s, episode steps: 21, steps per second: 54, episode reward: 1.000, mean reward: 0.048 [-1.000, 0.100], mean action: -0.000 [-1.000, 1.000], mean observation: 0.002 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "    1182/1750000: episode: 49, duration: 0.450s, episode steps: 20, steps per second: 44, episode reward: 0.900, mean reward: 0.045 [-1.000, 0.100], mean action: -0.110 [-1.000, 1.000], mean observation: 0.002 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "    1201/1750000: episode: 50, duration: 0.336s, episode steps: 19, steps per second: 57, episode reward: 0.800, mean reward: 0.042 [-1.000, 0.100], mean action: -0.061 [-0.900, 0.900], mean observation: 0.002 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "    1229/1750000: episode: 51, duration: 0.610s, episode steps: 28, steps per second: 46, episode reward: 1.700, mean reward: 0.061 [-1.000, 0.100], mean action: -0.080 [-1.000, 0.800], mean observation: 0.002 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "    1270/1750000: episode: 52, duration: 3.876s, episode steps: 41, steps per second: 11, episode reward: 3.000, mean reward: 0.073 [-1.000, 0.100], mean action: -0.002 [-1.000, 1.000], mean observation: 0.002 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "    1295/1750000: episode: 53, duration: 17.545s, episode steps: 25, steps per second: 1, episode reward: 1.400, mean reward: 0.056 [-1.000, 0.100], mean action: -0.032 [-1.000, 1.000], mean observation: 0.002 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "    1321/1750000: episode: 54, duration: 29.998s, episode steps: 26, steps per second: 1, episode reward: 1.500, mean reward: 0.058 [-1.000, 0.100], mean action: -0.013 [-1.000, 0.900], mean observation: 0.002 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "    1345/1750000: episode: 55, duration: 36.323s, episode steps: 24, steps per second: 1, episode reward: 1.300, mean reward: 0.054 [-1.000, 0.100], mean action: -0.027 [-0.900, 1.000], mean observation: 0.002 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "    1376/1750000: episode: 56, duration: 14.878s, episode steps: 31, steps per second: 2, episode reward: 2.000, mean reward: 0.065 [-1.000, 0.100], mean action: -0.005 [-1.000, 1.000], mean observation: 0.002 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "    1399/1750000: episode: 57, duration: 11.151s, episode steps: 23, steps per second: 2, episode reward: 1.200, mean reward: 0.052 [-1.000, 0.100], mean action: -0.089 [-1.000, 1.000], mean observation: 0.002 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "    1415/1750000: episode: 58, duration: 3.912s, episode steps: 16, steps per second: 4, episode reward: 0.500, mean reward: 0.031 [-1.000, 0.100], mean action: -0.100 [-0.900, 0.800], mean observation: 0.002 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "    1439/1750000: episode: 59, duration: 14.088s, episode steps: 24, steps per second: 2, episode reward: 1.300, mean reward: 0.054 [-1.000, 0.100], mean action: 0.058 [-1.000, 0.900], mean observation: 0.003 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "    1455/1750000: episode: 60, duration: 4.574s, episode steps: 16, steps per second: 3, episode reward: 0.500, mean reward: 0.031 [-1.000, 0.100], mean action: 0.156 [-0.500, 1.000], mean observation: 0.002 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "    1477/1750000: episode: 61, duration: 4.498s, episode steps: 22, steps per second: 5, episode reward: 1.100, mean reward: 0.050 [-1.000, 0.100], mean action: 0.025 [-0.900, 1.000], mean observation: 0.002 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "    1493/1750000: episode: 62, duration: 6.346s, episode steps: 16, steps per second: 3, episode reward: 0.500, mean reward: 0.031 [-1.000, 0.100], mean action: -0.028 [-1.000, 1.000], mean observation: 0.002 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    1512/1750000: episode: 63, duration: 0.872s, episode steps: 19, steps per second: 22, episode reward: 0.800, mean reward: 0.042 [-1.000, 0.100], mean action: -0.026 [-1.000, 0.700], mean observation: 0.002 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "    1531/1750000: episode: 64, duration: 0.419s, episode steps: 19, steps per second: 45, episode reward: 0.800, mean reward: 0.042 [-1.000, 0.100], mean action: -0.129 [-1.000, 0.800], mean observation: 0.002 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "    1548/1750000: episode: 65, duration: 3.211s, episode steps: 17, steps per second: 5, episode reward: 0.600, mean reward: 0.035 [-1.000, 0.100], mean action: 0.015 [-1.000, 1.000], mean observation: 0.002 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "    1570/1750000: episode: 66, duration: 4.040s, episode steps: 22, steps per second: 5, episode reward: 1.100, mean reward: 0.050 [-1.000, 0.100], mean action: -0.102 [-1.000, 1.000], mean observation: 0.002 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "    1586/1750000: episode: 67, duration: 8.089s, episode steps: 16, steps per second: 2, episode reward: 0.500, mean reward: 0.031 [-1.000, 0.100], mean action: -0.075 [-1.000, 1.000], mean observation: 0.002 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "    1604/1750000: episode: 68, duration: 3.727s, episode steps: 18, steps per second: 5, episode reward: 0.700, mean reward: 0.039 [-1.000, 0.100], mean action: -0.142 [-1.000, 0.900], mean observation: 0.002 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "    1620/1750000: episode: 69, duration: 5.410s, episode steps: 16, steps per second: 3, episode reward: 0.500, mean reward: 0.031 [-1.000, 0.100], mean action: -0.172 [-0.900, 0.900], mean observation: 0.002 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "    1641/1750000: episode: 70, duration: 5.734s, episode steps: 21, steps per second: 4, episode reward: 1.000, mean reward: 0.048 [-1.000, 0.100], mean action: 0.081 [-0.900, 1.000], mean observation: 0.002 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "    1658/1750000: episode: 71, duration: 0.434s, episode steps: 17, steps per second: 39, episode reward: 0.600, mean reward: 0.035 [-1.000, 0.100], mean action: -0.050 [-1.000, 1.000], mean observation: 0.002 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "    1677/1750000: episode: 72, duration: 2.518s, episode steps: 19, steps per second: 8, episode reward: 0.800, mean reward: 0.042 [-1.000, 0.100], mean action: 0.063 [-1.000, 1.000], mean observation: 0.002 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "    1698/1750000: episode: 73, duration: 17.027s, episode steps: 21, steps per second: 1, episode reward: 1.000, mean reward: 0.048 [-1.000, 0.100], mean action: -0.083 [-1.000, 1.000], mean observation: 0.002 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "    1719/1750000: episode: 74, duration: 28.991s, episode steps: 21, steps per second: 1, episode reward: 1.000, mean reward: 0.048 [-1.000, 0.100], mean action: 0.038 [-0.900, 1.000], mean observation: 0.002 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "    1737/1750000: episode: 75, duration: 38.061s, episode steps: 18, steps per second: 0, episode reward: 0.700, mean reward: 0.039 [-1.000, 0.100], mean action: 0.075 [-0.800, 1.000], mean observation: 0.002 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "    1770/1750000: episode: 76, duration: 20.612s, episode steps: 33, steps per second: 2, episode reward: 2.200, mean reward: 0.067 [-1.000, 0.100], mean action: -0.009 [-1.000, 0.900], mean observation: 0.002 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "    1785/1750000: episode: 77, duration: 14.949s, episode steps: 15, steps per second: 1, episode reward: 0.400, mean reward: 0.027 [-1.000, 0.100], mean action: 0.227 [-0.900, 1.000], mean observation: 0.002 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "    1800/1750000: episode: 78, duration: 11.274s, episode steps: 15, steps per second: 1, episode reward: 0.400, mean reward: 0.027 [-1.000, 0.100], mean action: -0.023 [-1.000, 1.000], mean observation: 0.002 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "    1819/1750000: episode: 79, duration: 4.414s, episode steps: 19, steps per second: 4, episode reward: 0.800, mean reward: 0.042 [-1.000, 0.100], mean action: 0.005 [-1.000, 1.000], mean observation: 0.002 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "    1843/1750000: episode: 80, duration: 3.763s, episode steps: 24, steps per second: 6, episode reward: 1.300, mean reward: 0.054 [-1.000, 0.100], mean action: -0.038 [-1.000, 1.000], mean observation: 0.002 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "    1862/1750000: episode: 81, duration: 18.303s, episode steps: 19, steps per second: 1, episode reward: 0.800, mean reward: 0.042 [-1.000, 0.100], mean action: 0.074 [-0.700, 1.000], mean observation: 0.002 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "    1892/1750000: episode: 82, duration: 17.157s, episode steps: 30, steps per second: 2, episode reward: 1.900, mean reward: 0.063 [-1.000, 0.100], mean action: 0.085 [-1.000, 1.000], mean observation: 0.002 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "    1912/1750000: episode: 83, duration: 7.011s, episode steps: 20, steps per second: 3, episode reward: 0.900, mean reward: 0.045 [-1.000, 0.100], mean action: -0.025 [-1.000, 1.000], mean observation: 0.002 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "    1944/1750000: episode: 84, duration: 6.796s, episode steps: 32, steps per second: 5, episode reward: 2.100, mean reward: 0.066 [-1.000, 0.100], mean action: 0.009 [-0.800, 1.000], mean observation: 0.002 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "    1962/1750000: episode: 85, duration: 0.360s, episode steps: 18, steps per second: 50, episode reward: 0.700, mean reward: 0.039 [-1.000, 0.100], mean action: 0.069 [-0.800, 1.000], mean observation: 0.002 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "    1982/1750000: episode: 86, duration: 3.962s, episode steps: 20, steps per second: 5, episode reward: 0.900, mean reward: 0.045 [-1.000, 0.100], mean action: 0.127 [-0.800, 1.000], mean observation: 0.002 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "    2001/1750000: episode: 87, duration: 15.098s, episode steps: 19, steps per second: 1, episode reward: 0.800, mean reward: 0.042 [-1.000, 0.100], mean action: -0.032 [-1.000, 1.000], mean observation: 0.002 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "    2023/1750000: episode: 88, duration: 31.806s, episode steps: 22, steps per second: 1, episode reward: 1.100, mean reward: 0.050 [-1.000, 0.100], mean action: -0.082 [-0.900, 1.000], mean observation: 0.002 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "    2038/1750000: episode: 89, duration: 2.429s, episode steps: 15, steps per second: 6, episode reward: 0.400, mean reward: 0.027 [-1.000, 0.100], mean action: 0.133 [-0.800, 0.900], mean observation: 0.002 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "    2070/1750000: episode: 90, duration: 49.337s, episode steps: 32, steps per second: 1, episode reward: 2.100, mean reward: 0.066 [-1.000, 0.100], mean action: -0.042 [-1.000, 1.000], mean observation: 0.002 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "    2088/1750000: episode: 91, duration: 30.076s, episode steps: 18, steps per second: 1, episode reward: 0.700, mean reward: 0.039 [-1.000, 0.100], mean action: -0.092 [-1.000, 1.000], mean observation: 0.002 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "    2110/1750000: episode: 92, duration: 40.277s, episode steps: 22, steps per second: 1, episode reward: 1.100, mean reward: 0.050 [-1.000, 0.100], mean action: -0.000 [-1.000, 1.000], mean observation: 0.002 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "    2135/1750000: episode: 93, duration: 50.187s, episode steps: 25, steps per second: 0, episode reward: 1.400, mean reward: 0.056 [-1.000, 0.100], mean action: 0.080 [-1.000, 1.000], mean observation: 0.002 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    2152/1750000: episode: 94, duration: 22.411s, episode steps: 17, steps per second: 1, episode reward: 0.600, mean reward: 0.035 [-1.000, 0.100], mean action: 0.112 [-0.800, 1.000], mean observation: 0.003 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "    2195/1750000: episode: 95, duration: 37.010s, episode steps: 43, steps per second: 1, episode reward: 3.200, mean reward: 0.074 [-1.000, 0.100], mean action: 0.008 [-1.000, 1.000], mean observation: 0.002 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "    2217/1750000: episode: 96, duration: 0.684s, episode steps: 22, steps per second: 32, episode reward: 1.100, mean reward: 0.050 [-1.000, 0.100], mean action: 0.077 [-1.000, 1.000], mean observation: 0.002 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "    2262/1750000: episode: 97, duration: 0.746s, episode steps: 45, steps per second: 60, episode reward: 3.400, mean reward: 0.076 [-1.000, 0.100], mean action: -0.051 [-0.900, 1.000], mean observation: 0.002 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "    2283/1750000: episode: 98, duration: 0.329s, episode steps: 21, steps per second: 64, episode reward: 1.000, mean reward: 0.048 [-1.000, 0.100], mean action: -0.100 [-1.000, 1.000], mean observation: 0.003 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "    2308/1750000: episode: 99, duration: 0.441s, episode steps: 25, steps per second: 57, episode reward: 1.400, mean reward: 0.056 [-1.000, 0.100], mean action: -0.050 [-1.000, 0.900], mean observation: 0.002 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "    2340/1750000: episode: 100, duration: 0.537s, episode steps: 32, steps per second: 60, episode reward: 2.100, mean reward: 0.066 [-1.000, 0.100], mean action: -0.053 [-1.000, 1.000], mean observation: 0.002 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "    2358/1750000: episode: 101, duration: 0.337s, episode steps: 18, steps per second: 53, episode reward: 0.700, mean reward: 0.039 [-1.000, 0.100], mean action: 0.017 [-1.000, 1.000], mean observation: 0.002 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "    2392/1750000: episode: 102, duration: 0.546s, episode steps: 34, steps per second: 62, episode reward: 2.300, mean reward: 0.068 [-1.000, 0.100], mean action: -0.076 [-1.000, 0.900], mean observation: 0.002 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "    2411/1750000: episode: 103, duration: 0.336s, episode steps: 19, steps per second: 56, episode reward: 0.800, mean reward: 0.042 [-1.000, 0.100], mean action: 0.087 [-0.800, 1.000], mean observation: 0.002 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "    2429/1750000: episode: 104, duration: 0.305s, episode steps: 18, steps per second: 59, episode reward: 0.700, mean reward: 0.039 [-1.000, 0.100], mean action: 0.067 [-0.800, 1.000], mean observation: 0.002 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "    2456/1750000: episode: 105, duration: 0.480s, episode steps: 27, steps per second: 56, episode reward: 1.600, mean reward: 0.059 [-1.000, 0.100], mean action: -0.041 [-1.000, 0.900], mean observation: 0.002 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "    2481/1750000: episode: 106, duration: 0.404s, episode steps: 25, steps per second: 62, episode reward: 1.400, mean reward: 0.056 [-1.000, 0.100], mean action: 0.014 [-0.700, 0.900], mean observation: 0.002 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "    2496/1750000: episode: 107, duration: 0.261s, episode steps: 15, steps per second: 57, episode reward: 0.400, mean reward: 0.027 [-1.000, 0.100], mean action: -0.113 [-1.000, 1.000], mean observation: 0.003 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "    2532/1750000: episode: 108, duration: 0.618s, episode steps: 36, steps per second: 58, episode reward: 2.500, mean reward: 0.069 [-1.000, 0.100], mean action: -0.004 [-1.000, 1.000], mean observation: 0.002 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "    2547/1750000: episode: 109, duration: 0.235s, episode steps: 15, steps per second: 64, episode reward: 0.400, mean reward: 0.027 [-1.000, 0.100], mean action: -0.060 [-1.000, 1.000], mean observation: 0.002 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "    2566/1750000: episode: 110, duration: 0.337s, episode steps: 19, steps per second: 56, episode reward: 0.800, mean reward: 0.042 [-1.000, 0.100], mean action: 0.032 [-0.900, 0.800], mean observation: 0.002 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "    2587/1750000: episode: 111, duration: 0.362s, episode steps: 21, steps per second: 58, episode reward: 1.000, mean reward: 0.048 [-1.000, 0.100], mean action: 0.090 [-0.900, 1.000], mean observation: 0.002 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "    2613/1750000: episode: 112, duration: 0.445s, episode steps: 26, steps per second: 58, episode reward: 1.500, mean reward: 0.058 [-1.000, 0.100], mean action: -0.027 [-1.000, 1.000], mean observation: 0.002 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "    2633/1750000: episode: 113, duration: 0.335s, episode steps: 20, steps per second: 60, episode reward: 0.900, mean reward: 0.045 [-1.000, 0.100], mean action: -0.003 [-1.000, 1.000], mean observation: 0.002 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "    2668/1750000: episode: 114, duration: 0.611s, episode steps: 35, steps per second: 57, episode reward: 2.400, mean reward: 0.069 [-1.000, 0.100], mean action: -0.001 [-1.000, 0.900], mean observation: 0.002 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "    2687/1750000: episode: 115, duration: 0.332s, episode steps: 19, steps per second: 57, episode reward: 0.800, mean reward: 0.042 [-1.000, 0.100], mean action: 0.003 [-0.800, 0.900], mean observation: 0.002 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "    2708/1750000: episode: 116, duration: 0.373s, episode steps: 21, steps per second: 56, episode reward: 1.000, mean reward: 0.048 [-1.000, 0.100], mean action: 0.067 [-1.000, 0.900], mean observation: 0.002 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "    2733/1750000: episode: 117, duration: 0.422s, episode steps: 25, steps per second: 59, episode reward: 1.400, mean reward: 0.056 [-1.000, 0.100], mean action: -0.044 [-0.900, 0.900], mean observation: 0.002 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "    2753/1750000: episode: 118, duration: 0.361s, episode steps: 20, steps per second: 55, episode reward: 0.900, mean reward: 0.045 [-1.000, 0.100], mean action: -0.040 [-1.000, 0.900], mean observation: 0.002 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "    2774/1750000: episode: 119, duration: 0.348s, episode steps: 21, steps per second: 60, episode reward: 1.000, mean reward: 0.048 [-1.000, 0.100], mean action: 0.038 [-0.800, 0.900], mean observation: 0.002 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "    2792/1750000: episode: 120, duration: 0.294s, episode steps: 18, steps per second: 61, episode reward: 0.700, mean reward: 0.039 [-1.000, 0.100], mean action: -0.019 [-1.000, 1.000], mean observation: 0.002 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "    2815/1750000: episode: 121, duration: 0.371s, episode steps: 23, steps per second: 62, episode reward: 1.200, mean reward: 0.052 [-1.000, 0.100], mean action: 0.028 [-1.000, 1.000], mean observation: 0.003 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "    2835/1750000: episode: 122, duration: 0.350s, episode steps: 20, steps per second: 57, episode reward: 0.900, mean reward: 0.045 [-1.000, 0.100], mean action: 0.055 [-1.000, 1.000], mean observation: 0.002 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "    2860/1750000: episode: 123, duration: 0.417s, episode steps: 25, steps per second: 60, episode reward: 1.400, mean reward: 0.056 [-1.000, 0.100], mean action: -0.010 [-1.000, 1.000], mean observation: 0.002 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "    2881/1750000: episode: 124, duration: 0.356s, episode steps: 21, steps per second: 59, episode reward: 1.000, mean reward: 0.048 [-1.000, 0.100], mean action: 0.060 [-1.000, 1.000], mean observation: 0.002 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    2903/1750000: episode: 125, duration: 0.368s, episode steps: 22, steps per second: 60, episode reward: 1.100, mean reward: 0.050 [-1.000, 0.100], mean action: 0.077 [-0.900, 1.000], mean observation: 0.002 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "    2923/1750000: episode: 126, duration: 0.350s, episode steps: 20, steps per second: 57, episode reward: 0.900, mean reward: 0.045 [-1.000, 0.100], mean action: -0.063 [-1.000, 0.800], mean observation: 0.002 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "    2942/1750000: episode: 127, duration: 0.336s, episode steps: 19, steps per second: 56, episode reward: 0.800, mean reward: 0.042 [-1.000, 0.100], mean action: -0.124 [-1.000, 1.000], mean observation: 0.002 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "    2962/1750000: episode: 128, duration: 0.349s, episode steps: 20, steps per second: 57, episode reward: 0.900, mean reward: 0.045 [-1.000, 0.100], mean action: 0.017 [-1.000, 0.900], mean observation: 0.002 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "    2981/1750000: episode: 129, duration: 0.329s, episode steps: 19, steps per second: 58, episode reward: 0.800, mean reward: 0.042 [-1.000, 0.100], mean action: 0.037 [-1.000, 1.000], mean observation: 0.002 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "    3000/1750000: episode: 130, duration: 0.328s, episode steps: 19, steps per second: 58, episode reward: 0.800, mean reward: 0.042 [-1.000, 0.100], mean action: -0.042 [-1.000, 1.000], mean observation: 0.002 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "    3023/1750000: episode: 131, duration: 0.384s, episode steps: 23, steps per second: 60, episode reward: 1.200, mean reward: 0.052 [-1.000, 0.100], mean action: -0.013 [-1.000, 1.000], mean observation: 0.002 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "    3044/1750000: episode: 132, duration: 0.371s, episode steps: 21, steps per second: 57, episode reward: 1.000, mean reward: 0.048 [-1.000, 0.100], mean action: -0.081 [-1.000, 1.000], mean observation: 0.002 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "    3068/1750000: episode: 133, duration: 0.376s, episode steps: 24, steps per second: 64, episode reward: 1.300, mean reward: 0.054 [-1.000, 0.100], mean action: -0.042 [-1.000, 1.000], mean observation: 0.004 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "    3087/1750000: episode: 134, duration: 0.320s, episode steps: 19, steps per second: 59, episode reward: 0.800, mean reward: 0.042 [-1.000, 0.100], mean action: -0.076 [-0.900, 0.700], mean observation: 0.002 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "    3105/1750000: episode: 135, duration: 0.297s, episode steps: 18, steps per second: 61, episode reward: 0.700, mean reward: 0.039 [-1.000, 0.100], mean action: -0.094 [-1.000, 0.800], mean observation: 0.002 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "    3123/1750000: episode: 136, duration: 0.315s, episode steps: 18, steps per second: 57, episode reward: 0.700, mean reward: 0.039 [-1.000, 0.100], mean action: -0.044 [-1.000, 0.900], mean observation: 0.002 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "    3142/1750000: episode: 137, duration: 0.303s, episode steps: 19, steps per second: 63, episode reward: 0.800, mean reward: 0.042 [-1.000, 0.100], mean action: -0.113 [-1.000, 0.900], mean observation: 0.002 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "    3161/1750000: episode: 138, duration: 0.329s, episode steps: 19, steps per second: 58, episode reward: 0.800, mean reward: 0.042 [-1.000, 0.100], mean action: -0.039 [-1.000, 0.900], mean observation: 0.002 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "    3183/1750000: episode: 139, duration: 0.384s, episode steps: 22, steps per second: 57, episode reward: 1.100, mean reward: 0.050 [-1.000, 0.100], mean action: 0.070 [-0.900, 1.000], mean observation: 0.002 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "    3212/1750000: episode: 140, duration: 0.483s, episode steps: 29, steps per second: 60, episode reward: 1.800, mean reward: 0.062 [-1.000, 0.100], mean action: -0.041 [-1.000, 0.900], mean observation: 0.002 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "    3245/1750000: episode: 141, duration: 0.810s, episode steps: 33, steps per second: 41, episode reward: 2.200, mean reward: 0.067 [-1.000, 0.100], mean action: 0.009 [-1.000, 1.000], mean observation: 0.003 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "    3268/1750000: episode: 142, duration: 0.441s, episode steps: 23, steps per second: 52, episode reward: 1.200, mean reward: 0.052 [-1.000, 0.100], mean action: 0.130 [-1.000, 1.000], mean observation: 0.002 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "    3287/1750000: episode: 143, duration: 0.329s, episode steps: 19, steps per second: 58, episode reward: 0.800, mean reward: 0.042 [-1.000, 0.100], mean action: -0.037 [-1.000, 1.000], mean observation: 0.002 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "    3305/1750000: episode: 144, duration: 0.322s, episode steps: 18, steps per second: 56, episode reward: 0.700, mean reward: 0.039 [-1.000, 0.100], mean action: 0.111 [-0.800, 1.000], mean observation: 0.002 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "    3329/1750000: episode: 145, duration: 0.412s, episode steps: 24, steps per second: 58, episode reward: 1.300, mean reward: 0.054 [-1.000, 0.100], mean action: -0.006 [-0.900, 0.900], mean observation: 0.002 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "    3363/1750000: episode: 146, duration: 0.577s, episode steps: 34, steps per second: 59, episode reward: 2.300, mean reward: 0.068 [-1.000, 0.100], mean action: -0.003 [-1.000, 1.000], mean observation: 0.002 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "    3392/1750000: episode: 147, duration: 0.482s, episode steps: 29, steps per second: 60, episode reward: 1.800, mean reward: 0.062 [-1.000, 0.100], mean action: -0.016 [-0.900, 1.000], mean observation: 0.003 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "    3412/1750000: episode: 148, duration: 0.354s, episode steps: 20, steps per second: 57, episode reward: 0.900, mean reward: 0.045 [-1.000, 0.100], mean action: -0.038 [-1.000, 0.900], mean observation: 0.002 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "    3428/1750000: episode: 149, duration: 0.350s, episode steps: 16, steps per second: 46, episode reward: 0.500, mean reward: 0.031 [-1.000, 0.100], mean action: 0.103 [-1.000, 1.000], mean observation: 0.002 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "    3461/1750000: episode: 150, duration: 0.745s, episode steps: 33, steps per second: 44, episode reward: 2.200, mean reward: 0.067 [-1.000, 0.100], mean action: 0.020 [-1.000, 1.000], mean observation: 0.002 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "    3486/1750000: episode: 151, duration: 0.428s, episode steps: 25, steps per second: 58, episode reward: 1.400, mean reward: 0.056 [-1.000, 0.100], mean action: -0.054 [-0.900, 0.900], mean observation: 0.002 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "    3513/1750000: episode: 152, duration: 0.540s, episode steps: 27, steps per second: 50, episode reward: 1.600, mean reward: 0.059 [-1.000, 0.100], mean action: -0.122 [-1.000, 1.000], mean observation: 0.002 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "    3530/1750000: episode: 153, duration: 0.332s, episode steps: 17, steps per second: 51, episode reward: 0.600, mean reward: 0.035 [-1.000, 0.100], mean action: -0.091 [-1.000, 0.900], mean observation: 0.002 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "    3550/1750000: episode: 154, duration: 0.359s, episode steps: 20, steps per second: 56, episode reward: 0.900, mean reward: 0.045 [-1.000, 0.100], mean action: -0.095 [-1.000, 1.000], mean observation: 0.003 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "    3570/1750000: episode: 155, duration: 0.350s, episode steps: 20, steps per second: 57, episode reward: 0.900, mean reward: 0.045 [-1.000, 0.100], mean action: -0.063 [-1.000, 1.000], mean observation: 0.002 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    3587/1750000: episode: 156, duration: 0.274s, episode steps: 17, steps per second: 62, episode reward: 0.600, mean reward: 0.035 [-1.000, 0.100], mean action: 0.132 [-0.700, 1.000], mean observation: 0.002 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "    3608/1750000: episode: 157, duration: 0.349s, episode steps: 21, steps per second: 60, episode reward: 1.000, mean reward: 0.048 [-1.000, 0.100], mean action: 0.067 [-0.700, 1.000], mean observation: 0.003 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "    3626/1750000: episode: 158, duration: 0.324s, episode steps: 18, steps per second: 56, episode reward: 0.700, mean reward: 0.039 [-1.000, 0.100], mean action: 0.072 [-0.700, 1.000], mean observation: 0.002 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "    3646/1750000: episode: 159, duration: 0.343s, episode steps: 20, steps per second: 58, episode reward: 0.900, mean reward: 0.045 [-1.000, 0.100], mean action: -0.048 [-1.000, 1.000], mean observation: 0.002 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "    3665/1750000: episode: 160, duration: 0.332s, episode steps: 19, steps per second: 57, episode reward: 0.800, mean reward: 0.042 [-1.000, 0.100], mean action: 0.016 [-1.000, 1.000], mean observation: 0.002 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "    3687/1750000: episode: 161, duration: 0.356s, episode steps: 22, steps per second: 62, episode reward: 1.100, mean reward: 0.050 [-1.000, 0.100], mean action: -0.066 [-1.000, 1.000], mean observation: 0.002 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "    3708/1750000: episode: 162, duration: 0.377s, episode steps: 21, steps per second: 56, episode reward: 1.000, mean reward: 0.048 [-1.000, 0.100], mean action: -0.067 [-1.000, 0.900], mean observation: 0.002 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "    3724/1750000: episode: 163, duration: 0.247s, episode steps: 16, steps per second: 65, episode reward: 0.500, mean reward: 0.031 [-1.000, 0.100], mean action: -0.063 [-1.000, 1.000], mean observation: 0.002 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "    3744/1750000: episode: 164, duration: 0.383s, episode steps: 20, steps per second: 52, episode reward: 0.900, mean reward: 0.045 [-1.000, 0.100], mean action: -0.128 [-1.000, 0.600], mean observation: 0.002 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "    3778/1750000: episode: 165, duration: 0.585s, episode steps: 34, steps per second: 58, episode reward: 2.300, mean reward: 0.068 [-1.000, 0.100], mean action: -0.006 [-1.000, 0.900], mean observation: 0.002 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "    3794/1750000: episode: 166, duration: 0.276s, episode steps: 16, steps per second: 58, episode reward: 0.500, mean reward: 0.031 [-1.000, 0.100], mean action: -0.009 [-0.900, 0.900], mean observation: 0.002 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "    3822/1750000: episode: 167, duration: 0.532s, episode steps: 28, steps per second: 53, episode reward: 1.700, mean reward: 0.061 [-1.000, 0.100], mean action: -0.005 [-1.000, 1.000], mean observation: 0.002 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "    3838/1750000: episode: 168, duration: 0.277s, episode steps: 16, steps per second: 58, episode reward: 0.500, mean reward: 0.031 [-1.000, 0.100], mean action: 0.075 [-0.700, 1.000], mean observation: 0.002 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "    3856/1750000: episode: 169, duration: 0.321s, episode steps: 18, steps per second: 56, episode reward: 0.700, mean reward: 0.039 [-1.000, 0.100], mean action: 0.056 [-0.900, 1.000], mean observation: 0.002 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "    3876/1750000: episode: 170, duration: 0.346s, episode steps: 20, steps per second: 58, episode reward: 0.900, mean reward: 0.045 [-1.000, 0.100], mean action: -0.020 [-1.000, 1.000], mean observation: 0.003 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "    3892/1750000: episode: 171, duration: 0.294s, episode steps: 16, steps per second: 54, episode reward: 0.500, mean reward: 0.031 [-1.000, 0.100], mean action: -0.009 [-0.800, 1.000], mean observation: 0.002 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "    3912/1750000: episode: 172, duration: 0.371s, episode steps: 20, steps per second: 54, episode reward: 0.900, mean reward: 0.045 [-1.000, 0.100], mean action: 0.002 [-1.000, 1.000], mean observation: 0.002 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "    3929/1750000: episode: 173, duration: 0.300s, episode steps: 17, steps per second: 57, episode reward: 0.600, mean reward: 0.035 [-1.000, 0.100], mean action: 0.056 [-0.900, 1.000], mean observation: 0.002 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "    3967/1750000: episode: 174, duration: 0.648s, episode steps: 38, steps per second: 59, episode reward: 2.700, mean reward: 0.071 [-1.000, 0.100], mean action: -0.039 [-1.000, 1.000], mean observation: 0.003 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "    3983/1750000: episode: 175, duration: 0.278s, episode steps: 16, steps per second: 58, episode reward: 0.500, mean reward: 0.031 [-1.000, 0.100], mean action: -0.053 [-1.000, 1.000], mean observation: 0.002 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "    4003/1750000: episode: 176, duration: 0.355s, episode steps: 20, steps per second: 56, episode reward: 0.900, mean reward: 0.045 [-1.000, 0.100], mean action: 0.120 [-1.000, 1.000], mean observation: 0.002 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "    4018/1750000: episode: 177, duration: 0.265s, episode steps: 15, steps per second: 57, episode reward: 0.400, mean reward: 0.027 [-1.000, 0.100], mean action: 0.180 [-0.800, 1.000], mean observation: 0.002 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "    4043/1750000: episode: 178, duration: 0.411s, episode steps: 25, steps per second: 61, episode reward: 1.400, mean reward: 0.056 [-1.000, 0.100], mean action: 0.054 [-1.000, 1.000], mean observation: 0.002 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "    4064/1750000: episode: 179, duration: 0.364s, episode steps: 21, steps per second: 58, episode reward: 1.000, mean reward: 0.048 [-1.000, 0.100], mean action: 0.012 [-1.000, 0.900], mean observation: 0.002 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "    4089/1750000: episode: 180, duration: 0.443s, episode steps: 25, steps per second: 56, episode reward: 1.400, mean reward: 0.056 [-1.000, 0.100], mean action: -0.036 [-1.000, 0.900], mean observation: 0.002 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "    4115/1750000: episode: 181, duration: 0.461s, episode steps: 26, steps per second: 56, episode reward: 1.500, mean reward: 0.058 [-1.000, 0.100], mean action: -0.006 [-1.000, 0.900], mean observation: 0.002 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "    4131/1750000: episode: 182, duration: 0.314s, episode steps: 16, steps per second: 51, episode reward: 0.500, mean reward: 0.031 [-1.000, 0.100], mean action: 0.037 [-1.000, 1.000], mean observation: 0.002 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "    4154/1750000: episode: 183, duration: 0.399s, episode steps: 23, steps per second: 58, episode reward: 1.200, mean reward: 0.052 [-1.000, 0.100], mean action: -0.052 [-1.000, 1.000], mean observation: 0.002 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "    4169/1750000: episode: 184, duration: 0.312s, episode steps: 15, steps per second: 48, episode reward: 0.400, mean reward: 0.027 [-1.000, 0.100], mean action: -0.057 [-1.000, 0.900], mean observation: 0.002 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "    4199/1750000: episode: 185, duration: 0.567s, episode steps: 30, steps per second: 53, episode reward: 1.900, mean reward: 0.063 [-1.000, 0.100], mean action: -0.025 [-1.000, 0.900], mean observation: 0.002 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "    4221/1750000: episode: 186, duration: 0.381s, episode steps: 22, steps per second: 58, episode reward: 1.100, mean reward: 0.050 [-1.000, 0.100], mean action: 0.009 [-1.000, 1.000], mean observation: 0.002 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    4245/1750000: episode: 187, duration: 0.415s, episode steps: 24, steps per second: 58, episode reward: 1.300, mean reward: 0.054 [-1.000, 0.100], mean action: 0.046 [-0.900, 1.000], mean observation: 0.002 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "    4270/1750000: episode: 188, duration: 0.452s, episode steps: 25, steps per second: 55, episode reward: 1.400, mean reward: 0.056 [-1.000, 0.100], mean action: 0.028 [-1.000, 0.900], mean observation: 0.002 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "    4295/1750000: episode: 189, duration: 0.512s, episode steps: 25, steps per second: 49, episode reward: 1.400, mean reward: 0.056 [-1.000, 0.100], mean action: 0.036 [-1.000, 1.000], mean observation: 0.002 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "    4317/1750000: episode: 190, duration: 0.394s, episode steps: 22, steps per second: 56, episode reward: 1.100, mean reward: 0.050 [-1.000, 0.100], mean action: 0.043 [-0.800, 1.000], mean observation: 0.002 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "    4340/1750000: episode: 191, duration: 0.403s, episode steps: 23, steps per second: 57, episode reward: 1.200, mean reward: 0.052 [-1.000, 0.100], mean action: 0.085 [-0.900, 0.900], mean observation: 0.002 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "    4361/1750000: episode: 192, duration: 0.364s, episode steps: 21, steps per second: 58, episode reward: 1.000, mean reward: 0.048 [-1.000, 0.100], mean action: 0.090 [-1.000, 1.000], mean observation: 0.002 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "    4384/1750000: episode: 193, duration: 0.433s, episode steps: 23, steps per second: 53, episode reward: 1.200, mean reward: 0.052 [-1.000, 0.100], mean action: 0.050 [-1.000, 1.000], mean observation: 0.002 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "    4402/1750000: episode: 194, duration: 0.296s, episode steps: 18, steps per second: 61, episode reward: 0.700, mean reward: 0.039 [-1.000, 0.100], mean action: -0.092 [-1.000, 0.800], mean observation: 0.002 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "    4418/1750000: episode: 195, duration: 0.284s, episode steps: 16, steps per second: 56, episode reward: 0.500, mean reward: 0.031 [-1.000, 0.100], mean action: -0.003 [-0.900, 0.900], mean observation: 0.002 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "    4438/1750000: episode: 196, duration: 0.400s, episode steps: 20, steps per second: 50, episode reward: 0.900, mean reward: 0.045 [-1.000, 0.100], mean action: -0.035 [-1.000, 1.000], mean observation: 0.002 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "    4462/1750000: episode: 197, duration: 0.520s, episode steps: 24, steps per second: 46, episode reward: 1.300, mean reward: 0.054 [-1.000, 0.100], mean action: 0.002 [-1.000, 1.000], mean observation: 0.002 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "    4481/1750000: episode: 198, duration: 0.397s, episode steps: 19, steps per second: 48, episode reward: 0.800, mean reward: 0.042 [-1.000, 0.100], mean action: 0.018 [-0.800, 1.000], mean observation: 0.002 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "    4500/1750000: episode: 199, duration: 0.357s, episode steps: 19, steps per second: 53, episode reward: 0.800, mean reward: 0.042 [-1.000, 0.100], mean action: 0.084 [-1.000, 1.000], mean observation: 0.002 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "    4517/1750000: episode: 200, duration: 0.323s, episode steps: 17, steps per second: 53, episode reward: 0.600, mean reward: 0.035 [-1.000, 0.100], mean action: -0.076 [-1.000, 1.000], mean observation: 0.002 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "    4548/1750000: episode: 201, duration: 0.594s, episode steps: 31, steps per second: 52, episode reward: 2.000, mean reward: 0.065 [-1.000, 0.100], mean action: -0.061 [-1.000, 1.000], mean observation: 0.002 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "    4572/1750000: episode: 202, duration: 0.487s, episode steps: 24, steps per second: 49, episode reward: 1.300, mean reward: 0.054 [-1.000, 0.100], mean action: 0.056 [-1.000, 1.000], mean observation: 0.002 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "    4594/1750000: episode: 203, duration: 0.448s, episode steps: 22, steps per second: 49, episode reward: 1.100, mean reward: 0.050 [-1.000, 0.100], mean action: 0.059 [-1.000, 1.000], mean observation: 0.002 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "    4621/1750000: episode: 204, duration: 0.582s, episode steps: 27, steps per second: 46, episode reward: 1.600, mean reward: 0.059 [-1.000, 0.100], mean action: -0.004 [-1.000, 0.900], mean observation: 0.002 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "    4650/1750000: episode: 205, duration: 0.500s, episode steps: 29, steps per second: 58, episode reward: 1.800, mean reward: 0.062 [-1.000, 0.100], mean action: 0.017 [-1.000, 1.000], mean observation: 0.002 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "    4673/1750000: episode: 206, duration: 0.449s, episode steps: 23, steps per second: 51, episode reward: 1.200, mean reward: 0.052 [-1.000, 0.100], mean action: 0.011 [-1.000, 0.900], mean observation: 0.002 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "    4698/1750000: episode: 207, duration: 0.535s, episode steps: 25, steps per second: 47, episode reward: 1.400, mean reward: 0.056 [-1.000, 0.100], mean action: -0.012 [-1.000, 1.000], mean observation: 0.002 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "    4718/1750000: episode: 208, duration: 0.421s, episode steps: 20, steps per second: 47, episode reward: 0.900, mean reward: 0.045 [-1.000, 0.100], mean action: 0.002 [-1.000, 1.000], mean observation: 0.002 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "    4758/1750000: episode: 209, duration: 0.689s, episode steps: 40, steps per second: 58, episode reward: 2.900, mean reward: 0.073 [-1.000, 0.100], mean action: -0.041 [-1.000, 1.000], mean observation: 0.002 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "    4781/1750000: episode: 210, duration: 0.402s, episode steps: 23, steps per second: 57, episode reward: 1.200, mean reward: 0.052 [-1.000, 0.100], mean action: 0.059 [-0.800, 1.000], mean observation: 0.002 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "    4804/1750000: episode: 211, duration: 0.410s, episode steps: 23, steps per second: 56, episode reward: 1.200, mean reward: 0.052 [-1.000, 0.100], mean action: 0.002 [-0.800, 1.000], mean observation: 0.003 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "    4828/1750000: episode: 212, duration: 0.480s, episode steps: 24, steps per second: 50, episode reward: 1.300, mean reward: 0.054 [-1.000, 0.100], mean action: 0.002 [-1.000, 0.900], mean observation: 0.003 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "    4852/1750000: episode: 213, duration: 0.425s, episode steps: 24, steps per second: 56, episode reward: 1.300, mean reward: 0.054 [-1.000, 0.100], mean action: 0.008 [-1.000, 1.000], mean observation: 0.002 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "    4880/1750000: episode: 214, duration: 0.441s, episode steps: 28, steps per second: 64, episode reward: 1.700, mean reward: 0.061 [-1.000, 0.100], mean action: -0.123 [-1.000, 1.000], mean observation: 0.002 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "    4899/1750000: episode: 215, duration: 0.340s, episode steps: 19, steps per second: 56, episode reward: 0.800, mean reward: 0.042 [-1.000, 0.100], mean action: -0.063 [-0.900, 1.000], mean observation: 0.002 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "    4919/1750000: episode: 216, duration: 0.394s, episode steps: 20, steps per second: 51, episode reward: 0.900, mean reward: 0.045 [-1.000, 0.100], mean action: -0.050 [-1.000, 0.800], mean observation: 0.002 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "    4939/1750000: episode: 217, duration: 0.381s, episode steps: 20, steps per second: 52, episode reward: 0.900, mean reward: 0.045 [-1.000, 0.100], mean action: -0.125 [-1.000, 1.000], mean observation: 0.002 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    4963/1750000: episode: 218, duration: 0.412s, episode steps: 24, steps per second: 58, episode reward: 1.300, mean reward: 0.054 [-1.000, 0.100], mean action: 0.085 [-1.000, 1.000], mean observation: 0.003 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "    4983/1750000: episode: 219, duration: 0.365s, episode steps: 20, steps per second: 55, episode reward: 0.900, mean reward: 0.045 [-1.000, 0.100], mean action: -0.093 [-0.900, 0.900], mean observation: 0.002 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "    5000/1750000: episode: 220, duration: 0.300s, episode steps: 17, steps per second: 57, episode reward: 0.600, mean reward: 0.035 [-1.000, 0.100], mean action: 0.103 [-1.000, 1.000], mean observation: 0.002 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "    5023/1750000: episode: 221, duration: 0.391s, episode steps: 23, steps per second: 59, episode reward: 1.200, mean reward: 0.052 [-1.000, 0.100], mean action: 0.004 [-1.000, 0.900], mean observation: 0.002 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "    5045/1750000: episode: 222, duration: 0.384s, episode steps: 22, steps per second: 57, episode reward: 1.100, mean reward: 0.050 [-1.000, 0.100], mean action: -0.080 [-1.000, 0.800], mean observation: 0.002 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "    5064/1750000: episode: 223, duration: 0.360s, episode steps: 19, steps per second: 53, episode reward: 0.800, mean reward: 0.042 [-1.000, 0.100], mean action: 0.011 [-0.900, 0.900], mean observation: 0.004 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "    5086/1750000: episode: 224, duration: 0.404s, episode steps: 22, steps per second: 55, episode reward: 1.100, mean reward: 0.050 [-1.000, 0.100], mean action: -0.016 [-1.000, 1.000], mean observation: 0.002 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "    5107/1750000: episode: 225, duration: 0.383s, episode steps: 21, steps per second: 55, episode reward: 1.000, mean reward: 0.048 [-1.000, 0.100], mean action: 0.090 [-1.000, 1.000], mean observation: 0.002 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "    5134/1750000: episode: 226, duration: 0.459s, episode steps: 27, steps per second: 59, episode reward: 1.600, mean reward: 0.059 [-1.000, 0.100], mean action: 0.122 [-1.000, 0.800], mean observation: 0.002 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "    5151/1750000: episode: 227, duration: 0.319s, episode steps: 17, steps per second: 53, episode reward: 0.600, mean reward: 0.035 [-1.000, 0.100], mean action: -0.032 [-0.900, 0.900], mean observation: 0.002 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "    5175/1750000: episode: 228, duration: 0.435s, episode steps: 24, steps per second: 55, episode reward: 1.300, mean reward: 0.054 [-1.000, 0.100], mean action: -0.010 [-1.000, 1.000], mean observation: 0.002 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "    5192/1750000: episode: 229, duration: 0.308s, episode steps: 17, steps per second: 55, episode reward: 0.600, mean reward: 0.035 [-1.000, 0.100], mean action: -0.035 [-0.900, 1.000], mean observation: 0.002 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "    5211/1750000: episode: 230, duration: 0.323s, episode steps: 19, steps per second: 59, episode reward: 0.800, mean reward: 0.042 [-1.000, 0.100], mean action: -0.024 [-1.000, 0.900], mean observation: 0.002 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "    5232/1750000: episode: 231, duration: 0.362s, episode steps: 21, steps per second: 58, episode reward: 1.000, mean reward: 0.048 [-1.000, 0.100], mean action: -0.071 [-1.000, 0.700], mean observation: 0.004 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "    5249/1750000: episode: 232, duration: 0.290s, episode steps: 17, steps per second: 59, episode reward: 0.600, mean reward: 0.035 [-1.000, 0.100], mean action: -0.059 [-1.000, 0.800], mean observation: 0.002 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "    5271/1750000: episode: 233, duration: 0.376s, episode steps: 22, steps per second: 59, episode reward: 1.100, mean reward: 0.050 [-1.000, 0.100], mean action: -0.007 [-1.000, 0.500], mean observation: 0.002 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "    5307/1750000: episode: 234, duration: 0.667s, episode steps: 36, steps per second: 54, episode reward: 2.500, mean reward: 0.069 [-1.000, 0.100], mean action: -0.022 [-0.900, 1.000], mean observation: 0.002 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "    5327/1750000: episode: 235, duration: 0.327s, episode steps: 20, steps per second: 61, episode reward: 0.900, mean reward: 0.045 [-1.000, 0.100], mean action: 0.007 [-1.000, 1.000], mean observation: 0.002 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "    5351/1750000: episode: 236, duration: 0.413s, episode steps: 24, steps per second: 58, episode reward: 1.300, mean reward: 0.054 [-1.000, 0.100], mean action: 0.023 [-0.900, 1.000], mean observation: 0.002 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "    5382/1750000: episode: 237, duration: 0.520s, episode steps: 31, steps per second: 60, episode reward: 2.000, mean reward: 0.065 [-1.000, 0.100], mean action: 0.006 [-0.900, 1.000], mean observation: 0.002 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "    5407/1750000: episode: 238, duration: 0.429s, episode steps: 25, steps per second: 58, episode reward: 1.400, mean reward: 0.056 [-1.000, 0.100], mean action: -0.094 [-1.000, 0.900], mean observation: 0.002 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "    5432/1750000: episode: 239, duration: 0.419s, episode steps: 25, steps per second: 60, episode reward: 1.400, mean reward: 0.056 [-1.000, 0.100], mean action: 0.030 [-1.000, 1.000], mean observation: 0.002 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "    5449/1750000: episode: 240, duration: 0.296s, episode steps: 17, steps per second: 57, episode reward: 0.600, mean reward: 0.035 [-1.000, 0.100], mean action: 0.000 [-1.000, 1.000], mean observation: 0.003 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "    5468/1750000: episode: 241, duration: 0.334s, episode steps: 19, steps per second: 57, episode reward: 0.800, mean reward: 0.042 [-1.000, 0.100], mean action: 0.171 [-0.900, 1.000], mean observation: 0.002 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "    5490/1750000: episode: 242, duration: 0.417s, episode steps: 22, steps per second: 53, episode reward: 1.100, mean reward: 0.050 [-1.000, 0.100], mean action: -0.080 [-1.000, 1.000], mean observation: 0.002 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "    5510/1750000: episode: 243, duration: 0.378s, episode steps: 20, steps per second: 53, episode reward: 0.900, mean reward: 0.045 [-1.000, 0.100], mean action: -0.050 [-1.000, 1.000], mean observation: 0.002 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "    5528/1750000: episode: 244, duration: 0.336s, episode steps: 18, steps per second: 54, episode reward: 0.700, mean reward: 0.039 [-1.000, 0.100], mean action: 0.067 [-1.000, 1.000], mean observation: 0.002 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "    5545/1750000: episode: 245, duration: 0.299s, episode steps: 17, steps per second: 57, episode reward: 0.600, mean reward: 0.035 [-1.000, 0.100], mean action: 0.006 [-1.000, 1.000], mean observation: 0.002 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "    5561/1750000: episode: 246, duration: 0.333s, episode steps: 16, steps per second: 48, episode reward: 0.500, mean reward: 0.031 [-1.000, 0.100], mean action: 0.034 [-1.000, 1.000], mean observation: 0.002 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "    5587/1750000: episode: 247, duration: 0.431s, episode steps: 26, steps per second: 60, episode reward: 1.500, mean reward: 0.058 [-1.000, 0.100], mean action: 0.052 [-1.000, 1.000], mean observation: 0.002 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "    5605/1750000: episode: 248, duration: 0.309s, episode steps: 18, steps per second: 58, episode reward: 0.700, mean reward: 0.039 [-1.000, 0.100], mean action: 0.094 [-0.800, 0.900], mean observation: 0.002 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    5640/1750000: episode: 249, duration: 0.619s, episode steps: 35, steps per second: 57, episode reward: 2.400, mean reward: 0.069 [-1.000, 0.100], mean action: -0.054 [-1.000, 1.000], mean observation: 0.002 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "    5683/1750000: episode: 250, duration: 0.731s, episode steps: 43, steps per second: 59, episode reward: 3.200, mean reward: 0.074 [-1.000, 0.100], mean action: -0.064 [-1.000, 1.000], mean observation: 0.002 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "    5711/1750000: episode: 251, duration: 0.463s, episode steps: 28, steps per second: 60, episode reward: 1.700, mean reward: 0.061 [-1.000, 0.100], mean action: -0.023 [-1.000, 1.000], mean observation: 0.002 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "    5737/1750000: episode: 252, duration: 0.487s, episode steps: 26, steps per second: 53, episode reward: 1.500, mean reward: 0.058 [-1.000, 0.100], mean action: -0.013 [-1.000, 1.000], mean observation: 0.002 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "    5761/1750000: episode: 253, duration: 0.409s, episode steps: 24, steps per second: 59, episode reward: 1.300, mean reward: 0.054 [-1.000, 0.100], mean action: -0.013 [-1.000, 1.000], mean observation: 0.002 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "    5783/1750000: episode: 254, duration: 0.437s, episode steps: 22, steps per second: 50, episode reward: 1.100, mean reward: 0.050 [-1.000, 0.100], mean action: 0.043 [-0.900, 1.000], mean observation: 0.002 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "    5807/1750000: episode: 255, duration: 0.395s, episode steps: 24, steps per second: 61, episode reward: 1.300, mean reward: 0.054 [-1.000, 0.100], mean action: -0.173 [-1.000, 0.700], mean observation: 0.002 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "    5833/1750000: episode: 256, duration: 0.432s, episode steps: 26, steps per second: 60, episode reward: 1.500, mean reward: 0.058 [-1.000, 0.100], mean action: 0.008 [-1.000, 0.900], mean observation: 0.003 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "    5868/1750000: episode: 257, duration: 3.658s, episode steps: 35, steps per second: 10, episode reward: 2.400, mean reward: 0.069 [-1.000, 0.100], mean action: -0.114 [-1.000, 1.000], mean observation: 0.002 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "    5888/1750000: episode: 258, duration: 7.013s, episode steps: 20, steps per second: 3, episode reward: 0.900, mean reward: 0.045 [-1.000, 0.100], mean action: -0.115 [-1.000, 1.000], mean observation: 0.002 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "    5912/1750000: episode: 259, duration: 33.676s, episode steps: 24, steps per second: 1, episode reward: 1.300, mean reward: 0.054 [-1.000, 0.100], mean action: -0.000 [-1.000, 0.900], mean observation: 0.002 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "    5929/1750000: episode: 260, duration: 7.120s, episode steps: 17, steps per second: 2, episode reward: 0.600, mean reward: 0.035 [-1.000, 0.100], mean action: -0.121 [-1.000, 0.800], mean observation: 0.002 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "    5949/1750000: episode: 261, duration: 9.551s, episode steps: 20, steps per second: 2, episode reward: 0.900, mean reward: 0.045 [-1.000, 0.100], mean action: 0.125 [-0.800, 1.000], mean observation: 0.002 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "    5976/1750000: episode: 262, duration: 8.140s, episode steps: 27, steps per second: 3, episode reward: 1.600, mean reward: 0.059 [-1.000, 0.100], mean action: -0.061 [-1.000, 1.000], mean observation: 0.002 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "    6012/1750000: episode: 263, duration: 3.317s, episode steps: 36, steps per second: 11, episode reward: 2.500, mean reward: 0.069 [-1.000, 0.100], mean action: 0.061 [-1.000, 1.000], mean observation: 0.002 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "    6031/1750000: episode: 264, duration: 0.332s, episode steps: 19, steps per second: 57, episode reward: 0.800, mean reward: 0.042 [-1.000, 0.100], mean action: -0.092 [-1.000, 1.000], mean observation: 0.003 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "    6047/1750000: episode: 265, duration: 0.288s, episode steps: 16, steps per second: 56, episode reward: 0.500, mean reward: 0.031 [-1.000, 0.100], mean action: 0.006 [-1.000, 1.000], mean observation: 0.002 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "    6073/1750000: episode: 266, duration: 0.431s, episode steps: 26, steps per second: 60, episode reward: 1.500, mean reward: 0.058 [-1.000, 0.100], mean action: 0.023 [-0.900, 0.900], mean observation: 0.002 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "    6094/1750000: episode: 267, duration: 0.496s, episode steps: 21, steps per second: 42, episode reward: 1.000, mean reward: 0.048 [-1.000, 0.100], mean action: 0.071 [-1.000, 1.000], mean observation: 0.002 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "    6117/1750000: episode: 268, duration: 0.428s, episode steps: 23, steps per second: 54, episode reward: 1.200, mean reward: 0.052 [-1.000, 0.100], mean action: 0.015 [-0.900, 1.000], mean observation: 0.002 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "    6133/1750000: episode: 269, duration: 0.337s, episode steps: 16, steps per second: 47, episode reward: 0.500, mean reward: 0.031 [-1.000, 0.100], mean action: -0.000 [-0.900, 0.900], mean observation: 0.002 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "    6149/1750000: episode: 270, duration: 0.325s, episode steps: 16, steps per second: 49, episode reward: 0.500, mean reward: 0.031 [-1.000, 0.100], mean action: 0.128 [-0.700, 1.000], mean observation: 0.002 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "    6168/1750000: episode: 271, duration: 0.424s, episode steps: 19, steps per second: 45, episode reward: 0.800, mean reward: 0.042 [-1.000, 0.100], mean action: -0.063 [-1.000, 1.000], mean observation: 0.004 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "    6189/1750000: episode: 272, duration: 0.410s, episode steps: 21, steps per second: 51, episode reward: 1.000, mean reward: 0.048 [-1.000, 0.100], mean action: -0.129 [-1.000, 1.000], mean observation: 0.002 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "    6210/1750000: episode: 273, duration: 0.432s, episode steps: 21, steps per second: 49, episode reward: 1.000, mean reward: 0.048 [-1.000, 0.100], mean action: 0.017 [-0.900, 1.000], mean observation: 0.002 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "    6244/1750000: episode: 274, duration: 0.610s, episode steps: 34, steps per second: 56, episode reward: 2.300, mean reward: 0.068 [-1.000, 0.100], mean action: 0.010 [-1.000, 1.000], mean observation: 0.003 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "done, took 819.056 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x120bc8080>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Okay, now it's time to learn something! We visualize the training here for show, but this\n",
    "# slows down training quite a lot. You can always safely abort the training prematurely using\n",
    "# Ctrl + C.\n",
    "#ENV_NAME = '3DBall_vis1'\n",
    "#dqn.load_weights('dqn_{}_weights.h5f'.format(ENV_NAME))\n",
    "dqn.fit(env, nb_steps=1750000, visualize=True, verbose=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 5 episodes ...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-dd8ba5238c9c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Finally, evaluate our algorithm for 5 episodes.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mdqn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_episodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvisualize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/RLPJ/lib/python3.6/site-packages/rl/core.py\u001b[0m in \u001b[0;36mtest\u001b[0;34m(self, env, nb_episodes, action_repetition, callbacks, visualize, nb_max_episode_steps, nb_max_start_steps, start_step_policy, verbose)\u001b[0m\n\u001b[1;32m    307\u001b[0m             \u001b[0;31m# Obtain the initial observation by resetting the environment.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_states\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 309\u001b[0;31m             \u001b[0mobservation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    310\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocessor\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m                 \u001b[0mobservation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocessor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_observation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/RLPJ/lib/python3.6/site-packages/gym_unity/envs/unity_env.py\u001b[0m in \u001b[0;36mreset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     94\u001b[0m             \u001b[0mspace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         \"\"\"\n\u001b[0;32m---> 96\u001b[0;31m         \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_env\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbrain_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m         \u001b[0mn_agents\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_agents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_agents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Downloads/ml-agents-bk/ml-agents/mlagents/envs/environment.py\u001b[0m in \u001b[0;36mreset\u001b[0;34m(self, config, train_mode)\u001b[0m\n\u001b[1;32m    252\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_loaded\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m             outputs = self.communicator.exchange(\n\u001b[0;32m--> 254\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_generate_reset_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_mode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    255\u001b[0m             )\n\u001b[1;32m    256\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Downloads/ml-agents-bk/ml-agents/mlagents/envs/rpc_communicator.py\u001b[0m in \u001b[0;36mexchange\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0mmessage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munity_input\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCopyFrom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munity_to_external\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparent_conn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munity_to_external\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparent_conn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/RLPJ/lib/python3.6/multiprocessing/connection.py\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    248\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_readable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m         \u001b[0mbuf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_recv_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_ForkingPickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetbuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/RLPJ/lib/python3.6/multiprocessing/connection.py\u001b[0m in \u001b[0;36m_recv_bytes\u001b[0;34m(self, maxsize)\u001b[0m\n\u001b[1;32m    405\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_recv_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 407\u001b[0;31m         \u001b[0mbuf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_recv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    408\u001b[0m         \u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstruct\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"!i\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmaxsize\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0msize\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mmaxsize\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/RLPJ/lib/python3.6/multiprocessing/connection.py\u001b[0m in \u001b[0;36m_recv\u001b[0;34m(self, size, read)\u001b[0m\n\u001b[1;32m    377\u001b[0m         \u001b[0mremaining\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mremaining\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 379\u001b[0;31m             \u001b[0mchunk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mremaining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    380\u001b[0m             \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "ENV_NAME = '3DBall_128'\n",
    "# After training is done, we save the final weights.\n",
    "dqn.save_weights('dqn_{}_weights.h5f'.format(ENV_NAME), overwrite=True)\n",
    "\n",
    "# Finally, evaluate our algorithm for 5 episodes.\n",
    "dqn.test(env, nb_episodes=5, visualize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV_NAME = '3DBall_128'\n",
    "if mode == 'train':\n",
    "    # Okay, now it's time to learn something! We capture the interrupt exception so that training\n",
    "    # can be prematurely aborted. Notice that you can the built-in Keras callbacks!\n",
    "    weights_filename = 'dqn_{}_weights.h5f'.format(ENV_NAME)\n",
    "    checkpoint_weights_filename = 'dqn_' + env_name + '_weights_{step}.h5f'\n",
    "    log_filename = 'dqn_{}_log.json'.format(env_name)\n",
    "    callbacks = [ModelIntervalCheckpoint(checkpoint_weights_filename, interval=250000)]\n",
    "    #callbacks += [FileLogger(log_filename, interval=100)]\n",
    "    dqn.fit(env, callbacks=callbacks, nb_steps=1750000, visualize=True)\n",
    "\n",
    "    # After training is done, we save the final weights one more time.\n",
    "    dqn.save_weights(weights_filename, overwrite=True)\n",
    "\n",
    "    # Finally, evaluate our algorithm for 10 episodes.\n",
    "    dqn.test(env, nb_episodes=10, visualize=True)\n",
    "    \n",
    "elif mode == 'test':\n",
    "    weights_filename = 'dqn_{}_weights.h5f'.format(env_name)\n",
    "    if weights:\n",
    "        weights_filename = weights\n",
    "    dqn.load_weights(weights_filename)\n",
    "    dqn.test(env, nb_episodes=10, visualize=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
