{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "import argparse\n",
    "\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Activation, Flatten, Convolution2D, Permute, Input, Concatenate\n",
    "from keras.optimizers import Adam\n",
    "import keras.backend as K\n",
    "\n",
    "from rl.agents.ddpg import DDPGAgent\n",
    "from rl.policy import LinearAnnealedPolicy, BoltzmannQPolicy, EpsGreedyQPolicy\n",
    "from rl.memory import SequentialMemory\n",
    "from rl.core import Processor\n",
    "from rl.callbacks import FileLogger, ModelIntervalCheckpoint\n",
    "from rl.random import OrnsteinUhlenbeckProcess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version:\n",
      "3.6.7 |Anaconda, Inc.| (default, Oct 23 2018, 14:01:38) \n",
      "[GCC 4.2.1 Compatible Clang 4.0.1 (tags/RELEASE_401/final)]\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "from gym_unity.envs import UnityEnv\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"Python version:\")\n",
    "print(sys.version)\n",
    "\n",
    "# check Python version\n",
    "if (sys.version_info[0] < 3):\n",
    "    raise Exception(\"ERROR: ML-Agents Toolkit (v0.3 onwards) requires Python 3\")\n",
    "    \n",
    "INPUT_SHAPE = (128, 128)\n",
    "WINDOW_LENGTH = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BallVecProcessor(Processor):\n",
    "    def process_observation(self, observation):\n",
    "        #print(np.shape(observation))\n",
    "        assert observation.ndim == 3  # (height, width, channel)\n",
    "        img = np.array(observation)\n",
    "        processed_observation = np.mean(img, axis=2)\n",
    "        assert processed_observation.shape == INPUT_SHAPE\n",
    "        #print(np.shape(processed_observation))\n",
    "        return processed_observation.astype('uint8')  # saves storage in experience memory\n",
    "    def process_action(self, action):\n",
    "        return action\n",
    "    def process_info(self, info):\n",
    "        key, value = info.items()\n",
    "        key = 1\n",
    "        value = value[1].rewards\n",
    "        info = {key: value}\n",
    "        return info\n",
    "    def process_reward(self, reward):\n",
    "        return np.clip(reward, -1., 1.) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:mlagents.envs:\n",
      "'Ball3DAcademy' started successfully!\n",
      "Unity Academy name: Ball3DAcademy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: Ball3DBrain\n",
      "        Number of Visual Observations (per agent): 1\n",
      "        Vector Observation space size (per agent): 8\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): [2]\n",
      "        Vector Action descriptions: , \n",
      "INFO:gym_unity:1 agents within environment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<UnityEnv instance>\n"
     ]
    }
   ],
   "source": [
    "env_name = \"mlagents/envs/3DBall_128\"  # Name of the Unity environment binary to launch\n",
    "env = UnityEnv(env_name, worker_id=0, use_visual=True)\n",
    "\n",
    "nb_actions = 2\n",
    "print(str(env))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "permute_1 (Permute)          (None, 128, 128, 1)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 31, 31, 32)        2080      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 31, 31, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 14, 14, 64)        32832     \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 12, 12, 64)        36928     \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 9216)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               4719104   \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 1026      \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 4,791,970\n",
      "Trainable params: 4,791,970\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Next, we build a very simple model.\n",
    "input_shape = (WINDOW_LENGTH,) + INPUT_SHAPE\n",
    "actor = Sequential()\n",
    "if K.image_dim_ordering() == 'tf':\n",
    "    # (width, height, channels)\n",
    "    actor.add(Permute((2, 3, 1), input_shape=input_shape))\n",
    "elif K.image_dim_ordering() == 'th':\n",
    "    # (channels, width, height)\n",
    "    actor.add(Permute((1, 2, 3), input_shape=input_shape))\n",
    "else:\n",
    "    raise RuntimeError('Unknown image_dim_ordering.')\n",
    "    \n",
    "actor.add(Convolution2D(32, (8, 8), strides=(4, 4)))\n",
    "actor.add(Activation('relu'))\n",
    "actor.add(Convolution2D(64, (4, 4), strides=(2, 2)))\n",
    "actor.add(Activation('relu'))\n",
    "actor.add(Convolution2D(64, (3, 3), strides=(1, 1)))\n",
    "actor.add(Flatten())\n",
    "actor.add(Dense(512))\n",
    "actor.add(Activation('relu'))\n",
    "actor.add(Dense(nb_actions))\n",
    "actor.add(Activation('linear'))\n",
    "print(actor.summary()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "observation_input (InputLayer)  (None, 1, 128, 128)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "permute_2 (Permute)             (None, 128, 128, 1)  0           observation_input[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 31, 31, 32)   2080        permute_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 31, 31, 32)   0           conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 14, 14, 64)   32832       activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 14, 14, 64)   0           conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 12, 12, 64)   36928       activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 12, 12, 64)   0           conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "action_input (InputLayer)       (None, 2)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 9216)         0           activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 9218)         0           action_input[0][0]               \n",
      "                                                                 flatten_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 512)          4720128     concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 512)          0           dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 1)            513         activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 1)            0           dense_4[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 4,792,481\n",
      "Trainable params: 4,792,481\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "input_shape1 = (WINDOW_LENGTH,) + INPUT_SHAPE\n",
    "\n",
    "action_input = Input(shape=(nb_actions,), name='action_input')\n",
    "observation_input = Input(shape=input_shape1, name='observation_input')\n",
    "\n",
    "if K.image_dim_ordering() == 'tf':\n",
    "    # (width, height, channels)\n",
    "    observation_x = Permute((2, 3, 1), input_shape=input_shape)(observation_input)\n",
    "elif K.image_dim_ordering() == 'th':\n",
    "    # (channels, width, height)\n",
    "    observation_x = Permute((1, 2, 3), input_shape=input_shape)(observation_input)\n",
    "else:\n",
    "    raise RuntimeError('Unknown image_dim_ordering.')\n",
    "\n",
    "observation_x = Convolution2D(32, (8, 8), strides=(4, 4))(observation_x)\n",
    "observation_x = Activation('relu')(observation_x)\n",
    "observation_x = Convolution2D(64, (4, 4), strides=(2, 2))(observation_x)\n",
    "observation_x = Activation('relu')(observation_x)\n",
    "observation_x = Convolution2D(64, (3, 3), strides=(1, 1))(observation_x)\n",
    "observation_x = Activation('relu')(observation_x)\n",
    "flattened_observation = Flatten()(observation_x)\n",
    "x = Concatenate()([action_input, flattened_observation])\n",
    "x = Dense(512)(x)\n",
    "x = Activation('relu')(x)\n",
    "x = Dense(1)(x)\n",
    "x = Activation('linear')(x)\n",
    "critic = Model(inputs=[action_input, observation_input], outputs=x)\n",
    "print(critic.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = BallVecProcessor()\n",
    "memory = SequentialMemory(limit=100000, window_length=1)\n",
    "random_process = OrnsteinUhlenbeckProcess(size=nb_actions, theta=.15, mu=0., sigma=.3)\n",
    "agent = DDPGAgent(nb_actions=nb_actions, actor=actor, critic=critic, critic_action_input=action_input,\n",
    "                  memory=memory, nb_steps_warmup_critic=100, nb_steps_warmup_actor=100,\n",
    "                  random_process=random_process, gamma=.99, target_model_update=1e-3, processor = processor)\n",
    "agent.compile(Adam(lr=.001, clipnorm=1.), metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 1000 steps ...\n",
      "  25/1000: episode: 1, duration: 0.887s, episode steps: 25, steps per second: 28, episode reward: 1.400, mean reward: 0.056 [-1.000, 0.100], mean action: -0.056 [-0.183, 0.070], mean observation: 0.000 [0.000, 0.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "  55/1000: episode: 2, duration: 0.354s, episode steps: 30, steps per second: 85, episode reward: 1.900, mean reward: 0.063 [-1.000, 0.100], mean action: 0.119 [-0.090, 0.356], mean observation: 0.000 [0.000, 0.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "  75/1000: episode: 3, duration: 0.226s, episode steps: 20, steps per second: 88, episode reward: 0.900, mean reward: 0.045 [-1.000, 0.100], mean action: -0.009 [-0.129, 0.117], mean observation: 0.000 [0.000, 0.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "  96/1000: episode: 4, duration: 0.266s, episode steps: 21, steps per second: 79, episode reward: 1.000, mean reward: 0.048 [-1.000, 0.100], mean action: 0.025 [-0.047, 0.082], mean observation: 0.000 [0.000, 0.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      " 132/1000: episode: 5, duration: 10.842s, episode steps: 36, steps per second: 3, episode reward: 2.500, mean reward: 0.069 [-1.000, 0.100], mean action: -0.019 [-0.109, 0.140], mean observation: 0.000 [0.000, 0.000], loss: 0.025593, mean_absolute_error: 0.098806, mean_q: 0.045069\n",
      " 157/1000: episode: 6, duration: 8.025s, episode steps: 25, steps per second: 3, episode reward: 1.400, mean reward: 0.056 [-1.000, 0.100], mean action: 0.049 [-0.032, 0.153], mean observation: 0.000 [0.000, 0.000], loss: 0.021412, mean_absolute_error: 0.075384, mean_q: 0.064140\n",
      " 180/1000: episode: 7, duration: 7.337s, episode steps: 23, steps per second: 3, episode reward: 1.200, mean reward: 0.052 [-1.000, 0.100], mean action: -0.024 [-0.156, 0.115], mean observation: 0.000 [0.000, 0.000], loss: 0.023709, mean_absolute_error: 0.085297, mean_q: 0.059640\n",
      " 204/1000: episode: 8, duration: 12.612s, episode steps: 24, steps per second: 2, episode reward: 1.300, mean reward: 0.054 [-1.000, 0.100], mean action: 0.048 [-0.160, 0.240], mean observation: 0.000 [0.000, 0.000], loss: 0.023768, mean_absolute_error: 0.083586, mean_q: 0.063108\n",
      " 226/1000: episode: 9, duration: 7.482s, episode steps: 22, steps per second: 3, episode reward: 1.100, mean reward: 0.050 [-1.000, 0.100], mean action: 0.029 [-0.148, 0.260], mean observation: 0.000 [0.000, 0.000], loss: 0.017718, mean_absolute_error: 0.060295, mean_q: 0.078121\n",
      " 255/1000: episode: 10, duration: 11.626s, episode steps: 29, steps per second: 2, episode reward: 1.800, mean reward: 0.062 [-1.000, 0.100], mean action: -0.027 [-0.133, 0.045], mean observation: 0.000 [0.000, 0.000], loss: 0.021403, mean_absolute_error: 0.076507, mean_q: 0.069819\n",
      " 273/1000: episode: 11, duration: 6.149s, episode steps: 18, steps per second: 3, episode reward: 0.700, mean reward: 0.039 [-1.000, 0.100], mean action: -0.120 [-0.317, 0.100], mean observation: 0.000 [0.000, 0.000], loss: 0.020895, mean_absolute_error: 0.084610, mean_q: 0.061561\n",
      " 302/1000: episode: 12, duration: 9.338s, episode steps: 29, steps per second: 3, episode reward: 1.800, mean reward: 0.062 [-1.000, 0.100], mean action: 0.003 [-0.102, 0.111], mean observation: 0.000 [0.000, 0.000], loss: 0.021235, mean_absolute_error: 0.073354, mean_q: 0.076354\n",
      " 324/1000: episode: 13, duration: 7.942s, episode steps: 22, steps per second: 3, episode reward: 1.100, mean reward: 0.050 [-1.000, 0.100], mean action: -0.056 [-0.220, 0.128], mean observation: 0.000 [0.000, 0.000], loss: 0.019572, mean_absolute_error: 0.069532, mean_q: 0.079294\n",
      " 350/1000: episode: 14, duration: 8.738s, episode steps: 26, steps per second: 3, episode reward: 1.500, mean reward: 0.058 [-1.000, 0.100], mean action: 0.056 [-0.105, 0.226], mean observation: 0.000 [0.000, 0.000], loss: 0.025075, mean_absolute_error: 0.094032, mean_q: 0.064986\n",
      " 395/1000: episode: 15, duration: 14.367s, episode steps: 45, steps per second: 3, episode reward: 3.400, mean reward: 0.076 [-1.000, 0.100], mean action: 0.010 [-0.107, 0.197], mean observation: 0.000 [0.000, 0.000], loss: 0.019459, mean_absolute_error: 0.070242, mean_q: 0.082920\n",
      " 414/1000: episode: 16, duration: 6.441s, episode steps: 19, steps per second: 3, episode reward: 0.800, mean reward: 0.042 [-1.000, 0.100], mean action: 0.092 [-0.073, 0.256], mean observation: 0.000 [0.000, 0.000], loss: 0.025091, mean_absolute_error: 0.087656, mean_q: 0.077808\n",
      " 432/1000: episode: 17, duration: 5.621s, episode steps: 18, steps per second: 3, episode reward: 0.700, mean reward: 0.039 [-1.000, 0.100], mean action: 0.111 [-0.053, 0.313], mean observation: 0.000 [0.000, 0.000], loss: 0.019425, mean_absolute_error: 0.058778, mean_q: 0.099723\n",
      " 454/1000: episode: 18, duration: 6.972s, episode steps: 22, steps per second: 3, episode reward: 1.100, mean reward: 0.050 [-1.000, 0.100], mean action: 0.114 [0.001, 0.203], mean observation: 0.000 [0.000, 0.000], loss: 0.023855, mean_absolute_error: 0.094530, mean_q: 0.071307\n",
      " 474/1000: episode: 19, duration: 6.394s, episode steps: 20, steps per second: 3, episode reward: 0.900, mean reward: 0.045 [-1.000, 0.100], mean action: 0.019 [-0.102, 0.113], mean observation: 0.000 [0.000, 0.000], loss: 0.021219, mean_absolute_error: 0.079250, mean_q: 0.084642\n",
      " 493/1000: episode: 20, duration: 6.100s, episode steps: 19, steps per second: 3, episode reward: 0.800, mean reward: 0.042 [-1.000, 0.100], mean action: -0.011 [-0.156, 0.126], mean observation: 0.000 [0.000, 0.000], loss: 0.029471, mean_absolute_error: 0.093769, mean_q: 0.086352\n",
      " 514/1000: episode: 21, duration: 6.863s, episode steps: 21, steps per second: 3, episode reward: 1.000, mean reward: 0.048 [-1.000, 0.100], mean action: 0.082 [-0.057, 0.185], mean observation: 0.000 [0.000, 0.000], loss: 0.027946, mean_absolute_error: 0.100202, mean_q: 0.078020\n",
      " 537/1000: episode: 22, duration: 7.328s, episode steps: 23, steps per second: 3, episode reward: 1.200, mean reward: 0.052 [-1.000, 0.100], mean action: 0.026 [-0.374, 0.267], mean observation: 0.000 [0.000, 1.000], loss: 0.017525, mean_absolute_error: 0.063684, mean_q: 0.108605\n",
      " 560/1000: episode: 23, duration: 7.239s, episode steps: 23, steps per second: 3, episode reward: 1.200, mean reward: 0.052 [-1.000, 0.100], mean action: 0.109 [-0.307, 0.470], mean observation: 0.000 [0.000, 0.000], loss: 0.024143, mean_absolute_error: 0.088418, mean_q: 0.087811\n",
      " 580/1000: episode: 24, duration: 6.347s, episode steps: 20, steps per second: 3, episode reward: 0.900, mean reward: 0.045 [-1.000, 0.100], mean action: 0.070 [-0.085, 0.269], mean observation: 0.000 [0.000, 0.000], loss: 0.029593, mean_absolute_error: 0.105125, mean_q: 0.081774\n",
      " 610/1000: episode: 25, duration: 10.102s, episode steps: 30, steps per second: 3, episode reward: 1.900, mean reward: 0.063 [-1.000, 0.100], mean action: 0.008 [-0.057, 0.076], mean observation: 0.000 [0.000, 0.000], loss: 0.024787, mean_absolute_error: 0.088196, mean_q: 0.093279\n",
      " 635/1000: episode: 26, duration: 8.059s, episode steps: 25, steps per second: 3, episode reward: 1.400, mean reward: 0.056 [-1.000, 0.100], mean action: 0.067 [-0.116, 0.316], mean observation: 0.000 [0.000, 0.000], loss: 0.024997, mean_absolute_error: 0.094542, mean_q: 0.089434\n",
      " 651/1000: episode: 27, duration: 5.246s, episode steps: 16, steps per second: 3, episode reward: 0.500, mean reward: 0.031 [-1.000, 0.100], mean action: 0.028 [-0.221, 0.268], mean observation: 0.000 [0.000, 0.000], loss: 0.022140, mean_absolute_error: 0.067804, mean_q: 0.115097\n",
      " 671/1000: episode: 28, duration: 6.743s, episode steps: 20, steps per second: 3, episode reward: 0.900, mean reward: 0.045 [-1.000, 0.100], mean action: 0.113 [-0.002, 0.186], mean observation: 0.000 [0.000, 0.000], loss: 0.027869, mean_absolute_error: 0.103456, mean_q: 0.088939\n",
      " 700/1000: episode: 29, duration: 9.205s, episode steps: 29, steps per second: 3, episode reward: 1.800, mean reward: 0.062 [-1.000, 0.100], mean action: 0.024 [-0.166, 0.196], mean observation: 0.000 [0.000, 0.000], loss: 0.030551, mean_absolute_error: 0.108424, mean_q: 0.090708\n",
      " 720/1000: episode: 30, duration: 6.322s, episode steps: 20, steps per second: 3, episode reward: 0.900, mean reward: 0.045 [-1.000, 0.100], mean action: 0.047 [-0.196, 0.196], mean observation: 0.000 [0.000, 0.000], loss: 0.023287, mean_absolute_error: 0.075860, mean_q: 0.114760\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 742/1000: episode: 31, duration: 6.967s, episode steps: 22, steps per second: 3, episode reward: 1.100, mean reward: 0.050 [-1.000, 0.100], mean action: 0.056 [-0.342, 0.458], mean observation: 0.000 [0.000, 0.000], loss: 0.028599, mean_absolute_error: 0.111199, mean_q: 0.087859\n",
      " 760/1000: episode: 32, duration: 5.700s, episode steps: 18, steps per second: 3, episode reward: 0.700, mean reward: 0.039 [-1.000, 0.100], mean action: 0.144 [-0.007, 0.280], mean observation: 0.000 [0.000, 0.000], loss: 0.017872, mean_absolute_error: 0.051970, mean_q: 0.141372\n",
      " 781/1000: episode: 33, duration: 6.710s, episode steps: 21, steps per second: 3, episode reward: 1.000, mean reward: 0.048 [-1.000, 0.100], mean action: 0.047 [-0.114, 0.235], mean observation: 0.000 [0.000, 0.000], loss: 0.022071, mean_absolute_error: 0.085105, mean_q: 0.108696\n",
      " 798/1000: episode: 34, duration: 5.468s, episode steps: 17, steps per second: 3, episode reward: 0.600, mean reward: 0.035 [-1.000, 0.100], mean action: 0.050 [-0.072, 0.177], mean observation: 0.000 [0.000, 0.000], loss: 0.021304, mean_absolute_error: 0.072697, mean_q: 0.122867\n",
      " 820/1000: episode: 35, duration: 6.936s, episode steps: 22, steps per second: 3, episode reward: 1.100, mean reward: 0.050 [-1.000, 0.100], mean action: 0.075 [-0.110, 0.275], mean observation: 0.000 [0.000, 0.000], loss: 0.024111, mean_absolute_error: 0.078687, mean_q: 0.123443\n",
      " 841/1000: episode: 36, duration: 6.730s, episode steps: 21, steps per second: 3, episode reward: 1.000, mean reward: 0.048 [-1.000, 0.100], mean action: 0.065 [-0.121, 0.242], mean observation: 0.000 [0.000, 0.000], loss: 0.024529, mean_absolute_error: 0.077293, mean_q: 0.128018\n",
      " 857/1000: episode: 37, duration: 5.146s, episode steps: 16, steps per second: 3, episode reward: 0.500, mean reward: 0.031 [-1.000, 0.100], mean action: 0.116 [-0.064, 0.296], mean observation: 0.000 [0.000, 0.000], loss: 0.020511, mean_absolute_error: 0.080131, mean_q: 0.119555\n",
      " 874/1000: episode: 38, duration: 5.556s, episode steps: 17, steps per second: 3, episode reward: 0.600, mean reward: 0.035 [-1.000, 0.100], mean action: 0.057 [-0.094, 0.169], mean observation: 0.000 [0.000, 0.000], loss: 0.017738, mean_absolute_error: 0.056012, mean_q: 0.141923\n",
      " 890/1000: episode: 39, duration: 5.172s, episode steps: 16, steps per second: 3, episode reward: 0.500, mean reward: 0.031 [-1.000, 0.100], mean action: 0.064 [-0.150, 0.228], mean observation: 0.000 [0.000, 0.000], loss: 0.021677, mean_absolute_error: 0.079105, mean_q: 0.126323\n",
      " 907/1000: episode: 40, duration: 5.433s, episode steps: 17, steps per second: 3, episode reward: 0.600, mean reward: 0.035 [-1.000, 0.100], mean action: -0.012 [-0.184, 0.194], mean observation: 0.000 [0.000, 0.000], loss: 0.024219, mean_absolute_error: 0.078614, mean_q: 0.132997\n",
      " 923/1000: episode: 41, duration: 5.133s, episode steps: 16, steps per second: 3, episode reward: 0.500, mean reward: 0.031 [-1.000, 0.100], mean action: 0.052 [-0.170, 0.216], mean observation: 0.000 [0.000, 0.000], loss: 0.024657, mean_absolute_error: 0.084249, mean_q: 0.129386\n",
      " 944/1000: episode: 42, duration: 6.622s, episode steps: 21, steps per second: 3, episode reward: 1.000, mean reward: 0.048 [-1.000, 0.100], mean action: -0.036 [-0.158, 0.154], mean observation: 0.000 [0.000, 0.000], loss: 0.021110, mean_absolute_error: 0.073556, mean_q: 0.135978\n",
      " 963/1000: episode: 43, duration: 6.153s, episode steps: 19, steps per second: 3, episode reward: 0.800, mean reward: 0.042 [-1.000, 0.100], mean action: 0.201 [0.043, 0.326], mean observation: 0.000 [0.000, 0.000], loss: 0.035236, mean_absolute_error: 0.113213, mean_q: 0.120774\n",
      " 978/1000: episode: 44, duration: 4.903s, episode steps: 15, steps per second: 3, episode reward: 0.400, mean reward: 0.027 [-1.000, 0.100], mean action: 0.097 [-0.134, 0.342], mean observation: 0.000 [0.000, 0.000], loss: 0.031078, mean_absolute_error: 0.125139, mean_q: 0.100613\n",
      " 999/1000: episode: 45, duration: 6.812s, episode steps: 21, steps per second: 3, episode reward: 1.000, mean reward: 0.048 [-1.000, 0.100], mean action: -0.054 [-0.168, 0.137], mean observation: 0.000 [0.000, 0.000], loss: 0.019905, mean_absolute_error: 0.064271, mean_q: 0.148357\n",
      "done, took 300.987 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x109d93898>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.fit(env, nb_steps=1750000, visualize=True, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 5 episodes ...\n",
      "Episode 1: reward: 1.500, steps: 26\n",
      "Episode 2: reward: 1.000, steps: 21\n",
      "Episode 3: reward: 1.200, steps: 23\n",
      "Episode 4: reward: 1.000, steps: 21\n",
      "Episode 5: reward: 1.000, steps: 21\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x102598978>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.test(env, nb_episodes=5, visualize=True, nb_max_episode_steps=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After training is done, we save the final weights.\n",
    "ENV_NAME = '3DBall_128'\n",
    "agent.save_weights('ddpg_{}_vis_weights.h5f'.format(ENV_NAME), overwrite=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
