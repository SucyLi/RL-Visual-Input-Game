{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "import argparse\n",
    "\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Activation, Flatten, Convolution2D, Permute, Input, Concatenate\n",
    "from keras.optimizers import Adam\n",
    "import keras.backend as K\n",
    "\n",
    "from rl.agents.ddpg import DDPGAgent\n",
    "from rl.policy import LinearAnnealedPolicy, BoltzmannQPolicy, EpsGreedyQPolicy\n",
    "from rl.memory import SequentialMemory\n",
    "from rl.core import Processor\n",
    "from rl.callbacks import FileLogger, ModelIntervalCheckpoint\n",
    "from rl.random import OrnsteinUhlenbeckProcess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version:\n",
      "3.6.7 |Anaconda, Inc.| (default, Oct 23 2018, 14:01:38) \n",
      "[GCC 4.2.1 Compatible Clang 4.0.1 (tags/RELEASE_401/final)]\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "from gym_unity.envs import UnityEnv\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"Python version:\")\n",
    "print(sys.version)\n",
    "\n",
    "# check Python version\n",
    "if (sys.version_info[0] < 3):\n",
    "    raise Exception(\"ERROR: ML-Agents Toolkit (v0.3 onwards) requires Python 3\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BallVecProcessor(Processor):\n",
    "    def process_action(self, action):\n",
    "        #print(action)\n",
    "        return action\n",
    "    def process_info(self, info):\n",
    "        key, value = info.items()\n",
    "        #print(key)\n",
    "        #print(value)\n",
    "        #print(value[1].rewards)\n",
    "        key = 1\n",
    "        value = value[1].rewards\n",
    "        info = {key: value}\n",
    "        return info\n",
    "    def process_reward(self, reward):\n",
    "        return np.clip(reward, -1., 1.)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:mlagents.envs:\n",
      "'Ball3DAcademy' started successfully!\n",
      "Unity Academy name: Ball3DAcademy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: Ball3DBrain\n",
      "        Number of Visual Observations (per agent): 1\n",
      "        Vector Observation space size (per agent): 8\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): [2]\n",
      "        Vector Action descriptions: , \n",
      "INFO:gym_unity:1 agents within environment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<UnityEnv instance>\n"
     ]
    }
   ],
   "source": [
    "env_name = \"mlagents/envs/3DBall_128\"  # Name of the Unity environment binary to launch\n",
    "env = UnityEnv(env_name, worker_id=0, use_visual=False)\n",
    "\n",
    "nb_actions = 2\n",
    "print(str(env))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_1 (Flatten)          (None, 8)                 0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 16)                144       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 2)                 34        \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 722\n",
      "Trainable params: 722\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "actor = Sequential()\n",
    "actor.add(Flatten(input_shape=(1,) + env.observation_space.shape))\n",
    "actor.add(Dense(16))\n",
    "actor.add(Activation('relu'))\n",
    "actor.add(Dense(16))\n",
    "actor.add(Activation('relu'))\n",
    "actor.add(Dense(16))\n",
    "actor.add(Activation('relu'))\n",
    "actor.add(Dense(nb_actions))\n",
    "actor.add(Activation('linear'))\n",
    "print(actor.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "observation_input (InputLayer)  (None, 1, 8)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "action_input (InputLayer)       (None, 2)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 8)            0           observation_input[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 10)           0           action_input[0][0]               \n",
      "                                                                 flatten_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 32)           352         concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 32)           0           dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 32)           1056        activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 32)           0           dense_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 32)           1056        activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 32)           0           dense_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 1)            33          activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 1)            0           dense_8[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 2,497\n",
      "Trainable params: 2,497\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "action_input = Input(shape=(nb_actions,), name='action_input')\n",
    "observation_input = Input(shape=(1,) + env.observation_space.shape, name='observation_input')\n",
    "flattened_observation = Flatten()(observation_input)\n",
    "x = Concatenate()([action_input, flattened_observation])\n",
    "x = Dense(32)(x)\n",
    "x = Activation('relu')(x)\n",
    "x = Dense(32)(x)\n",
    "x = Activation('relu')(x)\n",
    "x = Dense(32)(x)\n",
    "x = Activation('relu')(x)\n",
    "x = Dense(1)(x)\n",
    "x = Activation('linear')(x)\n",
    "critic = Model(inputs=[action_input, observation_input], outputs=x)\n",
    "print(critic.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = BallVecProcessor()\n",
    "memory = SequentialMemory(limit=100000, window_length=1)\n",
    "random_process = OrnsteinUhlenbeckProcess(size=nb_actions, theta=.15, mu=0., sigma=.3)\n",
    "agent = DDPGAgent(nb_actions=nb_actions, actor=actor, critic=critic, critic_action_input=action_input,\n",
    "                  memory=memory, nb_steps_warmup_critic=100, nb_steps_warmup_actor=100,\n",
    "                  random_process=random_process, gamma=.99, target_model_update=1e-3, processor = processor)\n",
    "agent.compile(Adam(lr=.001, clipnorm=1.), metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 50000 steps ...\n",
      "    19/50000: episode: 1, duration: 0.563s, episode steps: 19, steps per second: 34, episode reward: 0.800, mean reward: 0.042 [-1.000, 0.100], mean action: -0.074 [-0.965, 0.706], mean observation: 0.189 [-6.867, 4.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "    36/50000: episode: 2, duration: 0.172s, episode steps: 17, steps per second: 99, episode reward: 0.600, mean reward: 0.035 [-1.000, 0.100], mean action: -0.085 [-1.028, 0.495], mean observation: 0.120 [-7.848, 4.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "    56/50000: episode: 3, duration: 0.198s, episode steps: 20, steps per second: 101, episode reward: 0.900, mean reward: 0.045 [-1.000, 0.100], mean action: -0.068 [-0.960, 0.323], mean observation: 0.153 [-7.848, 4.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "    72/50000: episode: 4, duration: 0.162s, episode steps: 16, steps per second: 99, episode reward: 0.500, mean reward: 0.031 [-1.000, 0.100], mean action: -0.102 [-1.181, 0.698], mean observation: 0.178 [-7.848, 4.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "    86/50000: episode: 5, duration: 0.169s, episode steps: 14, steps per second: 83, episode reward: 0.300, mean reward: 0.021 [-1.000, 0.100], mean action: -0.087 [-1.422, 0.567], mean observation: 0.116 [-8.829, 4.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "   101/50000: episode: 6, duration: 0.952s, episode steps: 15, steps per second: 16, episode reward: 0.400, mean reward: 0.027 [-1.000, 0.100], mean action: -0.095 [-1.225, 0.775], mean observation: 0.149 [-7.848, 4.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "   122/50000: episode: 7, duration: 0.286s, episode steps: 21, steps per second: 73, episode reward: 1.000, mean reward: 0.048 [-1.000, 0.100], mean action: -0.131 [-0.814, 0.348], mean observation: 0.165 [-6.867, 4.000], loss: 0.027709, mean_absolute_error: 0.129399, mean_q: -0.350141\n",
      "   140/50000: episode: 8, duration: 0.231s, episode steps: 18, steps per second: 78, episode reward: 0.700, mean reward: 0.039 [-1.000, 0.100], mean action: -0.306 [-1.514, 0.394], mean observation: 0.167 [-7.848, 4.000], loss: 0.024981, mean_absolute_error: 0.123622, mean_q: -0.311976\n",
      "   154/50000: episode: 9, duration: 0.169s, episode steps: 14, steps per second: 83, episode reward: 0.300, mean reward: 0.021 [-1.000, 0.100], mean action: -0.689 [-1.884, 0.338], mean observation: 0.052 [-7.848, 4.000], loss: 0.024044, mean_absolute_error: 0.119575, mean_q: -0.335772\n",
      "   174/50000: episode: 10, duration: 0.249s, episode steps: 20, steps per second: 80, episode reward: 0.900, mean reward: 0.045 [-1.000, 0.100], mean action: -0.734 [-2.048, 0.564], mean observation: 0.277 [-6.867, 4.000], loss: 0.025145, mean_absolute_error: 0.127780, mean_q: -0.320080\n",
      "   190/50000: episode: 11, duration: 0.202s, episode steps: 16, steps per second: 79, episode reward: 0.500, mean reward: 0.031 [-1.000, 0.100], mean action: -0.894 [-2.435, 0.458], mean observation: -0.009 [-7.848, 4.114], loss: 0.022966, mean_absolute_error: 0.133574, mean_q: -0.337324\n",
      "   206/50000: episode: 12, duration: 0.208s, episode steps: 16, steps per second: 77, episode reward: 0.500, mean reward: 0.031 [-1.000, 0.100], mean action: -0.759 [-2.184, 0.090], mean observation: -0.198 [-7.848, 4.000], loss: 0.019457, mean_absolute_error: 0.124408, mean_q: -0.331035\n",
      "   226/50000: episode: 13, duration: 0.249s, episode steps: 20, steps per second: 80, episode reward: 0.900, mean reward: 0.045 [-1.000, 0.100], mean action: -0.382 [-2.031, 0.469], mean observation: 0.206 [-6.867, 4.000], loss: 0.017620, mean_absolute_error: 0.109060, mean_q: -0.345488\n",
      "   240/50000: episode: 14, duration: 0.178s, episode steps: 14, steps per second: 79, episode reward: 0.300, mean reward: 0.021 [-1.000, 0.100], mean action: -0.404 [-1.647, 1.140], mean observation: 0.100 [-8.829, 4.000], loss: 0.019255, mean_absolute_error: 0.113690, mean_q: -0.349070\n",
      "   261/50000: episode: 15, duration: 0.298s, episode steps: 21, steps per second: 71, episode reward: 1.000, mean reward: 0.048 [-1.000, 0.100], mean action: -0.282 [-1.795, 1.310], mean observation: 0.366 [-6.867, 4.000], loss: 0.016916, mean_absolute_error: 0.114090, mean_q: -0.372058\n",
      "   288/50000: episode: 16, duration: 0.375s, episode steps: 27, steps per second: 72, episode reward: 1.600, mean reward: 0.059 [-1.000, 0.100], mean action: -0.021 [-1.541, 1.572], mean observation: 0.571 [-7.848, 4.000], loss: 0.013631, mean_absolute_error: 0.102206, mean_q: -0.334206\n",
      "   307/50000: episode: 17, duration: 0.269s, episode steps: 19, steps per second: 71, episode reward: 0.800, mean reward: 0.042 [-1.000, 0.100], mean action: 0.407 [-1.052, 2.293], mean observation: 0.503 [-7.848, 4.000], loss: 0.011366, mean_absolute_error: 0.090345, mean_q: -0.305616\n",
      "   324/50000: episode: 18, duration: 0.239s, episode steps: 17, steps per second: 71, episode reward: 0.600, mean reward: 0.035 [-1.000, 0.100], mean action: 1.035 [-0.915, 4.482], mean observation: 0.194 [-6.867, 4.000], loss: 0.014517, mean_absolute_error: 0.101185, mean_q: -0.312029\n",
      "   342/50000: episode: 19, duration: 0.252s, episode steps: 18, steps per second: 71, episode reward: 0.700, mean reward: 0.039 [-1.000, 0.100], mean action: 0.986 [-0.263, 4.193], mean observation: -0.133 [-6.867, 4.000], loss: 0.013617, mean_absolute_error: 0.094092, mean_q: -0.337967\n",
      "   362/50000: episode: 20, duration: 0.249s, episode steps: 20, steps per second: 80, episode reward: 0.900, mean reward: 0.045 [-1.000, 0.100], mean action: 0.187 [-1.720, 3.546], mean observation: 0.251 [-6.867, 4.000], loss: 0.013363, mean_absolute_error: 0.095375, mean_q: -0.332440\n",
      "   379/50000: episode: 21, duration: 0.204s, episode steps: 17, steps per second: 84, episode reward: 0.600, mean reward: 0.035 [-1.000, 0.100], mean action: -0.431 [-2.774, 2.487], mean observation: 0.247 [-7.848, 4.000], loss: 0.011173, mean_absolute_error: 0.093331, mean_q: -0.350572\n",
      "   399/50000: episode: 22, duration: 0.253s, episode steps: 20, steps per second: 79, episode reward: 0.900, mean reward: 0.045 [-1.000, 0.100], mean action: -0.127 [-2.590, 3.055], mean observation: 0.383 [-5.886, 4.000], loss: 0.012446, mean_absolute_error: 0.092342, mean_q: -0.357875\n",
      "   425/50000: episode: 23, duration: 0.307s, episode steps: 26, steps per second: 85, episode reward: 1.500, mean reward: 0.058 [-1.000, 0.100], mean action: -0.065 [-1.348, 3.747], mean observation: 0.463 [-4.905, 4.000], loss: 0.009539, mean_absolute_error: 0.083295, mean_q: -0.322418\n",
      "   444/50000: episode: 24, duration: 0.248s, episode steps: 19, steps per second: 77, episode reward: 0.800, mean reward: 0.042 [-1.000, 0.100], mean action: -0.297 [-1.881, 1.557], mean observation: 0.443 [-7.848, 4.000], loss: 0.010021, mean_absolute_error: 0.085124, mean_q: -0.320464\n",
      "   462/50000: episode: 25, duration: 0.262s, episode steps: 18, steps per second: 69, episode reward: 0.700, mean reward: 0.039 [-1.000, 0.100], mean action: 0.055 [-1.221, 1.974], mean observation: 0.272 [-7.848, 4.000], loss: 0.009145, mean_absolute_error: 0.080919, mean_q: -0.306501\n",
      "   481/50000: episode: 26, duration: 0.244s, episode steps: 19, steps per second: 78, episode reward: 0.800, mean reward: 0.042 [-1.000, 0.100], mean action: -0.320 [-1.527, 1.142], mean observation: 0.458 [-7.848, 4.000], loss: 0.008457, mean_absolute_error: 0.080406, mean_q: -0.306943\n",
      "   496/50000: episode: 27, duration: 0.198s, episode steps: 15, steps per second: 76, episode reward: 0.400, mean reward: 0.027 [-1.000, 0.100], mean action: 0.707 [-1.228, 3.358], mean observation: 0.209 [-6.867, 4.000], loss: 0.008460, mean_absolute_error: 0.078406, mean_q: -0.289637\n",
      "   513/50000: episode: 28, duration: 0.216s, episode steps: 17, steps per second: 79, episode reward: 0.600, mean reward: 0.035 [-1.000, 0.100], mean action: 0.062 [-1.745, 2.211], mean observation: 0.291 [-7.848, 4.000], loss: 0.008406, mean_absolute_error: 0.077720, mean_q: -0.322285\n",
      "   532/50000: episode: 29, duration: 0.237s, episode steps: 19, steps per second: 80, episode reward: 0.800, mean reward: 0.042 [-1.000, 0.100], mean action: -0.108 [-1.842, 2.122], mean observation: 0.353 [-6.867, 4.000], loss: 0.008613, mean_absolute_error: 0.077898, mean_q: -0.295694\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   551/50000: episode: 30, duration: 0.318s, episode steps: 19, steps per second: 60, episode reward: 0.800, mean reward: 0.042 [-1.000, 0.100], mean action: -0.028 [-1.821, 1.601], mean observation: 0.356 [-7.848, 4.000], loss: 0.006909, mean_absolute_error: 0.071849, mean_q: -0.291069\n",
      "   570/50000: episode: 31, duration: 0.240s, episode steps: 19, steps per second: 79, episode reward: 0.800, mean reward: 0.042 [-1.000, 0.100], mean action: 0.193 [-1.509, 3.074], mean observation: 0.262 [-5.886, 4.000], loss: 0.008931, mean_absolute_error: 0.083821, mean_q: -0.292774\n",
      "   584/50000: episode: 32, duration: 0.173s, episode steps: 14, steps per second: 81, episode reward: 0.300, mean reward: 0.021 [-1.000, 0.100], mean action: 0.221 [-1.217, 1.842], mean observation: 0.152 [-8.829, 4.000], loss: 0.006739, mean_absolute_error: 0.074450, mean_q: -0.305965\n",
      "   604/50000: episode: 33, duration: 0.303s, episode steps: 20, steps per second: 66, episode reward: 0.900, mean reward: 0.045 [-1.000, 0.100], mean action: -0.140 [-2.249, 1.580], mean observation: 0.377 [-7.848, 4.000], loss: 0.008319, mean_absolute_error: 0.076654, mean_q: -0.300755\n",
      "   619/50000: episode: 34, duration: 0.210s, episode steps: 15, steps per second: 71, episode reward: 0.400, mean reward: 0.027 [-1.000, 0.100], mean action: 0.461 [-0.901, 1.974], mean observation: 0.273 [-7.848, 4.000], loss: 0.008599, mean_absolute_error: 0.080996, mean_q: -0.288730\n",
      "   637/50000: episode: 35, duration: 0.256s, episode steps: 18, steps per second: 70, episode reward: 0.700, mean reward: 0.039 [-1.000, 0.100], mean action: 0.375 [-0.841, 1.850], mean observation: 0.287 [-7.848, 4.000], loss: 0.007983, mean_absolute_error: 0.086613, mean_q: -0.313970\n",
      "   652/50000: episode: 36, duration: 0.179s, episode steps: 15, steps per second: 84, episode reward: 0.400, mean reward: 0.027 [-1.000, 0.100], mean action: 0.385 [-1.216, 2.025], mean observation: 0.160 [-7.848, 4.000], loss: 0.007873, mean_absolute_error: 0.081276, mean_q: -0.293120\n",
      "   668/50000: episode: 37, duration: 0.232s, episode steps: 16, steps per second: 69, episode reward: 0.500, mean reward: 0.031 [-1.000, 0.100], mean action: 0.387 [-1.067, 1.566], mean observation: 0.253 [-8.829, 4.000], loss: 0.007015, mean_absolute_error: 0.074291, mean_q: -0.318318\n",
      "   683/50000: episode: 38, duration: 0.211s, episode steps: 15, steps per second: 71, episode reward: 0.400, mean reward: 0.027 [-1.000, 0.100], mean action: 0.646 [-1.627, 3.256], mean observation: 0.146 [-6.867, 4.000], loss: 0.007750, mean_absolute_error: 0.074282, mean_q: -0.275496\n",
      "   698/50000: episode: 39, duration: 0.189s, episode steps: 15, steps per second: 79, episode reward: 0.400, mean reward: 0.027 [-1.000, 0.100], mean action: 0.432 [-0.853, 1.746], mean observation: 0.240 [-8.829, 4.000], loss: 0.007269, mean_absolute_error: 0.070721, mean_q: -0.289710\n",
      "   713/50000: episode: 40, duration: 0.303s, episode steps: 15, steps per second: 50, episode reward: 0.400, mean reward: 0.027 [-1.000, 0.100], mean action: 0.403 [-1.085, 1.813], mean observation: 0.200 [-8.829, 4.000], loss: 0.006248, mean_absolute_error: 0.071527, mean_q: -0.285602\n",
      "   728/50000: episode: 41, duration: 0.335s, episode steps: 15, steps per second: 45, episode reward: 0.400, mean reward: 0.027 [-1.000, 0.100], mean action: 0.352 [-1.106, 1.797], mean observation: 0.244 [-7.848, 4.000], loss: 0.009806, mean_absolute_error: 0.081610, mean_q: -0.310837\n",
      "   746/50000: episode: 42, duration: 0.384s, episode steps: 18, steps per second: 47, episode reward: 0.700, mean reward: 0.039 [-1.000, 0.100], mean action: 0.341 [-0.911, 1.643], mean observation: 0.310 [-7.848, 4.000], loss: 0.008413, mean_absolute_error: 0.086462, mean_q: -0.300743\n",
      "   762/50000: episode: 43, duration: 0.187s, episode steps: 16, steps per second: 85, episode reward: 0.500, mean reward: 0.031 [-1.000, 0.100], mean action: 0.456 [-1.187, 2.249], mean observation: 0.231 [-7.848, 4.000], loss: 0.007860, mean_absolute_error: 0.075112, mean_q: -0.293118\n",
      "   777/50000: episode: 44, duration: 0.186s, episode steps: 15, steps per second: 81, episode reward: 0.400, mean reward: 0.027 [-1.000, 0.100], mean action: 0.315 [-1.156, 1.494], mean observation: 0.140 [-9.810, 4.000], loss: 0.006148, mean_absolute_error: 0.066342, mean_q: -0.305661\n",
      "   796/50000: episode: 45, duration: 0.259s, episode steps: 19, steps per second: 73, episode reward: 0.800, mean reward: 0.042 [-1.000, 0.100], mean action: 0.261 [-1.023, 1.436], mean observation: 0.268 [-7.848, 4.000], loss: 0.004488, mean_absolute_error: 0.058732, mean_q: -0.278252\n",
      "   817/50000: episode: 46, duration: 0.259s, episode steps: 21, steps per second: 81, episode reward: 1.000, mean reward: 0.048 [-1.000, 0.100], mean action: -0.029 [-1.144, 1.410], mean observation: 0.455 [-6.867, 4.000], loss: 0.006945, mean_absolute_error: 0.068837, mean_q: -0.284356\n",
      "   840/50000: episode: 47, duration: 0.284s, episode steps: 23, steps per second: 81, episode reward: 1.200, mean reward: 0.052 [-1.000, 0.100], mean action: 0.139 [-1.282, 2.783], mean observation: 0.442 [-5.886, 4.000], loss: 0.005470, mean_absolute_error: 0.063059, mean_q: -0.265987\n",
      "   861/50000: episode: 48, duration: 0.260s, episode steps: 21, steps per second: 81, episode reward: 1.000, mean reward: 0.048 [-1.000, 0.100], mean action: 0.192 [-1.303, 3.544], mean observation: 0.414 [-4.905, 4.000], loss: 0.007724, mean_absolute_error: 0.077120, mean_q: -0.291565\n",
      "   881/50000: episode: 49, duration: 0.254s, episode steps: 20, steps per second: 79, episode reward: 0.900, mean reward: 0.045 [-1.000, 0.100], mean action: -0.040 [-1.278, 1.548], mean observation: 0.423 [-7.848, 4.000], loss: 0.006739, mean_absolute_error: 0.065155, mean_q: -0.258076\n",
      "   897/50000: episode: 50, duration: 0.215s, episode steps: 16, steps per second: 74, episode reward: 0.500, mean reward: 0.031 [-1.000, 0.100], mean action: 0.292 [-1.040, 1.273], mean observation: 0.326 [-7.848, 4.000], loss: 0.006107, mean_absolute_error: 0.064871, mean_q: -0.285682\n",
      "   917/50000: episode: 51, duration: 0.267s, episode steps: 20, steps per second: 75, episode reward: 0.900, mean reward: 0.045 [-1.000, 0.100], mean action: -0.009 [-1.181, 1.648], mean observation: 0.424 [-7.848, 4.000], loss: 0.005211, mean_absolute_error: 0.061792, mean_q: -0.233258\n",
      "   944/50000: episode: 52, duration: 0.339s, episode steps: 27, steps per second: 80, episode reward: 1.600, mean reward: 0.059 [-1.000, 0.100], mean action: 0.128 [-1.603, 3.850], mean observation: 0.535 [-4.905, 4.000], loss: 0.005443, mean_absolute_error: 0.063774, mean_q: -0.276923\n",
      "   967/50000: episode: 53, duration: 0.275s, episode steps: 23, steps per second: 84, episode reward: 1.200, mean reward: 0.052 [-1.000, 0.100], mean action: 0.029 [-1.791, 3.337], mean observation: 0.448 [-6.867, 4.000], loss: 0.005559, mean_absolute_error: 0.063270, mean_q: -0.276948\n",
      "   982/50000: episode: 54, duration: 0.185s, episode steps: 15, steps per second: 81, episode reward: 0.400, mean reward: 0.027 [-1.000, 0.100], mean action: 0.420 [-1.753, 3.114], mean observation: 0.173 [-7.848, 4.000], loss: 0.004916, mean_absolute_error: 0.060853, mean_q: -0.266637\n",
      "  1000/50000: episode: 55, duration: 0.232s, episode steps: 18, steps per second: 77, episode reward: 0.700, mean reward: 0.039 [-1.000, 0.100], mean action: 0.326 [-0.784, 1.447], mean observation: 0.336 [-7.848, 4.000], loss: 0.003545, mean_absolute_error: 0.050928, mean_q: -0.269885\n",
      "  1018/50000: episode: 56, duration: 0.234s, episode steps: 18, steps per second: 77, episode reward: 0.700, mean reward: 0.039 [-1.000, 0.100], mean action: -0.056 [-1.565, 1.623], mean observation: 0.399 [-6.867, 4.000], loss: 0.005485, mean_absolute_error: 0.059930, mean_q: -0.259164\n",
      "  1034/50000: episode: 57, duration: 0.196s, episode steps: 16, steps per second: 81, episode reward: 0.500, mean reward: 0.031 [-1.000, 0.100], mean action: 0.284 [-1.007, 1.915], mean observation: 0.332 [-7.848, 4.000], loss: 0.006652, mean_absolute_error: 0.073431, mean_q: -0.240721\n",
      "  1049/50000: episode: 58, duration: 0.181s, episode steps: 15, steps per second: 83, episode reward: 0.400, mean reward: 0.027 [-1.000, 0.100], mean action: 0.368 [-0.950, 1.400], mean observation: 0.281 [-8.829, 4.000], loss: 0.005397, mean_absolute_error: 0.066668, mean_q: -0.243314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1072/50000: episode: 59, duration: 0.303s, episode steps: 23, steps per second: 76, episode reward: 1.200, mean reward: 0.052 [-1.000, 0.100], mean action: 0.063 [-1.594, 4.268], mean observation: 0.407 [-4.905, 4.000], loss: 0.004265, mean_absolute_error: 0.057198, mean_q: -0.242825\n",
      "  1089/50000: episode: 60, duration: 0.215s, episode steps: 17, steps per second: 79, episode reward: 0.600, mean reward: 0.035 [-1.000, 0.100], mean action: 0.328 [-1.451, 3.833], mean observation: 0.307 [-6.867, 4.000], loss: 0.004426, mean_absolute_error: 0.055560, mean_q: -0.243024\n",
      "  1106/50000: episode: 61, duration: 0.243s, episode steps: 17, steps per second: 70, episode reward: 0.600, mean reward: 0.035 [-1.000, 0.100], mean action: -0.030 [-0.909, 1.130], mean observation: 0.343 [-7.848, 4.000], loss: 0.004848, mean_absolute_error: 0.059053, mean_q: -0.249043\n",
      "  1122/50000: episode: 62, duration: 0.215s, episode steps: 16, steps per second: 74, episode reward: 0.500, mean reward: 0.031 [-1.000, 0.100], mean action: -0.279 [-1.651, 0.876], mean observation: 0.295 [-7.848, 4.000], loss: 0.005400, mean_absolute_error: 0.059810, mean_q: -0.233132\n",
      "  1144/50000: episode: 63, duration: 0.265s, episode steps: 22, steps per second: 83, episode reward: 1.100, mean reward: 0.050 [-1.000, 0.100], mean action: -0.006 [-1.636, 2.101], mean observation: 0.506 [-6.867, 4.000], loss: 0.004615, mean_absolute_error: 0.054036, mean_q: -0.225005\n",
      "  1163/50000: episode: 64, duration: 0.238s, episode steps: 19, steps per second: 80, episode reward: 0.800, mean reward: 0.042 [-1.000, 0.100], mean action: -0.170 [-1.476, 1.387], mean observation: 0.403 [-7.848, 4.000], loss: 0.006515, mean_absolute_error: 0.068809, mean_q: -0.220726\n",
      "  1178/50000: episode: 65, duration: 0.203s, episode steps: 15, steps per second: 74, episode reward: 0.400, mean reward: 0.027 [-1.000, 0.100], mean action: 0.276 [-1.001, 1.762], mean observation: 0.309 [-8.829, 4.000], loss: 0.005050, mean_absolute_error: 0.059641, mean_q: -0.246340\n",
      "  1193/50000: episode: 66, duration: 0.185s, episode steps: 15, steps per second: 81, episode reward: 0.400, mean reward: 0.027 [-1.000, 0.100], mean action: 0.177 [-1.230, 1.252], mean observation: 0.232 [-8.829, 4.000], loss: 0.006280, mean_absolute_error: 0.067080, mean_q: -0.244188\n",
      "  1217/50000: episode: 67, duration: 0.310s, episode steps: 24, steps per second: 77, episode reward: 1.300, mean reward: 0.054 [-1.000, 0.100], mean action: 0.071 [-1.789, 2.875], mean observation: 0.246 [-5.886, 4.000], loss: 0.005212, mean_absolute_error: 0.058548, mean_q: -0.220671\n",
      "  1237/50000: episode: 68, duration: 0.251s, episode steps: 20, steps per second: 80, episode reward: 0.900, mean reward: 0.045 [-1.000, 0.100], mean action: -0.044 [-1.377, 1.963], mean observation: 0.388 [-7.848, 4.000], loss: 0.004417, mean_absolute_error: 0.060914, mean_q: -0.241600\n",
      "  1259/50000: episode: 69, duration: 0.270s, episode steps: 22, steps per second: 82, episode reward: 1.100, mean reward: 0.050 [-1.000, 0.100], mean action: -0.045 [-1.633, 2.261], mean observation: 0.451 [-6.867, 4.000], loss: 0.004697, mean_absolute_error: 0.058998, mean_q: -0.236865\n",
      "  1293/50000: episode: 70, duration: 0.414s, episode steps: 34, steps per second: 82, episode reward: 2.300, mean reward: 0.068 [-1.000, 0.100], mean action: 0.077 [-1.249, 2.779], mean observation: 0.335 [-6.867, 4.000], loss: 0.005466, mean_absolute_error: 0.061335, mean_q: -0.218125\n",
      "  1314/50000: episode: 71, duration: 0.255s, episode steps: 21, steps per second: 82, episode reward: 1.000, mean reward: 0.048 [-1.000, 0.100], mean action: 0.109 [-2.571, 3.259], mean observation: 0.212 [-5.886, 4.000], loss: 0.004464, mean_absolute_error: 0.061358, mean_q: -0.198838\n",
      "  1332/50000: episode: 72, duration: 0.221s, episode steps: 18, steps per second: 82, episode reward: 0.700, mean reward: 0.039 [-1.000, 0.100], mean action: 0.280 [-1.233, 1.589], mean observation: 0.328 [-7.848, 4.000], loss: 0.004197, mean_absolute_error: 0.054434, mean_q: -0.213271\n",
      "  1348/50000: episode: 73, duration: 0.189s, episode steps: 16, steps per second: 85, episode reward: 0.500, mean reward: 0.031 [-1.000, 0.100], mean action: -0.174 [-1.009, 1.221], mean observation: 0.284 [-7.848, 4.000], loss: 0.003859, mean_absolute_error: 0.051044, mean_q: -0.217616\n",
      "  1380/50000: episode: 74, duration: 0.389s, episode steps: 32, steps per second: 82, episode reward: 2.100, mean reward: 0.066 [-1.000, 0.100], mean action: 0.084 [-1.920, 3.693], mean observation: 0.283 [-6.867, 4.000], loss: 0.004102, mean_absolute_error: 0.055324, mean_q: -0.197824\n",
      "  1400/50000: episode: 75, duration: 0.271s, episode steps: 20, steps per second: 74, episode reward: 0.900, mean reward: 0.045 [-1.000, 0.100], mean action: -0.185 [-1.500, 2.017], mean observation: 0.375 [-7.848, 4.000], loss: 0.005263, mean_absolute_error: 0.059608, mean_q: -0.201078\n",
      "  1416/50000: episode: 76, duration: 0.196s, episode steps: 16, steps per second: 82, episode reward: 0.500, mean reward: 0.031 [-1.000, 0.100], mean action: -0.117 [-1.229, 1.559], mean observation: 0.195 [-7.848, 4.000], loss: 0.003879, mean_absolute_error: 0.056993, mean_q: -0.211291\n",
      "  1434/50000: episode: 77, duration: 0.230s, episode steps: 18, steps per second: 78, episode reward: 0.700, mean reward: 0.039 [-1.000, 0.100], mean action: 0.395 [-1.551, 4.005], mean observation: 0.397 [-6.867, 4.000], loss: 0.005102, mean_absolute_error: 0.056539, mean_q: -0.199536\n",
      "  1449/50000: episode: 78, duration: 0.224s, episode steps: 15, steps per second: 67, episode reward: 0.400, mean reward: 0.027 [-1.000, 0.100], mean action: 0.075 [-1.380, 1.448], mean observation: 0.170 [-8.829, 4.000], loss: 0.004168, mean_absolute_error: 0.055742, mean_q: -0.209037\n",
      "  1469/50000: episode: 79, duration: 0.262s, episode steps: 20, steps per second: 76, episode reward: 0.900, mean reward: 0.045 [-1.000, 0.100], mean action: -0.332 [-1.578, 1.122], mean observation: 0.404 [-7.848, 4.000], loss: 0.003722, mean_absolute_error: 0.058598, mean_q: -0.210814\n",
      "  1483/50000: episode: 80, duration: 0.176s, episode steps: 14, steps per second: 80, episode reward: 0.300, mean reward: 0.021 [-1.000, 0.100], mean action: -0.401 [-2.409, 1.680], mean observation: -0.065 [-8.829, 4.000], loss: 0.004548, mean_absolute_error: 0.059086, mean_q: -0.195708\n",
      "  1500/50000: episode: 81, duration: 0.215s, episode steps: 17, steps per second: 79, episode reward: 0.600, mean reward: 0.035 [-1.000, 0.100], mean action: -0.198 [-1.067, 1.179], mean observation: 0.331 [-7.848, 4.000], loss: 0.003938, mean_absolute_error: 0.055217, mean_q: -0.205097\n",
      "  1516/50000: episode: 82, duration: 0.197s, episode steps: 16, steps per second: 81, episode reward: 0.500, mean reward: 0.031 [-1.000, 0.100], mean action: -0.115 [-1.480, 1.058], mean observation: 0.175 [-7.848, 4.000], loss: 0.004370, mean_absolute_error: 0.051364, mean_q: -0.218226\n",
      "  1531/50000: episode: 83, duration: 0.203s, episode steps: 15, steps per second: 74, episode reward: 0.400, mean reward: 0.027 [-1.000, 0.100], mean action: -0.150 [-1.518, 1.460], mean observation: 0.125 [-8.829, 4.000], loss: 0.004092, mean_absolute_error: 0.052714, mean_q: -0.199234\n",
      "  1547/50000: episode: 84, duration: 0.216s, episode steps: 16, steps per second: 74, episode reward: 0.500, mean reward: 0.031 [-1.000, 0.100], mean action: -0.413 [-2.121, 1.327], mean observation: 0.014 [-7.848, 4.000], loss: 0.004077, mean_absolute_error: 0.054028, mean_q: -0.163167\n",
      "  1567/50000: episode: 85, duration: 0.246s, episode steps: 20, steps per second: 81, episode reward: 0.900, mean reward: 0.045 [-1.000, 0.100], mean action: 0.000 [-1.112, 1.480], mean observation: 0.352 [-7.848, 4.000], loss: 0.003068, mean_absolute_error: 0.050810, mean_q: -0.192879\n",
      "  1592/50000: episode: 86, duration: 0.301s, episode steps: 25, steps per second: 83, episode reward: 1.400, mean reward: 0.056 [-1.000, 0.100], mean action: -0.314 [-1.701, 1.748], mean observation: 0.481 [-6.867, 4.000], loss: 0.002648, mean_absolute_error: 0.044427, mean_q: -0.181761\n",
      "  1609/50000: episode: 87, duration: 0.210s, episode steps: 17, steps per second: 81, episode reward: 0.600, mean reward: 0.035 [-1.000, 0.100], mean action: -0.083 [-1.355, 1.439], mean observation: 0.260 [-8.829, 4.000], loss: 0.003487, mean_absolute_error: 0.054269, mean_q: -0.212585\n",
      "  1625/50000: episode: 88, duration: 0.188s, episode steps: 16, steps per second: 85, episode reward: 0.500, mean reward: 0.031 [-1.000, 0.100], mean action: 0.581 [-1.401, 4.626], mean observation: 0.269 [-6.867, 4.000], loss: 0.004731, mean_absolute_error: 0.058624, mean_q: -0.214301\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1640/50000: episode: 89, duration: 0.208s, episode steps: 15, steps per second: 72, episode reward: 0.400, mean reward: 0.027 [-1.000, 0.100], mean action: -0.039 [-1.493, 1.771], mean observation: 0.175 [-8.829, 4.000], loss: 0.003033, mean_absolute_error: 0.049859, mean_q: -0.187453\n",
      "  1654/50000: episode: 90, duration: 0.170s, episode steps: 14, steps per second: 83, episode reward: 0.300, mean reward: 0.021 [-1.000, 0.100], mean action: -0.604 [-3.255, 1.647], mean observation: -0.088 [-8.829, 4.000], loss: 0.003116, mean_absolute_error: 0.045873, mean_q: -0.187394\n",
      "  1673/50000: episode: 91, duration: 0.246s, episode steps: 19, steps per second: 77, episode reward: 0.800, mean reward: 0.042 [-1.000, 0.100], mean action: -0.452 [-2.306, 0.983], mean observation: 0.386 [-6.867, 4.000], loss: 0.003749, mean_absolute_error: 0.055552, mean_q: -0.211366\n",
      "  1708/50000: episode: 92, duration: 0.418s, episode steps: 35, steps per second: 84, episode reward: 2.400, mean reward: 0.069 [-1.000, 0.100], mean action: -0.080 [-1.603, 3.087], mean observation: 0.376 [-5.886, 4.000], loss: 0.003404, mean_absolute_error: 0.052075, mean_q: -0.205886\n",
      "  1727/50000: episode: 93, duration: 0.260s, episode steps: 19, steps per second: 73, episode reward: 0.800, mean reward: 0.042 [-1.000, 0.100], mean action: -0.477 [-2.145, 1.322], mean observation: 0.393 [-7.848, 4.000], loss: 0.002607, mean_absolute_error: 0.048817, mean_q: -0.185799\n",
      "  1745/50000: episode: 94, duration: 0.250s, episode steps: 18, steps per second: 72, episode reward: 0.700, mean reward: 0.039 [-1.000, 0.100], mean action: 0.226 [-1.637, 1.773], mean observation: 0.189 [-7.848, 4.000], loss: 0.003499, mean_absolute_error: 0.048446, mean_q: -0.213847\n",
      "  1760/50000: episode: 95, duration: 0.180s, episode steps: 15, steps per second: 83, episode reward: 0.400, mean reward: 0.027 [-1.000, 0.100], mean action: -0.452 [-2.781, 1.817], mean observation: 0.143 [-8.829, 4.000], loss: 0.003651, mean_absolute_error: 0.054287, mean_q: -0.194635\n",
      "  1775/50000: episode: 96, duration: 0.201s, episode steps: 15, steps per second: 74, episode reward: 0.400, mean reward: 0.027 [-1.000, 0.100], mean action: -0.572 [-3.456, 1.770], mean observation: 0.164 [-7.848, 4.000], loss: 0.003658, mean_absolute_error: 0.051132, mean_q: -0.182308\n",
      "  1796/50000: episode: 97, duration: 0.247s, episode steps: 21, steps per second: 85, episode reward: 1.000, mean reward: 0.048 [-1.000, 0.100], mean action: -0.198 [-1.753, 2.426], mean observation: 0.352 [-5.886, 4.000], loss: 0.002979, mean_absolute_error: 0.049980, mean_q: -0.197653\n",
      "  1820/50000: episode: 98, duration: 0.287s, episode steps: 24, steps per second: 84, episode reward: 1.300, mean reward: 0.054 [-1.000, 0.100], mean action: -0.120 [-1.508, 2.387], mean observation: 0.542 [-6.867, 4.000], loss: 0.002237, mean_absolute_error: 0.044290, mean_q: -0.176570\n",
      "  1835/50000: episode: 99, duration: 0.187s, episode steps: 15, steps per second: 80, episode reward: 0.400, mean reward: 0.027 [-1.000, 0.100], mean action: -0.108 [-1.753, 1.320], mean observation: 0.088 [-8.829, 4.000], loss: 0.003520, mean_absolute_error: 0.049091, mean_q: -0.204266\n",
      "  1853/50000: episode: 100, duration: 0.313s, episode steps: 18, steps per second: 58, episode reward: 0.700, mean reward: 0.039 [-1.000, 0.100], mean action: 0.298 [-1.240, 2.621], mean observation: 0.397 [-6.867, 4.000], loss: 0.003619, mean_absolute_error: 0.050879, mean_q: -0.187267\n",
      "  1870/50000: episode: 101, duration: 0.214s, episode steps: 17, steps per second: 79, episode reward: 0.600, mean reward: 0.035 [-1.000, 0.100], mean action: 0.186 [-1.338, 1.608], mean observation: 0.246 [-7.848, 4.000], loss: 0.002885, mean_absolute_error: 0.046307, mean_q: -0.179085\n",
      "  1885/50000: episode: 102, duration: 0.214s, episode steps: 15, steps per second: 70, episode reward: 0.400, mean reward: 0.027 [-1.000, 0.100], mean action: -0.768 [-4.382, 1.627], mean observation: 0.041 [-8.829, 4.000], loss: 0.002472, mean_absolute_error: 0.044884, mean_q: -0.186353\n",
      "  1902/50000: episode: 103, duration: 0.197s, episode steps: 17, steps per second: 86, episode reward: 0.600, mean reward: 0.035 [-1.000, 0.100], mean action: -0.430 [-3.222, 1.878], mean observation: 0.224 [-7.848, 4.000], loss: 0.002498, mean_absolute_error: 0.045397, mean_q: -0.191120\n",
      "  1921/50000: episode: 104, duration: 0.261s, episode steps: 19, steps per second: 73, episode reward: 0.800, mean reward: 0.042 [-1.000, 0.100], mean action: -0.128 [-1.885, 3.279], mean observation: 0.213 [-5.886, 4.000], loss: 0.003000, mean_absolute_error: 0.047469, mean_q: -0.189877\n",
      "  1935/50000: episode: 105, duration: 0.212s, episode steps: 14, steps per second: 66, episode reward: 0.300, mean reward: 0.021 [-1.000, 0.100], mean action: -0.885 [-5.023, 1.733], mean observation: -0.089 [-8.829, 4.000], loss: 0.002616, mean_absolute_error: 0.045392, mean_q: -0.163823\n",
      "  1958/50000: episode: 106, duration: 0.294s, episode steps: 23, steps per second: 78, episode reward: 1.200, mean reward: 0.052 [-1.000, 0.100], mean action: -0.378 [-3.063, 2.713], mean observation: 0.387 [-6.867, 4.000], loss: 0.002564, mean_absolute_error: 0.045829, mean_q: -0.184868\n",
      "  1972/50000: episode: 107, duration: 0.184s, episode steps: 14, steps per second: 76, episode reward: 0.300, mean reward: 0.021 [-1.000, 0.100], mean action: -0.476 [-3.837, 1.409], mean observation: 0.157 [-8.829, 4.000], loss: 0.002464, mean_absolute_error: 0.044088, mean_q: -0.201352\n",
      "  1987/50000: episode: 108, duration: 0.192s, episode steps: 15, steps per second: 78, episode reward: 0.400, mean reward: 0.027 [-1.000, 0.100], mean action: -0.701 [-4.574, 1.598], mean observation: 0.024 [-8.829, 4.000], loss: 0.002177, mean_absolute_error: 0.040226, mean_q: -0.162919\n",
      "  2001/50000: episode: 109, duration: 0.175s, episode steps: 14, steps per second: 80, episode reward: 0.300, mean reward: 0.021 [-1.000, 0.100], mean action: -1.139 [-6.010, 1.680], mean observation: -0.101 [-8.829, 4.000], loss: 0.002661, mean_absolute_error: 0.041928, mean_q: -0.187923\n",
      "  2019/50000: episode: 110, duration: 0.249s, episode steps: 18, steps per second: 72, episode reward: 0.700, mean reward: 0.039 [-1.000, 0.100], mean action: -0.805 [-4.239, 1.610], mean observation: 0.262 [-7.848, 4.000], loss: 0.002605, mean_absolute_error: 0.044423, mean_q: -0.180015\n",
      "  2033/50000: episode: 111, duration: 0.173s, episode steps: 14, steps per second: 81, episode reward: 0.300, mean reward: 0.021 [-1.000, 0.100], mean action: -0.937 [-5.926, 1.465], mean observation: 0.033 [-8.829, 4.000], loss: 0.003295, mean_absolute_error: 0.044657, mean_q: -0.175266\n",
      "  2050/50000: episode: 112, duration: 0.224s, episode steps: 17, steps per second: 76, episode reward: 0.600, mean reward: 0.035 [-1.000, 0.100], mean action: -0.596 [-3.293, 2.503], mean observation: 0.168 [-7.848, 4.000], loss: 0.003184, mean_absolute_error: 0.050133, mean_q: -0.178375\n",
      "  2067/50000: episode: 113, duration: 0.230s, episode steps: 17, steps per second: 74, episode reward: 0.600, mean reward: 0.035 [-1.000, 0.100], mean action: 0.738 [-1.147, 4.150], mean observation: 0.226 [-6.867, 4.000], loss: 0.002576, mean_absolute_error: 0.047178, mean_q: -0.181607\n",
      "  2081/50000: episode: 114, duration: 0.204s, episode steps: 14, steps per second: 68, episode reward: 0.300, mean reward: 0.021 [-1.000, 0.100], mean action: 0.641 [-1.639, 3.706], mean observation: 0.072 [-7.848, 4.000], loss: 0.002693, mean_absolute_error: 0.045359, mean_q: -0.183810\n",
      "  2096/50000: episode: 115, duration: 0.185s, episode steps: 15, steps per second: 81, episode reward: 0.400, mean reward: 0.027 [-1.000, 0.100], mean action: 0.051 [-1.838, 1.959], mean observation: 0.129 [-8.829, 4.000], loss: 0.002533, mean_absolute_error: 0.044807, mean_q: -0.159300\n",
      "  2120/50000: episode: 116, duration: 0.327s, episode steps: 24, steps per second: 73, episode reward: 1.300, mean reward: 0.054 [-1.000, 0.100], mean action: -0.357 [-2.760, 3.148], mean observation: 0.377 [-5.886, 4.000], loss: 0.002939, mean_absolute_error: 0.047620, mean_q: -0.176010\n",
      "  2141/50000: episode: 117, duration: 0.257s, episode steps: 21, steps per second: 82, episode reward: 1.000, mean reward: 0.048 [-1.000, 0.100], mean action: -0.473 [-3.571, 2.668], mean observation: 0.360 [-6.867, 4.000], loss: 0.002990, mean_absolute_error: 0.048359, mean_q: -0.169594\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  2162/50000: episode: 118, duration: 0.246s, episode steps: 21, steps per second: 85, episode reward: 1.000, mean reward: 0.048 [-1.000, 0.100], mean action: -0.611 [-4.824, 1.825], mean observation: 0.427 [-6.867, 4.000], loss: 0.001629, mean_absolute_error: 0.038884, mean_q: -0.150627\n",
      "  2180/50000: episode: 119, duration: 0.282s, episode steps: 18, steps per second: 64, episode reward: 0.700, mean reward: 0.039 [-1.000, 0.100], mean action: -0.259 [-1.440, 1.131], mean observation: 0.297 [-7.848, 4.000], loss: 0.002074, mean_absolute_error: 0.040838, mean_q: -0.167313\n",
      "  2208/50000: episode: 120, duration: 0.425s, episode steps: 28, steps per second: 66, episode reward: 1.700, mean reward: 0.061 [-1.000, 0.100], mean action: -0.139 [-2.838, 4.244], mean observation: 0.422 [-5.206, 4.000], loss: 0.002217, mean_absolute_error: 0.045703, mean_q: -0.165250\n",
      "  2223/50000: episode: 121, duration: 0.210s, episode steps: 15, steps per second: 72, episode reward: 0.400, mean reward: 0.027 [-1.000, 0.100], mean action: -0.397 [-2.934, 1.295], mean observation: 0.179 [-8.829, 4.000], loss: 0.002142, mean_absolute_error: 0.044191, mean_q: -0.139581\n",
      "  2241/50000: episode: 122, duration: 0.214s, episode steps: 18, steps per second: 84, episode reward: 0.700, mean reward: 0.039 [-1.000, 0.100], mean action: -0.425 [-1.981, 1.038], mean observation: 0.307 [-7.848, 4.000], loss: 0.002163, mean_absolute_error: 0.044136, mean_q: -0.146004\n",
      "  2258/50000: episode: 123, duration: 0.198s, episode steps: 17, steps per second: 86, episode reward: 0.600, mean reward: 0.035 [-1.000, 0.100], mean action: 0.231 [-1.836, 2.747], mean observation: 0.255 [-7.848, 4.000], loss: 0.002041, mean_absolute_error: 0.042264, mean_q: -0.171715\n",
      "  2276/50000: episode: 124, duration: 0.250s, episode steps: 18, steps per second: 72, episode reward: 0.700, mean reward: 0.039 [-1.000, 0.100], mean action: -0.352 [-1.710, 1.310], mean observation: 0.317 [-7.848, 4.000], loss: 0.002088, mean_absolute_error: 0.041030, mean_q: -0.167265\n",
      "  2291/50000: episode: 125, duration: 0.182s, episode steps: 15, steps per second: 82, episode reward: 0.400, mean reward: 0.027 [-1.000, 0.100], mean action: -0.407 [-3.099, 1.457], mean observation: 0.100 [-8.829, 4.000], loss: 0.001573, mean_absolute_error: 0.037877, mean_q: -0.148677\n",
      "  2305/50000: episode: 126, duration: 0.198s, episode steps: 14, steps per second: 71, episode reward: 0.300, mean reward: 0.021 [-1.000, 0.100], mean action: -0.487 [-3.506, 1.383], mean observation: 0.116 [-8.829, 4.000], loss: 0.001781, mean_absolute_error: 0.039700, mean_q: -0.146110\n",
      "  2323/50000: episode: 127, duration: 0.225s, episode steps: 18, steps per second: 80, episode reward: 0.700, mean reward: 0.039 [-1.000, 0.100], mean action: -0.506 [-3.169, 1.174], mean observation: 0.286 [-7.848, 4.000], loss: 0.002153, mean_absolute_error: 0.044188, mean_q: -0.125201\n",
      "  2342/50000: episode: 128, duration: 0.294s, episode steps: 19, steps per second: 65, episode reward: 0.800, mean reward: 0.042 [-1.000, 0.100], mean action: -0.355 [-2.549, 1.212], mean observation: 0.389 [-7.848, 4.000], loss: 0.002361, mean_absolute_error: 0.047406, mean_q: -0.180911\n",
      "  2359/50000: episode: 129, duration: 0.239s, episode steps: 17, steps per second: 71, episode reward: 0.600, mean reward: 0.035 [-1.000, 0.100], mean action: -0.440 [-1.884, 1.023], mean observation: 0.246 [-7.848, 4.000], loss: 0.002122, mean_absolute_error: 0.041509, mean_q: -0.178315\n",
      "  2376/50000: episode: 130, duration: 0.216s, episode steps: 17, steps per second: 79, episode reward: 0.600, mean reward: 0.035 [-1.000, 0.100], mean action: 0.708 [-1.864, 5.413], mean observation: 0.166 [-6.867, 4.000], loss: 0.002552, mean_absolute_error: 0.047491, mean_q: -0.149400\n",
      "  2400/50000: episode: 131, duration: 0.326s, episode steps: 24, steps per second: 74, episode reward: 1.300, mean reward: 0.054 [-1.000, 0.100], mean action: 0.093 [-1.811, 3.151], mean observation: 0.495 [-6.867, 4.000], loss: 0.001856, mean_absolute_error: 0.039916, mean_q: -0.125874\n",
      "  2426/50000: episode: 132, duration: 0.311s, episode steps: 26, steps per second: 84, episode reward: 1.500, mean reward: 0.058 [-1.000, 0.100], mean action: -0.057 [-1.712, 2.378], mean observation: 0.451 [-6.867, 4.000], loss: 0.002139, mean_absolute_error: 0.041150, mean_q: -0.143852\n",
      "  2471/50000: episode: 133, duration: 0.571s, episode steps: 45, steps per second: 79, episode reward: 3.400, mean reward: 0.076 [-1.000, 0.100], mean action: -0.133 [-3.763, 4.072], mean observation: 0.319 [-5.886, 4.000], loss: 0.001794, mean_absolute_error: 0.041153, mean_q: -0.138098\n",
      "  2495/50000: episode: 134, duration: 0.306s, episode steps: 24, steps per second: 78, episode reward: 1.300, mean reward: 0.054 [-1.000, 0.100], mean action: -0.111 [-2.740, 4.094], mean observation: 0.421 [-5.375, 4.000], loss: 0.002009, mean_absolute_error: 0.041903, mean_q: -0.153701\n",
      "  2517/50000: episode: 135, duration: 0.327s, episode steps: 22, steps per second: 67, episode reward: 1.100, mean reward: 0.050 [-1.000, 0.100], mean action: 0.162 [-1.656, 4.025], mean observation: 0.428 [-5.886, 4.000], loss: 0.002122, mean_absolute_error: 0.040909, mean_q: -0.148433\n",
      "  2539/50000: episode: 136, duration: 0.275s, episode steps: 22, steps per second: 80, episode reward: 1.100, mean reward: 0.050 [-1.000, 0.100], mean action: -0.408 [-4.145, 2.614], mean observation: 0.416 [-6.867, 4.000], loss: 0.001803, mean_absolute_error: 0.040019, mean_q: -0.105370\n",
      "  2558/50000: episode: 137, duration: 0.307s, episode steps: 19, steps per second: 62, episode reward: 0.800, mean reward: 0.042 [-1.000, 0.100], mean action: 0.232 [-1.374, 3.362], mean observation: 0.395 [-6.867, 4.000], loss: 0.002124, mean_absolute_error: 0.040972, mean_q: -0.129517\n",
      "  2574/50000: episode: 138, duration: 0.353s, episode steps: 16, steps per second: 45, episode reward: 0.500, mean reward: 0.031 [-1.000, 0.100], mean action: -0.136 [-1.718, 1.658], mean observation: 0.249 [-8.829, 4.000], loss: 0.001621, mean_absolute_error: 0.038005, mean_q: -0.138932\n",
      "  2588/50000: episode: 139, duration: 0.180s, episode steps: 14, steps per second: 78, episode reward: 0.300, mean reward: 0.021 [-1.000, 0.100], mean action: -1.016 [-4.947, 1.738], mean observation: -0.014 [-8.829, 4.000], loss: 0.001771, mean_absolute_error: 0.041496, mean_q: -0.114736\n",
      "  2605/50000: episode: 140, duration: 0.230s, episode steps: 17, steps per second: 74, episode reward: 0.600, mean reward: 0.035 [-1.000, 0.100], mean action: -0.400 [-2.625, 1.500], mean observation: 0.231 [-7.848, 4.000], loss: 0.001641, mean_absolute_error: 0.036707, mean_q: -0.114073\n",
      "  2625/50000: episode: 141, duration: 0.276s, episode steps: 20, steps per second: 72, episode reward: 0.900, mean reward: 0.045 [-1.000, 0.100], mean action: -0.642 [-5.088, 1.824], mean observation: 0.382 [-6.867, 4.000], loss: 0.001727, mean_absolute_error: 0.038219, mean_q: -0.102604\n",
      "  2641/50000: episode: 142, duration: 0.195s, episode steps: 16, steps per second: 82, episode reward: 0.500, mean reward: 0.031 [-1.000, 0.100], mean action: -0.430 [-2.907, 1.422], mean observation: 0.204 [-7.848, 4.000], loss: 0.002300, mean_absolute_error: 0.042929, mean_q: -0.106439\n",
      "  2658/50000: episode: 143, duration: 0.205s, episode steps: 17, steps per second: 83, episode reward: 0.600, mean reward: 0.035 [-1.000, 0.100], mean action: 1.017 [-1.602, 5.986], mean observation: 0.147 [-7.848, 4.000], loss: 0.001518, mean_absolute_error: 0.038821, mean_q: -0.096191\n",
      "  2676/50000: episode: 144, duration: 0.229s, episode steps: 18, steps per second: 79, episode reward: 0.700, mean reward: 0.039 [-1.000, 0.100], mean action: 0.697 [-1.810, 5.981], mean observation: 0.166 [-6.867, 4.000], loss: 0.002484, mean_absolute_error: 0.046152, mean_q: -0.143396\n",
      "  2690/50000: episode: 145, duration: 0.179s, episode steps: 14, steps per second: 78, episode reward: 0.300, mean reward: 0.021 [-1.000, 0.100], mean action: -0.580 [-4.580, 1.361], mean observation: 0.214 [-8.829, 4.000], loss: 0.001506, mean_absolute_error: 0.037462, mean_q: -0.112427\n",
      "  2704/50000: episode: 146, duration: 0.228s, episode steps: 14, steps per second: 61, episode reward: 0.300, mean reward: 0.021 [-1.000, 0.100], mean action: 1.040 [-1.724, 5.998], mean observation: 0.074 [-7.848, 4.000], loss: 0.001530, mean_absolute_error: 0.038538, mean_q: -0.121321\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  2744/50000: episode: 147, duration: 0.515s, episode steps: 40, steps per second: 78, episode reward: 2.900, mean reward: 0.073 [-1.000, 0.100], mean action: -0.018 [-2.356, 3.845], mean observation: 0.231 [-6.049, 4.000], loss: 0.001625, mean_absolute_error: 0.038019, mean_q: -0.141427\n",
      "  2760/50000: episode: 148, duration: 0.201s, episode steps: 16, steps per second: 80, episode reward: 0.500, mean reward: 0.031 [-1.000, 0.100], mean action: -0.312 [-2.560, 1.547], mean observation: 0.235 [-7.848, 4.000], loss: 0.001633, mean_absolute_error: 0.037217, mean_q: -0.121739\n",
      "  2777/50000: episode: 149, duration: 0.246s, episode steps: 17, steps per second: 69, episode reward: 0.600, mean reward: 0.035 [-1.000, 0.100], mean action: -0.094 [-2.028, 1.780], mean observation: 0.316 [-8.829, 4.000], loss: 0.001640, mean_absolute_error: 0.036327, mean_q: -0.114495\n",
      "  2795/50000: episode: 150, duration: 0.233s, episode steps: 18, steps per second: 77, episode reward: 0.700, mean reward: 0.039 [-1.000, 0.100], mean action: -0.473 [-2.597, 1.407], mean observation: 0.339 [-6.867, 4.000], loss: 0.001498, mean_absolute_error: 0.036801, mean_q: -0.110015\n",
      "  2811/50000: episode: 151, duration: 0.232s, episode steps: 16, steps per second: 69, episode reward: 0.500, mean reward: 0.031 [-1.000, 0.100], mean action: 0.599 [-1.056, 3.121], mean observation: 0.188 [-8.829, 4.000], loss: 0.001618, mean_absolute_error: 0.037777, mean_q: -0.124229\n",
      "  2827/50000: episode: 152, duration: 0.221s, episode steps: 16, steps per second: 73, episode reward: 0.500, mean reward: 0.031 [-1.000, 0.100], mean action: -0.555 [-2.674, 1.467], mean observation: 0.150 [-7.848, 4.000], loss: 0.001541, mean_absolute_error: 0.036926, mean_q: -0.098256\n",
      "  2842/50000: episode: 153, duration: 0.181s, episode steps: 15, steps per second: 83, episode reward: 0.400, mean reward: 0.027 [-1.000, 0.100], mean action: -0.104 [-2.176, 1.496], mean observation: 0.248 [-8.829, 4.000], loss: 0.001267, mean_absolute_error: 0.035122, mean_q: -0.100748\n",
      "  2858/50000: episode: 154, duration: 0.218s, episode steps: 16, steps per second: 73, episode reward: 0.500, mean reward: 0.031 [-1.000, 0.100], mean action: 0.674 [-1.534, 5.145], mean observation: 0.214 [-7.848, 4.000], loss: 0.001499, mean_absolute_error: 0.034795, mean_q: -0.101803\n",
      "  2875/50000: episode: 155, duration: 0.225s, episode steps: 17, steps per second: 76, episode reward: 0.600, mean reward: 0.035 [-1.000, 0.100], mean action: 0.939 [-1.646, 6.148], mean observation: 0.168 [-6.867, 4.000], loss: 0.001800, mean_absolute_error: 0.038576, mean_q: -0.124213\n",
      "  2891/50000: episode: 156, duration: 0.188s, episode steps: 16, steps per second: 85, episode reward: 0.500, mean reward: 0.031 [-1.000, 0.100], mean action: 0.949 [-1.778, 6.064], mean observation: 0.192 [-6.867, 4.000], loss: 0.001707, mean_absolute_error: 0.039836, mean_q: -0.137431\n",
      "  2909/50000: episode: 157, duration: 0.255s, episode steps: 18, steps per second: 71, episode reward: 0.700, mean reward: 0.039 [-1.000, 0.100], mean action: -0.430 [-2.888, 2.182], mean observation: 0.329 [-5.886, 4.000], loss: 0.001518, mean_absolute_error: 0.036332, mean_q: -0.107758\n",
      "  2928/50000: episode: 158, duration: 0.266s, episode steps: 19, steps per second: 71, episode reward: 0.800, mean reward: 0.042 [-1.000, 0.100], mean action: 0.583 [-1.687, 4.122], mean observation: 0.264 [-6.867, 4.000], loss: 0.001563, mean_absolute_error: 0.038863, mean_q: -0.129198\n",
      "  2942/50000: episode: 159, duration: 0.175s, episode steps: 14, steps per second: 80, episode reward: 0.300, mean reward: 0.021 [-1.000, 0.100], mean action: -0.833 [-4.361, 1.717], mean observation: -0.035 [-8.829, 4.000], loss: 0.001590, mean_absolute_error: 0.037982, mean_q: -0.111412\n",
      "  2970/50000: episode: 160, duration: 0.425s, episode steps: 28, steps per second: 66, episode reward: 1.700, mean reward: 0.061 [-1.000, 0.100], mean action: -0.317 [-5.114, 1.916], mean observation: 0.410 [-7.208, 4.000], loss: 0.001265, mean_absolute_error: 0.033996, mean_q: -0.090478\n",
      "  2988/50000: episode: 161, duration: 0.277s, episode steps: 18, steps per second: 65, episode reward: 0.700, mean reward: 0.039 [-1.000, 0.100], mean action: -0.280 [-2.321, 1.709], mean observation: 0.348 [-7.848, 4.000], loss: 0.001380, mean_absolute_error: 0.036685, mean_q: -0.104861\n",
      "  3008/50000: episode: 162, duration: 0.248s, episode steps: 20, steps per second: 81, episode reward: 0.900, mean reward: 0.045 [-1.000, 0.100], mean action: 0.265 [-1.221, 2.360], mean observation: 0.425 [-7.848, 4.000], loss: 0.001478, mean_absolute_error: 0.036184, mean_q: -0.084816\n",
      "  3025/50000: episode: 163, duration: 0.237s, episode steps: 17, steps per second: 72, episode reward: 0.600, mean reward: 0.035 [-1.000, 0.100], mean action: -0.499 [-2.453, 1.142], mean observation: 0.222 [-7.848, 4.000], loss: 0.001415, mean_absolute_error: 0.037375, mean_q: -0.086445\n",
      "  3041/50000: episode: 164, duration: 0.229s, episode steps: 16, steps per second: 70, episode reward: 0.500, mean reward: 0.031 [-1.000, 0.100], mean action: 0.587 [-0.980, 3.411], mean observation: 0.213 [-8.829, 4.000], loss: 0.001436, mean_absolute_error: 0.036333, mean_q: -0.125104\n",
      "  3071/50000: episode: 165, duration: 0.383s, episode steps: 30, steps per second: 78, episode reward: 1.900, mean reward: 0.063 [-1.000, 0.100], mean action: -0.244 [-3.413, 1.464], mean observation: 0.441 [-8.431, 4.000], loss: 0.001750, mean_absolute_error: 0.038156, mean_q: -0.092100\n",
      "  3099/50000: episode: 166, duration: 0.382s, episode steps: 28, steps per second: 73, episode reward: 1.700, mean reward: 0.061 [-1.000, 0.100], mean action: 0.041 [-3.381, 5.268], mean observation: 0.473 [-4.905, 4.000], loss: 0.001412, mean_absolute_error: 0.035421, mean_q: -0.086368\n",
      "  3119/50000: episode: 167, duration: 0.288s, episode steps: 20, steps per second: 70, episode reward: 0.900, mean reward: 0.045 [-1.000, 0.100], mean action: 0.594 [-1.181, 5.401], mean observation: 0.168 [-5.886, 4.000], loss: 0.001068, mean_absolute_error: 0.032163, mean_q: -0.063192\n",
      "  3134/50000: episode: 168, duration: 0.194s, episode steps: 15, steps per second: 77, episode reward: 0.400, mean reward: 0.027 [-1.000, 0.100], mean action: 0.642 [-1.092, 3.739], mean observation: 0.234 [-8.829, 4.000], loss: 0.001375, mean_absolute_error: 0.036340, mean_q: -0.103688\n",
      "  3150/50000: episode: 169, duration: 0.198s, episode steps: 16, steps per second: 81, episode reward: 0.500, mean reward: 0.031 [-1.000, 0.100], mean action: 0.182 [-1.215, 2.059], mean observation: 0.212 [-7.848, 4.000], loss: 0.001673, mean_absolute_error: 0.038711, mean_q: -0.097393\n",
      "  3182/50000: episode: 170, duration: 0.428s, episode steps: 32, steps per second: 75, episode reward: 2.100, mean reward: 0.066 [-1.000, 0.100], mean action: -0.394 [-6.043, 2.999], mean observation: 0.420 [-6.867, 4.000], loss: 0.002025, mean_absolute_error: 0.043745, mean_q: -0.095750\n",
      "  3198/50000: episode: 171, duration: 0.194s, episode steps: 16, steps per second: 83, episode reward: 0.500, mean reward: 0.031 [-1.000, 0.100], mean action: -0.515 [-2.524, 1.192], mean observation: 0.041 [-7.848, 4.000], loss: 0.001394, mean_absolute_error: 0.036704, mean_q: -0.070858\n",
      "  3220/50000: episode: 172, duration: 0.273s, episode steps: 22, steps per second: 80, episode reward: 1.100, mean reward: 0.050 [-1.000, 0.100], mean action: 0.785 [-3.018, 4.829], mean observation: 0.234 [-7.848, 4.000], loss: 0.001501, mean_absolute_error: 0.036730, mean_q: -0.098857\n",
      "  3234/50000: episode: 173, duration: 0.183s, episode steps: 14, steps per second: 77, episode reward: 0.300, mean reward: 0.021 [-1.000, 0.100], mean action: -0.205 [-5.495, 2.217], mean observation: 0.289 [-7.848, 4.000], loss: 0.001331, mean_absolute_error: 0.034403, mean_q: -0.077919\n",
      "  3272/50000: episode: 174, duration: 0.454s, episode steps: 38, steps per second: 84, episode reward: 2.700, mean reward: 0.071 [-1.000, 0.100], mean action: -0.708 [-6.976, 3.628], mean observation: 0.022 [-5.886, 4.000], loss: 0.001387, mean_absolute_error: 0.036910, mean_q: -0.090674\n",
      "  3290/50000: episode: 175, duration: 0.209s, episode steps: 18, steps per second: 86, episode reward: 0.700, mean reward: 0.039 [-1.000, 0.100], mean action: -0.052 [-3.811, 1.957], mean observation: 0.218 [-7.848, 4.000], loss: 0.001469, mean_absolute_error: 0.036497, mean_q: -0.061546\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  3319/50000: episode: 176, duration: 0.376s, episode steps: 29, steps per second: 77, episode reward: 1.800, mean reward: 0.062 [-1.000, 0.100], mean action: -0.970 [-5.917, 1.428], mean observation: 0.006 [-7.848, 4.000], loss: 0.001603, mean_absolute_error: 0.037435, mean_q: -0.075260\n",
      "  3335/50000: episode: 177, duration: 0.195s, episode steps: 16, steps per second: 82, episode reward: 0.500, mean reward: 0.031 [-1.000, 0.100], mean action: 0.393 [-0.925, 3.074], mean observation: 0.219 [-8.829, 4.000], loss: 0.001898, mean_absolute_error: 0.041823, mean_q: -0.086818\n",
      "  3349/50000: episode: 178, duration: 0.194s, episode steps: 14, steps per second: 72, episode reward: 0.300, mean reward: 0.021 [-1.000, 0.100], mean action: -0.238 [-2.613, 1.810], mean observation: 0.124 [-8.829, 4.000], loss: 0.002138, mean_absolute_error: 0.041991, mean_q: -0.103798\n",
      "  3370/50000: episode: 179, duration: 0.285s, episode steps: 21, steps per second: 74, episode reward: 1.000, mean reward: 0.048 [-1.000, 0.100], mean action: 0.764 [-1.250, 4.364], mean observation: 0.431 [-7.848, 4.000], loss: 0.001505, mean_absolute_error: 0.037622, mean_q: -0.049972\n",
      "  3386/50000: episode: 180, duration: 0.233s, episode steps: 16, steps per second: 69, episode reward: 0.500, mean reward: 0.031 [-1.000, 0.100], mean action: 0.856 [-1.816, 5.782], mean observation: 0.243 [-7.848, 4.000], loss: 0.001677, mean_absolute_error: 0.039194, mean_q: -0.082992\n",
      "  3402/50000: episode: 181, duration: 0.214s, episode steps: 16, steps per second: 75, episode reward: 0.500, mean reward: 0.031 [-1.000, 0.100], mean action: 0.345 [-2.830, 1.583], mean observation: 0.207 [-7.848, 4.000], loss: 0.001649, mean_absolute_error: 0.037129, mean_q: -0.069334\n",
      "  3419/50000: episode: 182, duration: 0.226s, episode steps: 17, steps per second: 75, episode reward: 0.600, mean reward: 0.035 [-1.000, 0.100], mean action: -0.209 [-1.623, 1.276], mean observation: 0.197 [-7.848, 4.000], loss: 0.002109, mean_absolute_error: 0.044642, mean_q: -0.049282\n",
      "  3439/50000: episode: 183, duration: 0.295s, episode steps: 20, steps per second: 68, episode reward: 0.900, mean reward: 0.045 [-1.000, 0.100], mean action: 0.818 [-3.417, 4.710], mean observation: 0.288 [-7.848, 4.000], loss: 0.002010, mean_absolute_error: 0.042064, mean_q: -0.040972\n",
      "  3455/50000: episode: 184, duration: 0.211s, episode steps: 16, steps per second: 76, episode reward: 0.500, mean reward: 0.031 [-1.000, 0.100], mean action: -0.962 [-3.536, 0.680], mean observation: -0.121 [-8.829, 4.000], loss: 0.002583, mean_absolute_error: 0.041307, mean_q: -0.074694\n",
      "  3474/50000: episode: 185, duration: 0.238s, episode steps: 19, steps per second: 80, episode reward: 0.800, mean reward: 0.042 [-1.000, 0.100], mean action: 1.027 [-2.675, 4.782], mean observation: 0.178 [-7.848, 4.000], loss: 0.001639, mean_absolute_error: 0.037914, mean_q: -0.066311\n",
      "  3493/50000: episode: 186, duration: 0.265s, episode steps: 19, steps per second: 72, episode reward: 0.800, mean reward: 0.042 [-1.000, 0.100], mean action: 0.589 [-1.314, 4.320], mean observation: 0.427 [-7.848, 4.000], loss: 0.001606, mean_absolute_error: 0.037249, mean_q: -0.056766\n",
      "  3510/50000: episode: 187, duration: 0.250s, episode steps: 17, steps per second: 68, episode reward: 0.600, mean reward: 0.035 [-1.000, 0.100], mean action: -0.261 [-2.335, 1.459], mean observation: 0.214 [-7.848, 4.000], loss: 0.001430, mean_absolute_error: 0.037882, mean_q: -0.081240\n",
      "  3526/50000: episode: 188, duration: 0.198s, episode steps: 16, steps per second: 81, episode reward: 0.500, mean reward: 0.031 [-1.000, 0.100], mean action: -0.394 [-4.823, 2.741], mean observation: 0.435 [-7.848, 4.000], loss: 0.001490, mean_absolute_error: 0.039156, mean_q: -0.075311\n",
      "  3549/50000: episode: 189, duration: 0.320s, episode steps: 23, steps per second: 72, episode reward: 1.200, mean reward: 0.052 [-1.000, 0.100], mean action: -0.234 [-1.993, 1.961], mean observation: 0.337 [-6.867, 4.000], loss: 0.001789, mean_absolute_error: 0.042575, mean_q: -0.056711\n",
      "  3567/50000: episode: 190, duration: 0.244s, episode steps: 18, steps per second: 74, episode reward: 0.700, mean reward: 0.039 [-1.000, 0.100], mean action: 0.424 [-1.663, 4.406], mean observation: 0.198 [-7.848, 4.000], loss: 0.002680, mean_absolute_error: 0.040985, mean_q: -0.085377\n",
      "  3586/50000: episode: 191, duration: 0.286s, episode steps: 19, steps per second: 66, episode reward: 0.800, mean reward: 0.042 [-1.000, 0.100], mean action: 0.169 [-3.346, 2.245], mean observation: 0.294 [-7.848, 4.000], loss: 0.002506, mean_absolute_error: 0.043806, mean_q: -0.057029\n",
      "  3601/50000: episode: 192, duration: 0.187s, episode steps: 15, steps per second: 80, episode reward: 0.400, mean reward: 0.027 [-1.000, 0.100], mean action: 0.664 [-1.196, 5.151], mean observation: 0.155 [-8.829, 4.000], loss: 0.001502, mean_absolute_error: 0.037332, mean_q: -0.063986\n",
      "  3618/50000: episode: 193, duration: 0.243s, episode steps: 17, steps per second: 70, episode reward: 0.600, mean reward: 0.035 [-1.000, 0.100], mean action: -1.096 [-7.408, 2.353], mean observation: 0.417 [-7.848, 4.000], loss: 0.001601, mean_absolute_error: 0.037838, mean_q: -0.052371\n",
      "  3632/50000: episode: 194, duration: 0.186s, episode steps: 14, steps per second: 75, episode reward: 0.300, mean reward: 0.021 [-1.000, 0.100], mean action: 0.481 [-2.517, 2.921], mean observation: 0.202 [-8.829, 4.000], loss: 0.001245, mean_absolute_error: 0.034069, mean_q: -0.010972\n",
      "  3646/50000: episode: 195, duration: 0.195s, episode steps: 14, steps per second: 72, episode reward: 0.300, mean reward: 0.021 [-1.000, 0.100], mean action: 0.596 [-0.652, 3.509], mean observation: 0.239 [-8.829, 4.000], loss: 0.002304, mean_absolute_error: 0.040434, mean_q: -0.072485\n",
      "  3675/50000: episode: 196, duration: 0.387s, episode steps: 29, steps per second: 75, episode reward: 1.800, mean reward: 0.062 [-1.000, 0.100], mean action: -0.268 [-5.454, 2.950], mean observation: 0.372 [-6.867, 4.000], loss: 0.001964, mean_absolute_error: 0.041118, mean_q: -0.070341\n",
      "  3691/50000: episode: 197, duration: 0.216s, episode steps: 16, steps per second: 74, episode reward: 0.500, mean reward: 0.031 [-1.000, 0.100], mean action: 0.727 [-1.832, 5.762], mean observation: 0.087 [-7.848, 4.000], loss: 0.001444, mean_absolute_error: 0.036175, mean_q: -0.056346\n",
      "  3728/50000: episode: 198, duration: 0.518s, episode steps: 37, steps per second: 71, episode reward: 2.600, mean reward: 0.070 [-1.000, 0.100], mean action: -0.081 [-6.704, 4.195], mean observation: 0.318 [-6.867, 4.000], loss: 0.001427, mean_absolute_error: 0.034640, mean_q: -0.056959\n",
      "  3756/50000: episode: 199, duration: 0.331s, episode steps: 28, steps per second: 85, episode reward: 1.700, mean reward: 0.061 [-1.000, 0.100], mean action: 0.582 [-2.121, 3.944], mean observation: 0.338 [-6.867, 4.000], loss: 0.001409, mean_absolute_error: 0.037811, mean_q: -0.062816\n",
      "  3771/50000: episode: 200, duration: 0.201s, episode steps: 15, steps per second: 75, episode reward: 0.400, mean reward: 0.027 [-1.000, 0.100], mean action: 0.702 [-1.199, 4.829], mean observation: 0.166 [-7.848, 4.000], loss: 0.001391, mean_absolute_error: 0.038756, mean_q: -0.047435\n",
      "  3792/50000: episode: 201, duration: 0.269s, episode steps: 21, steps per second: 78, episode reward: 1.000, mean reward: 0.048 [-1.000, 0.100], mean action: 0.821 [-2.785, 4.822], mean observation: 0.312 [-7.848, 4.000], loss: 0.001359, mean_absolute_error: 0.035455, mean_q: -0.051368\n",
      "  3825/50000: episode: 202, duration: 0.397s, episode steps: 33, steps per second: 83, episode reward: 2.200, mean reward: 0.067 [-1.000, 0.100], mean action: 0.374 [-2.077, 3.069], mean observation: 0.442 [-6.893, 4.000], loss: 0.001372, mean_absolute_error: 0.036167, mean_q: -0.052407\n",
      "  3844/50000: episode: 203, duration: 0.230s, episode steps: 19, steps per second: 83, episode reward: 0.800, mean reward: 0.042 [-1.000, 0.100], mean action: -0.486 [-2.188, 1.205], mean observation: 0.139 [-7.848, 4.000], loss: 0.001229, mean_absolute_error: 0.034548, mean_q: -0.024405\n",
      "  3861/50000: episode: 204, duration: 0.248s, episode steps: 17, steps per second: 69, episode reward: 0.600, mean reward: 0.035 [-1.000, 0.100], mean action: 0.412 [-0.745, 2.818], mean observation: 0.242 [-8.829, 4.000], loss: 0.001564, mean_absolute_error: 0.036955, mean_q: -0.026734\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  3879/50000: episode: 205, duration: 0.214s, episode steps: 18, steps per second: 84, episode reward: 0.700, mean reward: 0.039 [-1.000, 0.100], mean action: 0.132 [-1.945, 2.461], mean observation: 0.299 [-6.867, 4.000], loss: 0.001928, mean_absolute_error: 0.037160, mean_q: -0.049734\n",
      "  3897/50000: episode: 206, duration: 0.248s, episode steps: 18, steps per second: 73, episode reward: 0.700, mean reward: 0.039 [-1.000, 0.100], mean action: 0.625 [-1.166, 3.508], mean observation: 0.407 [-7.848, 4.000], loss: 0.002177, mean_absolute_error: 0.046302, mean_q: -0.066154\n",
      "  3921/50000: episode: 207, duration: 0.290s, episode steps: 24, steps per second: 83, episode reward: 1.300, mean reward: 0.054 [-1.000, 0.100], mean action: 0.460 [-1.595, 4.111], mean observation: 0.369 [-6.867, 4.000], loss: 0.001670, mean_absolute_error: 0.042444, mean_q: -0.019599\n",
      "  3936/50000: episode: 208, duration: 0.200s, episode steps: 15, steps per second: 75, episode reward: 0.400, mean reward: 0.027 [-1.000, 0.100], mean action: -0.234 [-2.194, 1.254], mean observation: 0.170 [-7.848, 4.000], loss: 0.001909, mean_absolute_error: 0.041461, mean_q: -0.062483\n",
      "  3955/50000: episode: 209, duration: 0.232s, episode steps: 19, steps per second: 82, episode reward: 0.800, mean reward: 0.042 [-1.000, 0.100], mean action: 0.559 [-1.657, 4.146], mean observation: 0.183 [-6.867, 4.000], loss: 0.001905, mean_absolute_error: 0.042516, mean_q: -0.010469\n",
      "  3973/50000: episode: 210, duration: 0.207s, episode steps: 18, steps per second: 87, episode reward: 0.700, mean reward: 0.039 [-1.000, 0.100], mean action: 0.333 [-1.728, 3.210], mean observation: 0.211 [-6.867, 4.000], loss: 0.001704, mean_absolute_error: 0.040778, mean_q: -0.069877\n",
      "  4013/50000: episode: 211, duration: 0.485s, episode steps: 40, steps per second: 82, episode reward: 2.900, mean reward: 0.073 [-1.000, 0.100], mean action: -0.876 [-7.247, 3.088], mean observation: 0.084 [-5.011, 4.000], loss: 0.001599, mean_absolute_error: 0.039678, mean_q: -0.028777\n",
      "  4042/50000: episode: 212, duration: 0.382s, episode steps: 29, steps per second: 76, episode reward: 1.800, mean reward: 0.062 [-1.000, 0.100], mean action: -0.113 [-3.371, 2.612], mean observation: 0.369 [-6.351, 4.000], loss: 0.001458, mean_absolute_error: 0.038041, mean_q: -0.030144\n",
      "  4059/50000: episode: 213, duration: 0.251s, episode steps: 17, steps per second: 68, episode reward: 0.600, mean reward: 0.035 [-1.000, 0.100], mean action: 0.372 [-1.800, 3.826], mean observation: 0.187 [-6.867, 4.000], loss: 0.001301, mean_absolute_error: 0.035858, mean_q: -0.063589\n",
      "  4076/50000: episode: 214, duration: 0.225s, episode steps: 17, steps per second: 76, episode reward: 0.600, mean reward: 0.035 [-1.000, 0.100], mean action: 0.194 [-1.769, 2.898], mean observation: 0.178 [-6.867, 4.000], loss: 0.001391, mean_absolute_error: 0.037658, mean_q: -0.023138\n",
      "  4091/50000: episode: 215, duration: 0.217s, episode steps: 15, steps per second: 69, episode reward: 0.400, mean reward: 0.027 [-1.000, 0.100], mean action: -0.922 [-2.980, 1.161], mean observation: -0.157 [-8.829, 4.000], loss: 0.001455, mean_absolute_error: 0.036116, mean_q: -0.042408\n",
      "  4113/50000: episode: 216, duration: 0.264s, episode steps: 22, steps per second: 83, episode reward: 1.100, mean reward: 0.050 [-1.000, 0.100], mean action: -0.153 [-2.182, 2.119], mean observation: 0.311 [-5.886, 4.000], loss: 0.001515, mean_absolute_error: 0.037247, mean_q: -0.020863\n",
      "  4129/50000: episode: 217, duration: 0.200s, episode steps: 16, steps per second: 80, episode reward: 0.500, mean reward: 0.031 [-1.000, 0.100], mean action: -0.749 [-4.164, 1.438], mean observation: -0.129 [-7.848, 4.000], loss: 0.001176, mean_absolute_error: 0.033919, mean_q: -0.006834\n",
      "  4173/50000: episode: 218, duration: 0.615s, episode steps: 44, steps per second: 72, episode reward: 3.300, mean reward: 0.075 [-1.000, 0.100], mean action: -0.969 [-7.468, 2.565], mean observation: 0.020 [-4.905, 4.000], loss: 0.001406, mean_absolute_error: 0.036923, mean_q: -0.050808\n",
      "  4200/50000: episode: 219, duration: 0.366s, episode steps: 27, steps per second: 74, episode reward: 1.600, mean reward: 0.059 [-1.000, 0.100], mean action: -0.155 [-3.515, 2.022], mean observation: 0.168 [-6.867, 4.000], loss: 0.001326, mean_absolute_error: 0.035494, mean_q: -0.006996\n",
      "  4216/50000: episode: 220, duration: 0.195s, episode steps: 16, steps per second: 82, episode reward: 0.500, mean reward: 0.031 [-1.000, 0.100], mean action: 0.158 [-1.896, 3.503], mean observation: 0.193 [-6.867, 4.000], loss: 0.001484, mean_absolute_error: 0.037339, mean_q: 0.015343\n",
      "  4238/50000: episode: 221, duration: 0.308s, episode steps: 22, steps per second: 71, episode reward: 1.100, mean reward: 0.050 [-1.000, 0.100], mean action: -0.228 [-2.179, 1.395], mean observation: 0.390 [-6.867, 4.000], loss: 0.001971, mean_absolute_error: 0.039874, mean_q: -0.017493\n",
      "  4258/50000: episode: 222, duration: 0.282s, episode steps: 20, steps per second: 71, episode reward: 0.900, mean reward: 0.045 [-1.000, 0.100], mean action: 0.256 [-1.803, 3.491], mean observation: 0.255 [-6.867, 4.000], loss: 0.002105, mean_absolute_error: 0.040437, mean_q: 0.007606\n",
      "  4277/50000: episode: 223, duration: 0.256s, episode steps: 19, steps per second: 74, episode reward: 0.800, mean reward: 0.042 [-1.000, 0.100], mean action: -0.933 [-8.157, 1.963], mean observation: 0.433 [-7.848, 4.000], loss: 0.002028, mean_absolute_error: 0.043936, mean_q: -0.040822\n",
      "  4306/50000: episode: 224, duration: 0.376s, episode steps: 29, steps per second: 77, episode reward: 1.800, mean reward: 0.062 [-1.000, 0.100], mean action: 0.125 [-2.163, 2.638], mean observation: 0.514 [-5.886, 4.000], loss: 0.002023, mean_absolute_error: 0.043744, mean_q: -0.026528\n",
      "  4322/50000: episode: 225, duration: 0.215s, episode steps: 16, steps per second: 75, episode reward: 0.500, mean reward: 0.031 [-1.000, 0.100], mean action: 0.411 [-1.764, 4.289], mean observation: 0.117 [-7.848, 4.000], loss: 0.001863, mean_absolute_error: 0.037778, mean_q: -0.020799\n",
      "  4339/50000: episode: 226, duration: 0.217s, episode steps: 17, steps per second: 78, episode reward: 0.600, mean reward: 0.035 [-1.000, 0.100], mean action: 0.140 [-3.270, 3.171], mean observation: 0.423 [-7.848, 4.000], loss: 0.001567, mean_absolute_error: 0.038014, mean_q: -0.020389\n",
      "  4365/50000: episode: 227, duration: 0.311s, episode steps: 26, steps per second: 84, episode reward: 1.500, mean reward: 0.058 [-1.000, 0.100], mean action: 0.142 [-1.818, 2.573], mean observation: 0.422 [-5.886, 4.000], loss: 0.002043, mean_absolute_error: 0.040548, mean_q: 0.008588\n",
      "  4393/50000: episode: 228, duration: 0.381s, episode steps: 28, steps per second: 74, episode reward: 1.700, mean reward: 0.061 [-1.000, 0.100], mean action: 0.342 [-3.376, 2.766], mean observation: 0.381 [-7.848, 4.000], loss: 0.001501, mean_absolute_error: 0.035801, mean_q: 0.027698\n",
      "  4430/50000: episode: 229, duration: 0.520s, episode steps: 37, steps per second: 71, episode reward: 2.600, mean reward: 0.070 [-1.000, 0.100], mean action: -0.190 [-5.919, 2.714], mean observation: 0.308 [-7.848, 4.000], loss: 0.002069, mean_absolute_error: 0.039385, mean_q: -0.000467\n",
      "  4448/50000: episode: 230, duration: 0.252s, episode steps: 18, steps per second: 72, episode reward: 0.700, mean reward: 0.039 [-1.000, 0.100], mean action: 0.231 [-1.291, 2.974], mean observation: 0.284 [-6.867, 4.000], loss: 0.001827, mean_absolute_error: 0.042211, mean_q: -0.006931\n",
      "  4475/50000: episode: 231, duration: 0.324s, episode steps: 27, steps per second: 83, episode reward: 1.600, mean reward: 0.059 [-1.000, 0.100], mean action: -0.087 [-2.363, 2.206], mean observation: 0.419 [-5.353, 4.000], loss: 0.001567, mean_absolute_error: 0.038828, mean_q: 0.002990\n",
      "  4491/50000: episode: 232, duration: 0.211s, episode steps: 16, steps per second: 76, episode reward: 0.500, mean reward: 0.031 [-1.000, 0.100], mean action: -0.161 [-2.174, 1.373], mean observation: 0.155 [-7.848, 4.000], loss: 0.001468, mean_absolute_error: 0.036051, mean_q: -0.016578\n",
      "  4509/50000: episode: 233, duration: 0.233s, episode steps: 18, steps per second: 77, episode reward: 0.700, mean reward: 0.039 [-1.000, 0.100], mean action: 0.267 [-1.324, 2.583], mean observation: 0.228 [-7.848, 4.000], loss: 0.001817, mean_absolute_error: 0.039261, mean_q: 0.015847\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  4526/50000: episode: 234, duration: 0.217s, episode steps: 17, steps per second: 78, episode reward: 0.600, mean reward: 0.035 [-1.000, 0.100], mean action: 0.294 [-1.442, 2.670], mean observation: 0.147 [-7.848, 4.000], loss: 0.001570, mean_absolute_error: 0.038816, mean_q: 0.028305\n",
      "  4541/50000: episode: 235, duration: 0.188s, episode steps: 15, steps per second: 80, episode reward: 0.400, mean reward: 0.027 [-1.000, 0.100], mean action: 0.211 [-1.289, 2.757], mean observation: 0.135 [-7.848, 4.000], loss: 0.002073, mean_absolute_error: 0.041216, mean_q: 0.033524\n",
      "  4556/50000: episode: 236, duration: 0.180s, episode steps: 15, steps per second: 83, episode reward: 0.400, mean reward: 0.027 [-1.000, 0.100], mean action: 0.205 [-1.209, 2.219], mean observation: 0.179 [-7.848, 4.000], loss: 0.001875, mean_absolute_error: 0.040248, mean_q: 0.026000\n",
      "  4573/50000: episode: 237, duration: 0.240s, episode steps: 17, steps per second: 71, episode reward: 0.600, mean reward: 0.035 [-1.000, 0.100], mean action: 0.089 [-2.455, 2.634], mean observation: 0.390 [-6.867, 4.000], loss: 0.001928, mean_absolute_error: 0.038328, mean_q: 0.023396\n",
      "  4589/50000: episode: 238, duration: 0.195s, episode steps: 16, steps per second: 82, episode reward: 0.500, mean reward: 0.031 [-1.000, 0.100], mean action: -0.011 [-1.722, 2.909], mean observation: 0.104 [-7.848, 4.000], loss: 0.001175, mean_absolute_error: 0.035798, mean_q: 0.043480\n",
      "  4614/50000: episode: 239, duration: 0.314s, episode steps: 25, steps per second: 80, episode reward: 1.400, mean reward: 0.056 [-1.000, 0.100], mean action: 0.296 [-2.960, 2.115], mean observation: 0.463 [-7.848, 4.000], loss: 0.001596, mean_absolute_error: 0.037963, mean_q: -0.013582\n",
      "  4637/50000: episode: 240, duration: 0.334s, episode steps: 23, steps per second: 69, episode reward: 1.200, mean reward: 0.052 [-1.000, 0.100], mean action: 0.174 [-2.502, 1.987], mean observation: 0.443 [-7.848, 4.000], loss: 0.001530, mean_absolute_error: 0.036255, mean_q: 0.059636\n",
      "  4678/50000: episode: 241, duration: 0.576s, episode steps: 41, steps per second: 71, episode reward: 3.000, mean reward: 0.073 [-1.000, 0.100], mean action: -1.006 [-7.031, 1.952], mean observation: -0.037 [-6.867, 4.000], loss: 0.001350, mean_absolute_error: 0.035951, mean_q: 0.023682\n",
      "  4703/50000: episode: 242, duration: 0.318s, episode steps: 25, steps per second: 79, episode reward: 1.400, mean reward: 0.056 [-1.000, 0.100], mean action: 0.304 [-4.346, 2.781], mean observation: 0.399 [-7.848, 4.000], loss: 0.001304, mean_absolute_error: 0.034049, mean_q: 0.040464\n",
      "  4721/50000: episode: 243, duration: 0.236s, episode steps: 18, steps per second: 76, episode reward: 0.700, mean reward: 0.039 [-1.000, 0.100], mean action: 0.397 [-1.150, 2.635], mean observation: 0.281 [-7.848, 4.000], loss: 0.001966, mean_absolute_error: 0.038855, mean_q: 0.016280\n",
      "  4752/50000: episode: 244, duration: 0.393s, episode steps: 31, steps per second: 79, episode reward: 2.000, mean reward: 0.065 [-1.000, 0.100], mean action: 0.164 [-3.357, 2.226], mean observation: 0.385 [-6.867, 4.000], loss: 0.001447, mean_absolute_error: 0.034925, mean_q: 0.061283\n",
      "  4781/50000: episode: 245, duration: 0.358s, episode steps: 29, steps per second: 81, episode reward: 1.800, mean reward: 0.062 [-1.000, 0.100], mean action: 0.379 [-1.633, 2.419], mean observation: 0.485 [-7.531, 4.000], loss: 0.002085, mean_absolute_error: 0.042708, mean_q: 0.059479\n",
      "  4810/50000: episode: 246, duration: 0.405s, episode steps: 29, steps per second: 72, episode reward: 1.800, mean reward: 0.062 [-1.000, 0.100], mean action: 0.541 [-1.697, 2.835], mean observation: 0.382 [-8.582, 4.000], loss: 0.001690, mean_absolute_error: 0.038280, mean_q: 0.033659\n",
      "  4842/50000: episode: 247, duration: 0.461s, episode steps: 32, steps per second: 69, episode reward: 2.100, mean reward: 0.066 [-1.000, 0.100], mean action: 0.405 [-2.369, 3.061], mean observation: 0.371 [-8.842, 4.000], loss: 0.001671, mean_absolute_error: 0.037736, mean_q: 0.011285\n",
      "  4869/50000: episode: 248, duration: 17.935s, episode steps: 27, steps per second: 2, episode reward: 1.600, mean reward: 0.059 [-1.000, 0.100], mean action: 0.523 [-1.994, 2.645], mean observation: 0.445 [-6.867, 4.000], loss: 0.001376, mean_absolute_error: 0.035070, mean_q: 0.066703\n",
      "  4885/50000: episode: 249, duration: 0.367s, episode steps: 16, steps per second: 44, episode reward: 0.500, mean reward: 0.031 [-1.000, 0.100], mean action: 0.464 [-1.299, 4.264], mean observation: 0.170 [-7.848, 4.000], loss: 0.001759, mean_absolute_error: 0.037869, mean_q: 0.043099\n",
      "  4899/50000: episode: 250, duration: 0.187s, episode steps: 14, steps per second: 75, episode reward: 0.300, mean reward: 0.021 [-1.000, 0.100], mean action: 0.651 [-1.355, 5.227], mean observation: 0.131 [-8.829, 4.000], loss: 0.001149, mean_absolute_error: 0.034533, mean_q: 0.034243\n",
      "  4950/50000: episode: 251, duration: 0.632s, episode steps: 51, steps per second: 81, episode reward: 4.000, mean reward: 0.078 [-1.000, 0.100], mean action: -0.012 [-2.010, 2.699], mean observation: 0.341 [-4.905, 4.000], loss: 0.001467, mean_absolute_error: 0.035267, mean_q: 0.046011\n",
      "  4968/50000: episode: 252, duration: 0.226s, episode steps: 18, steps per second: 80, episode reward: 0.700, mean reward: 0.039 [-1.000, 0.100], mean action: 0.093 [-1.572, 3.127], mean observation: 0.299 [-7.848, 4.000], loss: 0.001672, mean_absolute_error: 0.038344, mean_q: 0.076693\n",
      "  4983/50000: episode: 253, duration: 0.188s, episode steps: 15, steps per second: 80, episode reward: 0.400, mean reward: 0.027 [-1.000, 0.100], mean action: 0.483 [-2.539, 3.919], mean observation: 0.278 [-7.848, 4.000], loss: 0.001963, mean_absolute_error: 0.042897, mean_q: 0.052876\n",
      "  5049/50000: episode: 254, duration: 0.778s, episode steps: 66, steps per second: 85, episode reward: 5.500, mean reward: 0.083 [-1.000, 0.100], mean action: -0.096 [-2.732, 1.730], mean observation: 0.168 [-6.867, 4.000], loss: 0.001614, mean_absolute_error: 0.039222, mean_q: 0.053584\n",
      "  5083/50000: episode: 255, duration: 0.458s, episode steps: 34, steps per second: 74, episode reward: 2.300, mean reward: 0.068 [-1.000, 0.100], mean action: -0.305 [-4.622, 2.199], mean observation: 0.216 [-8.799, 4.000], loss: 0.001493, mean_absolute_error: 0.037986, mean_q: 0.081906\n",
      "  5118/50000: episode: 256, duration: 0.413s, episode steps: 35, steps per second: 85, episode reward: 2.400, mean reward: 0.069 [-1.000, 0.100], mean action: -0.030 [-2.792, 2.190], mean observation: 0.291 [-6.867, 4.000], loss: 0.001293, mean_absolute_error: 0.035050, mean_q: 0.062437\n",
      "  5136/50000: episode: 257, duration: 0.236s, episode steps: 18, steps per second: 76, episode reward: 0.700, mean reward: 0.039 [-1.000, 0.100], mean action: 0.055 [-1.611, 2.563], mean observation: 0.216 [-7.848, 4.000], loss: 0.001350, mean_absolute_error: 0.034861, mean_q: 0.082742\n",
      "  5166/50000: episode: 258, duration: 0.384s, episode steps: 30, steps per second: 78, episode reward: 1.900, mean reward: 0.063 [-1.000, 0.100], mean action: 0.123 [-2.588, 1.960], mean observation: 0.368 [-7.848, 4.000], loss: 0.001673, mean_absolute_error: 0.040654, mean_q: 0.045851\n",
      "  5183/50000: episode: 259, duration: 0.203s, episode steps: 17, steps per second: 84, episode reward: 0.600, mean reward: 0.035 [-1.000, 0.100], mean action: -0.074 [-4.837, 3.607], mean observation: 0.143 [-7.848, 4.000], loss: 0.001415, mean_absolute_error: 0.036100, mean_q: 0.050451\n",
      "  5198/50000: episode: 260, duration: 0.188s, episode steps: 15, steps per second: 80, episode reward: 0.400, mean reward: 0.027 [-1.000, 0.100], mean action: -0.123 [-4.220, 3.210], mean observation: 0.216 [-7.848, 4.000], loss: 0.001646, mean_absolute_error: 0.039760, mean_q: 0.055148\n",
      "  5244/50000: episode: 261, duration: 0.557s, episode steps: 46, steps per second: 83, episode reward: 3.500, mean reward: 0.076 [-1.000, 0.100], mean action: -0.312 [-4.470, 1.779], mean observation: 0.230 [-7.848, 4.000], loss: 0.001479, mean_absolute_error: 0.036831, mean_q: 0.056065\n",
      "  5268/50000: episode: 262, duration: 0.294s, episode steps: 24, steps per second: 82, episode reward: 1.300, mean reward: 0.054 [-1.000, 0.100], mean action: 0.168 [-3.179, 1.982], mean observation: 0.462 [-7.848, 4.000], loss: 0.001393, mean_absolute_error: 0.035229, mean_q: 0.070204\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  5310/50000: episode: 263, duration: 0.502s, episode steps: 42, steps per second: 84, episode reward: 3.100, mean reward: 0.074 [-1.000, 0.100], mean action: -0.700 [-5.396, 1.932], mean observation: 0.021 [-6.867, 4.000], loss: 0.001406, mean_absolute_error: 0.037123, mean_q: 0.083875\n",
      "  5335/50000: episode: 264, duration: 0.307s, episode steps: 25, steps per second: 81, episode reward: 1.400, mean reward: 0.056 [-1.000, 0.100], mean action: 0.207 [-1.948, 2.688], mean observation: 0.492 [-7.848, 4.000], loss: 0.001425, mean_absolute_error: 0.036631, mean_q: 0.075698\n",
      "  5361/50000: episode: 265, duration: 0.307s, episode steps: 26, steps per second: 85, episode reward: 1.500, mean reward: 0.058 [-1.000, 0.100], mean action: -0.232 [-3.469, 1.384], mean observation: 0.319 [-7.848, 4.000], loss: 0.001157, mean_absolute_error: 0.034817, mean_q: 0.071092\n",
      "  5421/50000: episode: 266, duration: 0.720s, episode steps: 60, steps per second: 83, episode reward: 4.900, mean reward: 0.082 [-1.000, 0.100], mean action: 0.054 [-1.978, 1.770], mean observation: 0.412 [-6.867, 4.000], loss: 0.001383, mean_absolute_error: 0.035771, mean_q: 0.087237\n",
      "  5449/50000: episode: 267, duration: 0.330s, episode steps: 28, steps per second: 85, episode reward: 1.700, mean reward: 0.061 [-1.000, 0.100], mean action: 0.190 [-2.683, 3.246], mean observation: 0.542 [-6.867, 4.000], loss: 0.001464, mean_absolute_error: 0.035739, mean_q: 0.084027\n",
      "  5469/50000: episode: 268, duration: 0.269s, episode steps: 20, steps per second: 74, episode reward: 0.900, mean reward: 0.045 [-1.000, 0.100], mean action: 0.094 [-5.297, 3.351], mean observation: 0.352 [-7.848, 4.000], loss: 0.001585, mean_absolute_error: 0.037299, mean_q: 0.076202\n",
      "  5505/50000: episode: 269, duration: 0.549s, episode steps: 36, steps per second: 66, episode reward: 2.500, mean reward: 0.069 [-1.000, 0.100], mean action: -0.765 [-3.354, 1.244], mean observation: 0.112 [-7.079, 4.000], loss: 0.001391, mean_absolute_error: 0.035575, mean_q: 0.078991\n",
      "  5536/50000: episode: 270, duration: 0.404s, episode steps: 31, steps per second: 77, episode reward: 2.000, mean reward: 0.065 [-1.000, 0.100], mean action: -0.090 [-2.582, 1.520], mean observation: 0.367 [-8.472, 4.000], loss: 0.001229, mean_absolute_error: 0.034158, mean_q: 0.082882\n",
      "  5554/50000: episode: 271, duration: 0.269s, episode steps: 18, steps per second: 67, episode reward: 0.700, mean reward: 0.039 [-1.000, 0.100], mean action: -0.351 [-2.807, 1.952], mean observation: 0.315 [-7.848, 4.000], loss: 0.001475, mean_absolute_error: 0.037435, mean_q: 0.071038\n",
      "  5571/50000: episode: 272, duration: 0.209s, episode steps: 17, steps per second: 82, episode reward: 0.600, mean reward: 0.035 [-1.000, 0.100], mean action: -1.613 [-4.379, 0.544], mean observation: -0.525 [-7.848, 4.000], loss: 0.001745, mean_absolute_error: 0.040930, mean_q: 0.056841\n",
      "  5595/50000: episode: 273, duration: 0.336s, episode steps: 24, steps per second: 71, episode reward: 1.300, mean reward: 0.054 [-1.000, 0.100], mean action: -1.108 [-4.977, 1.294], mean observation: -0.013 [-7.221, 4.000], loss: 0.001576, mean_absolute_error: 0.040101, mean_q: 0.095584\n",
      "  5610/50000: episode: 274, duration: 0.218s, episode steps: 15, steps per second: 69, episode reward: 0.400, mean reward: 0.027 [-1.000, 0.100], mean action: -0.034 [-2.436, 2.130], mean observation: 0.169 [-8.829, 4.000], loss: 0.001457, mean_absolute_error: 0.037486, mean_q: 0.062673\n",
      "  5627/50000: episode: 275, duration: 0.253s, episode steps: 17, steps per second: 67, episode reward: 0.600, mean reward: 0.035 [-1.000, 0.100], mean action: -1.550 [-4.054, 0.888], mean observation: -0.482 [-6.867, 4.000], loss: 0.001402, mean_absolute_error: 0.036556, mean_q: 0.085780\n",
      "  5674/50000: episode: 276, duration: 0.661s, episode steps: 47, steps per second: 71, episode reward: 3.600, mean reward: 0.077 [-1.000, 0.100], mean action: -0.465 [-4.381, 2.585], mean observation: 0.021 [-7.848, 4.000], loss: 0.001552, mean_absolute_error: 0.039160, mean_q: 0.078829\n",
      "  5693/50000: episode: 277, duration: 0.240s, episode steps: 19, steps per second: 79, episode reward: 0.800, mean reward: 0.042 [-1.000, 0.100], mean action: -1.238 [-5.086, 0.958], mean observation: -0.212 [-5.886, 4.000], loss: 0.001330, mean_absolute_error: 0.036562, mean_q: 0.077076\n",
      "  5708/50000: episode: 278, duration: 0.188s, episode steps: 15, steps per second: 80, episode reward: 0.400, mean reward: 0.027 [-1.000, 0.100], mean action: -1.607 [-4.415, 1.645], mean observation: -0.436 [-7.848, 4.000], loss: 0.001547, mean_absolute_error: 0.038311, mean_q: 0.048601\n",
      "  5736/50000: episode: 279, duration: 0.358s, episode steps: 28, steps per second: 78, episode reward: 1.700, mean reward: 0.061 [-1.000, 0.100], mean action: -0.605 [-4.429, 2.412], mean observation: 0.194 [-7.893, 4.000], loss: 0.001763, mean_absolute_error: 0.041605, mean_q: 0.120447\n",
      "  5760/50000: episode: 280, duration: 0.289s, episode steps: 24, steps per second: 83, episode reward: 1.300, mean reward: 0.054 [-1.000, 0.100], mean action: 0.217 [-2.908, 2.243], mean observation: 0.377 [-7.848, 4.000], loss: 0.001974, mean_absolute_error: 0.040416, mean_q: 0.072754\n",
      "  5776/50000: episode: 281, duration: 0.198s, episode steps: 16, steps per second: 81, episode reward: 0.500, mean reward: 0.031 [-1.000, 0.100], mean action: -0.010 [-4.868, 3.387], mean observation: 0.215 [-7.848, 4.000], loss: 0.001214, mean_absolute_error: 0.034300, mean_q: 0.153382\n",
      "  5794/50000: episode: 282, duration: 0.233s, episode steps: 18, steps per second: 77, episode reward: 0.700, mean reward: 0.039 [-1.000, 0.100], mean action: -0.265 [-5.183, 3.511], mean observation: 0.387 [-6.867, 4.000], loss: 0.001889, mean_absolute_error: 0.039427, mean_q: 0.082140\n",
      "  5828/50000: episode: 283, duration: 0.468s, episode steps: 34, steps per second: 73, episode reward: 2.300, mean reward: 0.068 [-1.000, 0.100], mean action: -0.008 [-6.171, 3.308], mean observation: 0.266 [-7.848, 4.000], loss: 0.001390, mean_absolute_error: 0.037197, mean_q: 0.106521\n",
      "  5845/50000: episode: 284, duration: 0.259s, episode steps: 17, steps per second: 66, episode reward: 0.600, mean reward: 0.035 [-1.000, 0.100], mean action: -0.459 [-3.286, 2.012], mean observation: 0.213 [-8.829, 4.000], loss: 0.001194, mean_absolute_error: 0.034919, mean_q: 0.122707\n",
      "  5864/50000: episode: 285, duration: 0.256s, episode steps: 19, steps per second: 74, episode reward: 0.800, mean reward: 0.042 [-1.000, 0.100], mean action: -0.428 [-4.737, 2.537], mean observation: 0.298 [-7.848, 4.000], loss: 0.001384, mean_absolute_error: 0.037057, mean_q: 0.092305\n",
      "  5899/50000: episode: 286, duration: 0.469s, episode steps: 35, steps per second: 75, episode reward: 2.400, mean reward: 0.069 [-1.000, 0.100], mean action: -0.420 [-3.946, 1.867], mean observation: -0.062 [-6.867, 4.000], loss: 0.001749, mean_absolute_error: 0.038708, mean_q: 0.104079\n",
      "  5926/50000: episode: 287, duration: 0.361s, episode steps: 27, steps per second: 75, episode reward: 1.600, mean reward: 0.059 [-1.000, 0.100], mean action: -0.136 [-3.284, 1.327], mean observation: 0.372 [-6.867, 4.000], loss: 0.001826, mean_absolute_error: 0.043551, mean_q: 0.102591\n",
      "  5967/50000: episode: 288, duration: 0.561s, episode steps: 41, steps per second: 73, episode reward: 3.000, mean reward: 0.073 [-1.000, 0.100], mean action: -0.116 [-3.587, 1.663], mean observation: 0.350 [-6.867, 4.000], loss: 0.001851, mean_absolute_error: 0.043248, mean_q: 0.104210\n",
      "  6002/50000: episode: 289, duration: 0.416s, episode steps: 35, steps per second: 84, episode reward: 2.400, mean reward: 0.069 [-1.000, 0.100], mean action: -0.527 [-4.059, 1.639], mean observation: -0.068 [-6.867, 4.000], loss: 0.001790, mean_absolute_error: 0.042349, mean_q: 0.110323\n",
      "  6030/50000: episode: 290, duration: 0.334s, episode steps: 28, steps per second: 84, episode reward: 1.700, mean reward: 0.061 [-1.000, 0.100], mean action: -0.054 [-1.963, 1.774], mean observation: 0.346 [-6.867, 4.000], loss: 0.001560, mean_absolute_error: 0.039062, mean_q: 0.098593\n",
      "  6060/50000: episode: 291, duration: 0.378s, episode steps: 30, steps per second: 79, episode reward: 1.900, mean reward: 0.063 [-1.000, 0.100], mean action: -0.176 [-4.239, 2.387], mean observation: 0.081 [-7.848, 4.000], loss: 0.001398, mean_absolute_error: 0.037937, mean_q: 0.131868\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  6078/50000: episode: 292, duration: 0.224s, episode steps: 18, steps per second: 80, episode reward: 0.700, mean reward: 0.039 [-1.000, 0.100], mean action: -1.573 [-4.796, 0.974], mean observation: -0.447 [-6.867, 4.000], loss: 0.001576, mean_absolute_error: 0.040698, mean_q: 0.086575\n",
      "  6105/50000: episode: 293, duration: 0.337s, episode steps: 27, steps per second: 80, episode reward: 1.600, mean reward: 0.059 [-1.000, 0.100], mean action: -0.671 [-4.193, 2.094], mean observation: 0.082 [-7.848, 4.000], loss: 0.001603, mean_absolute_error: 0.039181, mean_q: 0.116837\n",
      "  6129/50000: episode: 294, duration: 0.320s, episode steps: 24, steps per second: 75, episode reward: 1.300, mean reward: 0.054 [-1.000, 0.100], mean action: -1.074 [-7.467, 2.308], mean observation: -0.082 [-7.877, 4.000], loss: 0.001622, mean_absolute_error: 0.039686, mean_q: 0.111774\n",
      "  6152/50000: episode: 295, duration: 0.360s, episode steps: 23, steps per second: 64, episode reward: 1.200, mean reward: 0.052 [-1.000, 0.100], mean action: -0.278 [-6.309, 1.888], mean observation: 0.334 [-7.848, 4.000], loss: 0.001679, mean_absolute_error: 0.040874, mean_q: 0.146359\n",
      "  6177/50000: episode: 296, duration: 0.324s, episode steps: 25, steps per second: 77, episode reward: 1.400, mean reward: 0.056 [-1.000, 0.100], mean action: -0.190 [-4.668, 1.948], mean observation: 0.296 [-6.867, 4.000], loss: 0.002280, mean_absolute_error: 0.046031, mean_q: 0.131190\n",
      "  6193/50000: episode: 297, duration: 0.191s, episode steps: 16, steps per second: 84, episode reward: 0.500, mean reward: 0.031 [-1.000, 0.100], mean action: -0.383 [-4.202, 2.140], mean observation: 0.317 [-7.848, 4.000], loss: 0.001648, mean_absolute_error: 0.040394, mean_q: 0.148299\n",
      "  6213/50000: episode: 298, duration: 0.274s, episode steps: 20, steps per second: 73, episode reward: 0.900, mean reward: 0.045 [-1.000, 0.100], mean action: -0.938 [-9.670, 1.944], mean observation: 0.217 [-7.848, 4.000], loss: 0.001770, mean_absolute_error: 0.041672, mean_q: 0.096223\n",
      "  6243/50000: episode: 299, duration: 0.406s, episode steps: 30, steps per second: 74, episode reward: 1.900, mean reward: 0.063 [-1.000, 0.100], mean action: -0.901 [-6.403, 2.782], mean observation: 0.028 [-7.848, 4.000], loss: 0.002392, mean_absolute_error: 0.046372, mean_q: 0.119497\n",
      "  6268/50000: episode: 300, duration: 0.365s, episode steps: 25, steps per second: 69, episode reward: 1.400, mean reward: 0.056 [-1.000, 0.100], mean action: -0.782 [-5.600, 2.689], mean observation: -0.072 [-7.269, 4.000], loss: 0.001657, mean_absolute_error: 0.041776, mean_q: 0.146673\n",
      "  6289/50000: episode: 301, duration: 0.287s, episode steps: 21, steps per second: 73, episode reward: 1.000, mean reward: 0.048 [-1.000, 0.100], mean action: -1.561 [-7.068, 2.171], mean observation: -0.189 [-7.311, 4.000], loss: 0.002155, mean_absolute_error: 0.044617, mean_q: 0.086560\n",
      "  6305/50000: episode: 302, duration: 0.201s, episode steps: 16, steps per second: 80, episode reward: 0.500, mean reward: 0.031 [-1.000, 0.100], mean action: -2.059 [-8.339, 0.908], mean observation: -0.542 [-7.848, 4.000], loss: 0.001809, mean_absolute_error: 0.040720, mean_q: 0.136669\n",
      "  6326/50000: episode: 303, duration: 0.294s, episode steps: 21, steps per second: 72, episode reward: 1.000, mean reward: 0.048 [-1.000, 0.100], mean action: 0.019 [-2.436, 2.835], mean observation: 0.197 [-7.848, 4.000], loss: 0.001891, mean_absolute_error: 0.042667, mean_q: 0.136511\n",
      "  6359/50000: episode: 304, duration: 0.466s, episode steps: 33, steps per second: 71, episode reward: 2.200, mean reward: 0.067 [-1.000, 0.100], mean action: -0.970 [-7.120, 2.887], mean observation: 0.031 [-7.848, 4.000], loss: 0.002022, mean_absolute_error: 0.044283, mean_q: 0.161271\n",
      "  6385/50000: episode: 305, duration: 0.355s, episode steps: 26, steps per second: 73, episode reward: 1.500, mean reward: 0.058 [-1.000, 0.100], mean action: 0.335 [-3.081, 3.111], mean observation: 0.343 [-7.848, 4.000], loss: 0.002076, mean_absolute_error: 0.043772, mean_q: 0.144217\n",
      "  6416/50000: episode: 306, duration: 0.640s, episode steps: 31, steps per second: 48, episode reward: 2.000, mean reward: 0.065 [-1.000, 0.100], mean action: -0.539 [-4.439, 3.030], mean observation: 0.114 [-7.848, 4.000], loss: 0.002204, mean_absolute_error: 0.046373, mean_q: 0.130664\n",
      "  6469/50000: episode: 307, duration: 0.629s, episode steps: 53, steps per second: 84, episode reward: 4.200, mean reward: 0.079 [-1.000, 0.100], mean action: 0.053 [-3.224, 3.657], mean observation: 0.125 [-6.867, 4.000], loss: 0.002109, mean_absolute_error: 0.045377, mean_q: 0.144514\n",
      "  6485/50000: episode: 308, duration: 0.195s, episode steps: 16, steps per second: 82, episode reward: 0.500, mean reward: 0.031 [-1.000, 0.100], mean action: 0.349 [-7.115, 4.417], mean observation: 0.344 [-7.848, 4.000], loss: 0.002001, mean_absolute_error: 0.046935, mean_q: 0.201120\n",
      "  6524/50000: episode: 309, duration: 0.455s, episode steps: 39, steps per second: 86, episode reward: 2.800, mean reward: 0.072 [-1.000, 0.100], mean action: 0.439 [-3.655, 4.189], mean observation: 0.208 [-6.867, 4.000], loss: 0.002191, mean_absolute_error: 0.049248, mean_q: 0.161980\n",
      "  6558/50000: episode: 310, duration: 0.411s, episode steps: 34, steps per second: 83, episode reward: 2.300, mean reward: 0.068 [-1.000, 0.100], mean action: -0.276 [-3.882, 3.203], mean observation: -0.129 [-6.867, 4.000], loss: 0.002001, mean_absolute_error: 0.044836, mean_q: 0.139329\n",
      "  6581/50000: episode: 311, duration: 0.314s, episode steps: 23, steps per second: 73, episode reward: 1.200, mean reward: 0.052 [-1.000, 0.100], mean action: -0.269 [-6.047, 2.762], mean observation: 0.401 [-6.867, 4.000], loss: 0.002371, mean_absolute_error: 0.046537, mean_q: 0.164104\n",
      "  6610/50000: episode: 312, duration: 0.358s, episode steps: 29, steps per second: 81, episode reward: 1.800, mean reward: 0.062 [-1.000, 0.100], mean action: -0.813 [-8.338, 2.969], mean observation: 0.084 [-8.825, 4.000], loss: 0.001848, mean_absolute_error: 0.041621, mean_q: 0.171074\n",
      "  6637/50000: episode: 313, duration: 0.336s, episode steps: 27, steps per second: 80, episode reward: 1.600, mean reward: 0.059 [-1.000, 0.100], mean action: -0.496 [-7.029, 3.558], mean observation: -0.006 [-7.802, 4.000], loss: 0.001692, mean_absolute_error: 0.040353, mean_q: 0.167599\n",
      "  6669/50000: episode: 314, duration: 0.390s, episode steps: 32, steps per second: 82, episode reward: 2.100, mean reward: 0.066 [-1.000, 0.100], mean action: -0.184 [-4.220, 3.418], mean observation: -0.131 [-6.867, 4.000], loss: 0.001918, mean_absolute_error: 0.042130, mean_q: 0.198151\n",
      "  6698/50000: episode: 315, duration: 0.358s, episode steps: 29, steps per second: 81, episode reward: 1.800, mean reward: 0.062 [-1.000, 0.100], mean action: -0.148 [-4.464, 3.477], mean observation: 0.101 [-7.848, 4.000], loss: 0.002538, mean_absolute_error: 0.046730, mean_q: 0.195946\n",
      "  6721/50000: episode: 316, duration: 0.282s, episode steps: 23, steps per second: 81, episode reward: 1.200, mean reward: 0.052 [-1.000, 0.100], mean action: -0.337 [-5.192, 3.341], mean observation: 0.197 [-6.867, 4.000], loss: 0.001516, mean_absolute_error: 0.037994, mean_q: 0.181647\n",
      "  6744/50000: episode: 317, duration: 0.283s, episode steps: 23, steps per second: 81, episode reward: 1.200, mean reward: 0.052 [-1.000, 0.100], mean action: -0.153 [-3.693, 3.091], mean observation: -0.019 [-7.645, 4.000], loss: 0.002186, mean_absolute_error: 0.044478, mean_q: 0.170661\n",
      "  6763/50000: episode: 318, duration: 0.225s, episode steps: 19, steps per second: 84, episode reward: 0.800, mean reward: 0.042 [-1.000, 0.100], mean action: -0.344 [-5.136, 1.887], mean observation: 0.280 [-7.848, 4.000], loss: 0.002206, mean_absolute_error: 0.045160, mean_q: 0.166016\n",
      "  6784/50000: episode: 319, duration: 0.263s, episode steps: 21, steps per second: 80, episode reward: 1.000, mean reward: 0.048 [-1.000, 0.100], mean action: -0.306 [-5.286, 3.431], mean observation: 0.328 [-6.867, 4.000], loss: 0.002276, mean_absolute_error: 0.046033, mean_q: 0.133689\n",
      "  6812/50000: episode: 320, duration: 0.324s, episode steps: 28, steps per second: 86, episode reward: 1.700, mean reward: 0.061 [-1.000, 0.100], mean action: -0.030 [-4.360, 1.523], mean observation: 0.301 [-6.867, 4.000], loss: 0.002022, mean_absolute_error: 0.044289, mean_q: 0.182767\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  6836/50000: episode: 321, duration: 0.316s, episode steps: 24, steps per second: 76, episode reward: 1.300, mean reward: 0.054 [-1.000, 0.100], mean action: -0.247 [-8.584, 3.296], mean observation: 0.160 [-7.848, 4.000], loss: 0.001936, mean_absolute_error: 0.044554, mean_q: 0.181534\n",
      "  6874/50000: episode: 322, duration: 0.461s, episode steps: 38, steps per second: 82, episode reward: 2.700, mean reward: 0.071 [-1.000, 0.100], mean action: 0.001 [-3.771, 2.783], mean observation: 0.256 [-6.867, 4.000], loss: 0.001706, mean_absolute_error: 0.040841, mean_q: 0.172711\n",
      "  6910/50000: episode: 323, duration: 0.431s, episode steps: 36, steps per second: 83, episode reward: 2.500, mean reward: 0.069 [-1.000, 0.100], mean action: 0.137 [-2.671, 4.257], mean observation: 0.341 [-8.572, 4.000], loss: 0.001972, mean_absolute_error: 0.041971, mean_q: 0.180111\n",
      "  6942/50000: episode: 324, duration: 0.391s, episode steps: 32, steps per second: 82, episode reward: 2.100, mean reward: 0.066 [-1.000, 0.100], mean action: -0.162 [-6.985, 5.012], mean observation: 0.455 [-9.222, 4.000], loss: 0.002018, mean_absolute_error: 0.044489, mean_q: 0.175875\n",
      "  6971/50000: episode: 325, duration: 0.333s, episode steps: 29, steps per second: 87, episode reward: 1.800, mean reward: 0.062 [-1.000, 0.100], mean action: -0.511 [-6.206, 2.931], mean observation: 0.143 [-8.117, 4.000], loss: 0.001908, mean_absolute_error: 0.043851, mean_q: 0.179389\n",
      "  6991/50000: episode: 326, duration: 0.253s, episode steps: 20, steps per second: 79, episode reward: 0.900, mean reward: 0.045 [-1.000, 0.100], mean action: 0.300 [-3.808, 2.778], mean observation: 0.338 [-7.848, 4.000], loss: 0.002057, mean_absolute_error: 0.043359, mean_q: 0.198621\n",
      "  7028/50000: episode: 327, duration: 0.462s, episode steps: 37, steps per second: 80, episode reward: 2.600, mean reward: 0.070 [-1.000, 0.100], mean action: -0.162 [-4.743, 3.720], mean observation: -0.059 [-6.867, 4.000], loss: 0.001818, mean_absolute_error: 0.042124, mean_q: 0.178813\n",
      "  7052/50000: episode: 328, duration: 0.284s, episode steps: 24, steps per second: 84, episode reward: 1.300, mean reward: 0.054 [-1.000, 0.100], mean action: 0.451 [-2.239, 3.148], mean observation: 0.342 [-7.848, 4.000], loss: 0.001541, mean_absolute_error: 0.039739, mean_q: 0.203861\n",
      "  7087/50000: episode: 329, duration: 0.418s, episode steps: 35, steps per second: 84, episode reward: 2.400, mean reward: 0.069 [-1.000, 0.100], mean action: -0.172 [-4.943, 4.073], mean observation: 0.379 [-8.294, 4.000], loss: 0.001772, mean_absolute_error: 0.041166, mean_q: 0.215325\n",
      "  7113/50000: episode: 330, duration: 0.313s, episode steps: 26, steps per second: 83, episode reward: 1.500, mean reward: 0.058 [-1.000, 0.100], mean action: -0.380 [-4.525, 1.593], mean observation: 0.370 [-6.867, 4.000], loss: 0.001751, mean_absolute_error: 0.039318, mean_q: 0.204805\n",
      "  7141/50000: episode: 331, duration: 0.347s, episode steps: 28, steps per second: 81, episode reward: 1.700, mean reward: 0.061 [-1.000, 0.100], mean action: 0.323 [-2.907, 3.440], mean observation: 0.353 [-7.848, 4.000], loss: 0.002046, mean_absolute_error: 0.043096, mean_q: 0.195237\n",
      "  7158/50000: episode: 332, duration: 0.211s, episode steps: 17, steps per second: 81, episode reward: 0.600, mean reward: 0.035 [-1.000, 0.100], mean action: -1.723 [-7.360, 0.924], mean observation: -0.564 [-7.848, 4.000], loss: 0.001773, mean_absolute_error: 0.039683, mean_q: 0.209340\n",
      "  7175/50000: episode: 333, duration: 0.197s, episode steps: 17, steps per second: 86, episode reward: 0.600, mean reward: 0.035 [-1.000, 0.100], mean action: -0.112 [-4.232, 2.293], mean observation: 0.282 [-7.848, 4.000], loss: 0.001524, mean_absolute_error: 0.037790, mean_q: 0.223427\n",
      "  7208/50000: episode: 334, duration: 0.400s, episode steps: 33, steps per second: 83, episode reward: 2.200, mean reward: 0.067 [-1.000, 0.100], mean action: -0.718 [-6.527, 2.349], mean observation: 0.021 [-6.867, 4.000], loss: 0.002093, mean_absolute_error: 0.046667, mean_q: 0.240407\n",
      "  7231/50000: episode: 335, duration: 0.284s, episode steps: 23, steps per second: 81, episode reward: 1.200, mean reward: 0.052 [-1.000, 0.100], mean action: 0.162 [-3.700, 2.131], mean observation: 0.375 [-7.848, 4.000], loss: 0.001992, mean_absolute_error: 0.042526, mean_q: 0.198697\n",
      "  7255/50000: episode: 336, duration: 0.291s, episode steps: 24, steps per second: 83, episode reward: 1.300, mean reward: 0.054 [-1.000, 0.100], mean action: -0.200 [-4.210, 1.546], mean observation: 0.356 [-6.867, 4.000], loss: 0.001871, mean_absolute_error: 0.042530, mean_q: 0.192553\n",
      "  7271/50000: episode: 337, duration: 0.211s, episode steps: 16, steps per second: 76, episode reward: 0.500, mean reward: 0.031 [-1.000, 0.100], mean action: -0.779 [-9.694, 3.575], mean observation: 0.285 [-8.829, 4.000], loss: 0.001620, mean_absolute_error: 0.038462, mean_q: 0.211333\n",
      "  7288/50000: episode: 338, duration: 0.207s, episode steps: 17, steps per second: 82, episode reward: 0.600, mean reward: 0.035 [-1.000, 0.100], mean action: -0.285 [-8.801, 3.305], mean observation: 0.266 [-7.848, 4.000], loss: 0.001931, mean_absolute_error: 0.042276, mean_q: 0.182715\n",
      "  7317/50000: episode: 339, duration: 0.375s, episode steps: 29, steps per second: 77, episode reward: 1.800, mean reward: 0.062 [-1.000, 0.100], mean action: -0.636 [-4.650, 1.720], mean observation: 0.431 [-7.848, 4.000], loss: 0.001903, mean_absolute_error: 0.040712, mean_q: 0.196933\n",
      "  7335/50000: episode: 340, duration: 0.240s, episode steps: 18, steps per second: 75, episode reward: 0.700, mean reward: 0.039 [-1.000, 0.100], mean action: -0.629 [-7.122, 3.666], mean observation: 0.393 [-8.829, 4.000], loss: 0.001742, mean_absolute_error: 0.039488, mean_q: 0.236350\n",
      "  7372/50000: episode: 341, duration: 0.446s, episode steps: 37, steps per second: 83, episode reward: 2.600, mean reward: 0.070 [-1.000, 0.100], mean action: -0.761 [-6.391, 3.504], mean observation: 0.046 [-6.867, 4.000], loss: 0.002634, mean_absolute_error: 0.050025, mean_q: 0.216677\n",
      "  7395/50000: episode: 342, duration: 0.283s, episode steps: 23, steps per second: 81, episode reward: 1.200, mean reward: 0.052 [-1.000, 0.100], mean action: -0.211 [-4.504, 3.266], mean observation: 0.405 [-6.867, 4.000], loss: 0.001872, mean_absolute_error: 0.040757, mean_q: 0.237979\n",
      "  7427/50000: episode: 343, duration: 0.367s, episode steps: 32, steps per second: 87, episode reward: 2.100, mean reward: 0.066 [-1.000, 0.100], mean action: -0.449 [-5.360, 2.127], mean observation: 0.460 [-6.867, 4.000], loss: 0.002201, mean_absolute_error: 0.045247, mean_q: 0.258364\n",
      "  7450/50000: episode: 344, duration: 0.282s, episode steps: 23, steps per second: 82, episode reward: 1.200, mean reward: 0.052 [-1.000, 0.100], mean action: -0.366 [-5.387, 4.247], mean observation: 0.393 [-6.867, 4.000], loss: 0.002167, mean_absolute_error: 0.044898, mean_q: 0.243284\n",
      "  7495/50000: episode: 345, duration: 0.531s, episode steps: 45, steps per second: 85, episode reward: 3.400, mean reward: 0.076 [-1.000, 0.100], mean action: -0.248 [-3.194, 2.598], mean observation: 0.031 [-6.867, 4.000], loss: 0.002207, mean_absolute_error: 0.044677, mean_q: 0.243943\n",
      "  7525/50000: episode: 346, duration: 0.371s, episode steps: 30, steps per second: 81, episode reward: 1.900, mean reward: 0.063 [-1.000, 0.100], mean action: -0.400 [-4.414, 2.856], mean observation: 0.401 [-7.848, 4.000], loss: 0.002456, mean_absolute_error: 0.045241, mean_q: 0.234440\n",
      "  7566/50000: episode: 347, duration: 0.482s, episode steps: 41, steps per second: 85, episode reward: 3.000, mean reward: 0.073 [-1.000, 0.100], mean action: -0.304 [-7.300, 3.707], mean observation: 0.239 [-6.867, 4.000], loss: 0.002047, mean_absolute_error: 0.045255, mean_q: 0.244006\n",
      "  7633/50000: episode: 348, duration: 0.796s, episode steps: 67, steps per second: 84, episode reward: 5.600, mean reward: 0.084 [-1.000, 0.100], mean action: -0.238 [-5.178, 4.108], mean observation: 0.258 [-6.964, 4.000], loss: 0.002100, mean_absolute_error: 0.043226, mean_q: 0.232871\n",
      "  7655/50000: episode: 349, duration: 0.275s, episode steps: 22, steps per second: 80, episode reward: 1.100, mean reward: 0.050 [-1.000, 0.100], mean action: 0.487 [-2.194, 3.045], mean observation: 0.451 [-7.848, 4.000], loss: 0.002299, mean_absolute_error: 0.048209, mean_q: 0.245100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  7688/50000: episode: 350, duration: 0.435s, episode steps: 33, steps per second: 76, episode reward: 2.200, mean reward: 0.067 [-1.000, 0.100], mean action: -0.343 [-2.462, 2.431], mean observation: 0.365 [-6.867, 4.000], loss: 0.001929, mean_absolute_error: 0.042831, mean_q: 0.250880\n",
      "  7712/50000: episode: 351, duration: 0.288s, episode steps: 24, steps per second: 83, episode reward: 1.300, mean reward: 0.054 [-1.000, 0.100], mean action: 0.340 [-2.426, 2.521], mean observation: 0.414 [-7.848, 4.000], loss: 0.001725, mean_absolute_error: 0.040854, mean_q: 0.246035\n",
      "  7739/50000: episode: 352, duration: 0.337s, episode steps: 27, steps per second: 80, episode reward: 1.600, mean reward: 0.059 [-1.000, 0.100], mean action: 0.042 [-3.835, 3.000], mean observation: 0.345 [-6.867, 4.000], loss: 0.001851, mean_absolute_error: 0.041023, mean_q: 0.269702\n",
      "  7777/50000: episode: 353, duration: 0.482s, episode steps: 38, steps per second: 79, episode reward: 2.700, mean reward: 0.071 [-1.000, 0.100], mean action: -0.658 [-7.585, 2.551], mean observation: 0.109 [-7.848, 4.000], loss: 0.001920, mean_absolute_error: 0.042315, mean_q: 0.277771\n",
      "  7810/50000: episode: 354, duration: 0.399s, episode steps: 33, steps per second: 83, episode reward: 2.200, mean reward: 0.067 [-1.000, 0.100], mean action: -0.012 [-3.456, 4.312], mean observation: 0.421 [-7.092, 4.000], loss: 0.002272, mean_absolute_error: 0.043867, mean_q: 0.252435\n",
      "  7892/50000: episode: 355, duration: 0.983s, episode steps: 82, steps per second: 83, episode reward: 7.100, mean reward: 0.087 [-1.000, 0.100], mean action: -0.654 [-10.123, 2.821], mean observation: 0.095 [-8.931, 4.000], loss: 0.002199, mean_absolute_error: 0.044452, mean_q: 0.261812\n",
      "  7916/50000: episode: 356, duration: 0.295s, episode steps: 24, steps per second: 81, episode reward: 1.300, mean reward: 0.054 [-1.000, 0.100], mean action: -0.068 [-4.100, 3.534], mean observation: 0.413 [-6.867, 4.000], loss: 0.001910, mean_absolute_error: 0.041881, mean_q: 0.252450\n",
      "  7942/50000: episode: 357, duration: 0.316s, episode steps: 26, steps per second: 82, episode reward: 1.500, mean reward: 0.058 [-1.000, 0.100], mean action: 0.171 [-3.140, 3.494], mean observation: 0.466 [-7.848, 4.000], loss: 0.001797, mean_absolute_error: 0.041669, mean_q: 0.266326\n",
      "  7960/50000: episode: 358, duration: 0.210s, episode steps: 18, steps per second: 86, episode reward: 0.700, mean reward: 0.039 [-1.000, 0.100], mean action: -0.155 [-3.878, 1.754], mean observation: 0.296 [-7.848, 4.000], loss: 0.002107, mean_absolute_error: 0.045035, mean_q: 0.242237\n",
      "  7994/50000: episode: 359, duration: 0.407s, episode steps: 34, steps per second: 84, episode reward: 2.300, mean reward: 0.068 [-1.000, 0.100], mean action: -0.860 [-8.704, 2.879], mean observation: -0.092 [-6.867, 4.000], loss: 0.002242, mean_absolute_error: 0.044492, mean_q: 0.252672\n",
      "  8015/50000: episode: 360, duration: 0.267s, episode steps: 21, steps per second: 79, episode reward: 1.000, mean reward: 0.048 [-1.000, 0.100], mean action: 0.560 [-2.038, 2.524], mean observation: 0.330 [-7.848, 4.000], loss: 0.001975, mean_absolute_error: 0.043248, mean_q: 0.304553\n",
      "  8036/50000: episode: 361, duration: 0.257s, episode steps: 21, steps per second: 82, episode reward: 1.000, mean reward: 0.048 [-1.000, 0.100], mean action: 0.395 [-3.544, 3.808], mean observation: 0.424 [-7.848, 4.000], loss: 0.002402, mean_absolute_error: 0.049077, mean_q: 0.273240\n",
      "  8056/50000: episode: 362, duration: 0.269s, episode steps: 20, steps per second: 74, episode reward: 0.900, mean reward: 0.045 [-1.000, 0.100], mean action: 0.009 [-3.164, 3.800], mean observation: 0.418 [-7.848, 4.000], loss: 0.002275, mean_absolute_error: 0.044381, mean_q: 0.286690\n",
      "  8093/50000: episode: 363, duration: 0.457s, episode steps: 37, steps per second: 81, episode reward: 2.600, mean reward: 0.070 [-1.000, 0.100], mean action: -0.894 [-8.151, 2.729], mean observation: 0.136 [-7.668, 4.000], loss: 0.002356, mean_absolute_error: 0.047075, mean_q: 0.293205\n",
      "  8123/50000: episode: 364, duration: 0.365s, episode steps: 30, steps per second: 82, episode reward: 1.900, mean reward: 0.063 [-1.000, 0.100], mean action: -0.230 [-4.544, 2.764], mean observation: 0.424 [-6.867, 4.000], loss: 0.002091, mean_absolute_error: 0.045954, mean_q: 0.269318\n",
      "  8140/50000: episode: 365, duration: 0.220s, episode steps: 17, steps per second: 77, episode reward: 0.600, mean reward: 0.035 [-1.000, 0.100], mean action: -0.197 [-3.643, 1.927], mean observation: 0.262 [-7.848, 4.000], loss: 0.002080, mean_absolute_error: 0.042617, mean_q: 0.277643\n",
      "  8158/50000: episode: 366, duration: 0.223s, episode steps: 18, steps per second: 81, episode reward: 0.700, mean reward: 0.039 [-1.000, 0.100], mean action: 0.522 [-2.274, 3.631], mean observation: 0.464 [-7.848, 4.000], loss: 0.002135, mean_absolute_error: 0.046612, mean_q: 0.269829\n",
      "  8183/50000: episode: 367, duration: 0.298s, episode steps: 25, steps per second: 84, episode reward: 1.400, mean reward: 0.056 [-1.000, 0.100], mean action: -0.387 [-5.977, 3.641], mean observation: 0.490 [-7.848, 4.000], loss: 0.001838, mean_absolute_error: 0.040017, mean_q: 0.291576\n",
      "  8210/50000: episode: 368, duration: 0.322s, episode steps: 27, steps per second: 84, episode reward: 1.600, mean reward: 0.059 [-1.000, 0.100], mean action: -0.202 [-4.365, 2.891], mean observation: 0.439 [-7.848, 4.000], loss: 0.002180, mean_absolute_error: 0.045188, mean_q: 0.285425\n",
      "  8235/50000: episode: 369, duration: 0.300s, episode steps: 25, steps per second: 83, episode reward: 1.400, mean reward: 0.056 [-1.000, 0.100], mean action: -0.297 [-5.016, 3.408], mean observation: 0.471 [-7.848, 4.000], loss: 0.002596, mean_absolute_error: 0.046586, mean_q: 0.259995\n",
      "  8260/50000: episode: 370, duration: 0.299s, episode steps: 25, steps per second: 84, episode reward: 1.400, mean reward: 0.056 [-1.000, 0.100], mean action: 0.260 [-3.058, 3.671], mean observation: 0.438 [-6.867, 4.000], loss: 0.002269, mean_absolute_error: 0.043251, mean_q: 0.290044\n",
      "  8292/50000: episode: 371, duration: 0.389s, episode steps: 32, steps per second: 82, episode reward: 2.100, mean reward: 0.066 [-1.000, 0.100], mean action: -0.312 [-3.743, 1.828], mean observation: 0.454 [-6.867, 4.000], loss: 0.002345, mean_absolute_error: 0.045125, mean_q: 0.273160\n",
      "  8318/50000: episode: 372, duration: 0.311s, episode steps: 26, steps per second: 84, episode reward: 1.500, mean reward: 0.058 [-1.000, 0.100], mean action: -0.265 [-5.385, 3.619], mean observation: 0.453 [-7.848, 4.000], loss: 0.002255, mean_absolute_error: 0.045108, mean_q: 0.297910\n",
      "  8349/50000: episode: 373, duration: 0.385s, episode steps: 31, steps per second: 81, episode reward: 2.000, mean reward: 0.065 [-1.000, 0.100], mean action: -0.027 [-4.436, 5.216], mean observation: 0.419 [-9.097, 4.000], loss: 0.001964, mean_absolute_error: 0.043874, mean_q: 0.299311\n",
      "  8362/50000: episode: 374, duration: 0.184s, episode steps: 13, steps per second: 71, episode reward: 0.200, mean reward: 0.015 [-1.000, 0.100], mean action: -2.304 [-5.097, -0.069], mean observation: -0.446 [-9.810, 4.000], loss: 0.001988, mean_absolute_error: 0.042660, mean_q: 0.307662\n",
      "  8392/50000: episode: 375, duration: 0.365s, episode steps: 30, steps per second: 82, episode reward: 1.900, mean reward: 0.063 [-1.000, 0.100], mean action: -0.048 [-6.294, 4.219], mean observation: 0.449 [-6.867, 4.000], loss: 0.001883, mean_absolute_error: 0.042212, mean_q: 0.258629\n",
      "  8416/50000: episode: 376, duration: 0.290s, episode steps: 24, steps per second: 83, episode reward: 1.300, mean reward: 0.054 [-1.000, 0.100], mean action: 0.425 [-3.445, 3.963], mean observation: 0.415 [-6.867, 4.000], loss: 0.002023, mean_absolute_error: 0.043384, mean_q: 0.256468\n",
      "  8435/50000: episode: 377, duration: 0.240s, episode steps: 19, steps per second: 79, episode reward: 0.800, mean reward: 0.042 [-1.000, 0.100], mean action: 0.344 [-2.976, 2.453], mean observation: 0.370 [-7.848, 4.000], loss: 0.002160, mean_absolute_error: 0.043461, mean_q: 0.287828\n",
      "  8453/50000: episode: 378, duration: 0.229s, episode steps: 18, steps per second: 79, episode reward: 0.700, mean reward: 0.039 [-1.000, 0.100], mean action: 0.720 [-2.578, 2.967], mean observation: 0.468 [-7.848, 4.000], loss: 0.002639, mean_absolute_error: 0.047592, mean_q: 0.328665\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  8489/50000: episode: 379, duration: 0.429s, episode steps: 36, steps per second: 84, episode reward: 2.500, mean reward: 0.069 [-1.000, 0.100], mean action: -0.681 [-6.210, 2.563], mean observation: 0.185 [-7.008, 4.000], loss: 0.001843, mean_absolute_error: 0.041885, mean_q: 0.290378\n",
      "  8506/50000: episode: 380, duration: 0.221s, episode steps: 17, steps per second: 77, episode reward: 0.600, mean reward: 0.035 [-1.000, 0.100], mean action: 0.200 [-2.811, 2.715], mean observation: 0.286 [-7.848, 4.000], loss: 0.001881, mean_absolute_error: 0.042224, mean_q: 0.293369\n",
      "  8562/50000: episode: 381, duration: 0.677s, episode steps: 56, steps per second: 83, episode reward: 4.500, mean reward: 0.080 [-1.000, 0.100], mean action: -0.688 [-5.607, 1.818], mean observation: -0.035 [-6.867, 4.000], loss: 0.001991, mean_absolute_error: 0.042714, mean_q: 0.308220\n",
      "  8607/50000: episode: 382, duration: 0.542s, episode steps: 45, steps per second: 83, episode reward: 3.400, mean reward: 0.076 [-1.000, 0.100], mean action: -0.139 [-5.229, 3.128], mean observation: 0.055 [-6.867, 4.000], loss: 0.002368, mean_absolute_error: 0.047015, mean_q: 0.293302\n",
      "  8662/50000: episode: 383, duration: 0.642s, episode steps: 55, steps per second: 86, episode reward: 4.400, mean reward: 0.080 [-1.000, 0.100], mean action: -0.666 [-5.518, 2.416], mean observation: 0.119 [-5.886, 4.000], loss: 0.002800, mean_absolute_error: 0.044848, mean_q: 0.290883\n",
      "  8731/50000: episode: 384, duration: 0.814s, episode steps: 69, steps per second: 85, episode reward: 5.800, mean reward: 0.084 [-1.000, 0.100], mean action: 0.118 [-4.154, 6.297], mean observation: 0.077 [-6.867, 4.000], loss: 0.002221, mean_absolute_error: 0.044484, mean_q: 0.301576\n",
      "  8774/50000: episode: 385, duration: 0.512s, episode steps: 43, steps per second: 84, episode reward: 3.200, mean reward: 0.074 [-1.000, 0.100], mean action: -0.200 [-3.679, 3.583], mean observation: 0.107 [-7.248, 4.000], loss: 0.002931, mean_absolute_error: 0.049536, mean_q: 0.290999\n",
      "  8807/50000: episode: 386, duration: 0.400s, episode steps: 33, steps per second: 83, episode reward: 2.200, mean reward: 0.067 [-1.000, 0.100], mean action: -0.440 [-6.091, 2.539], mean observation: 0.360 [-5.886, 4.000], loss: 0.002745, mean_absolute_error: 0.052802, mean_q: 0.286157\n",
      "  8837/50000: episode: 387, duration: 0.356s, episode steps: 30, steps per second: 84, episode reward: 1.900, mean reward: 0.063 [-1.000, 0.100], mean action: 0.298 [-3.167, 3.868], mean observation: 0.385 [-8.369, 4.000], loss: 0.002657, mean_absolute_error: 0.048434, mean_q: 0.309109\n",
      "  8870/50000: episode: 388, duration: 0.393s, episode steps: 33, steps per second: 84, episode reward: 2.200, mean reward: 0.067 [-1.000, 0.100], mean action: 0.304 [-3.859, 6.184], mean observation: -0.016 [-5.886, 4.000], loss: 0.002120, mean_absolute_error: 0.044694, mean_q: 0.328293\n",
      "  8913/50000: episode: 389, duration: 0.516s, episode steps: 43, steps per second: 83, episode reward: 3.200, mean reward: 0.074 [-1.000, 0.100], mean action: 0.380 [-2.307, 7.072], mean observation: 0.135 [-7.848, 4.000], loss: 0.002289, mean_absolute_error: 0.044847, mean_q: 0.337370\n",
      "  8964/50000: episode: 390, duration: 0.606s, episode steps: 51, steps per second: 84, episode reward: 4.000, mean reward: 0.078 [-1.000, 0.100], mean action: -0.151 [-3.830, 3.222], mean observation: -0.037 [-6.867, 4.000], loss: 0.002779, mean_absolute_error: 0.049469, mean_q: 0.317156\n",
      "  8993/50000: episode: 391, duration: 0.348s, episode steps: 29, steps per second: 83, episode reward: 1.800, mean reward: 0.062 [-1.000, 0.100], mean action: 0.045 [-4.250, 3.036], mean observation: 0.330 [-8.790, 4.000], loss: 0.002308, mean_absolute_error: 0.047039, mean_q: 0.326845\n",
      "  9017/50000: episode: 392, duration: 0.294s, episode steps: 24, steps per second: 82, episode reward: 1.300, mean reward: 0.054 [-1.000, 0.100], mean action: 0.120 [-3.049, 1.803], mean observation: 0.446 [-7.848, 4.000], loss: 0.002686, mean_absolute_error: 0.047647, mean_q: 0.315657\n",
      "  9038/50000: episode: 393, duration: 0.242s, episode steps: 21, steps per second: 87, episode reward: 1.000, mean reward: 0.048 [-1.000, 0.100], mean action: 0.444 [-2.454, 2.443], mean observation: 0.390 [-7.848, 4.000], loss: 0.002358, mean_absolute_error: 0.047659, mean_q: 0.328144\n",
      "  9060/50000: episode: 394, duration: 0.273s, episode steps: 22, steps per second: 81, episode reward: 1.100, mean reward: 0.050 [-1.000, 0.100], mean action: 0.202 [-2.633, 1.877], mean observation: 0.400 [-7.848, 4.000], loss: 0.004245, mean_absolute_error: 0.049912, mean_q: 0.345404\n",
      "  9079/50000: episode: 395, duration: 0.226s, episode steps: 19, steps per second: 84, episode reward: 0.800, mean reward: 0.042 [-1.000, 0.100], mean action: -0.031 [-3.794, 2.242], mean observation: 0.336 [-7.848, 4.000], loss: 0.003087, mean_absolute_error: 0.046563, mean_q: 0.287106\n",
      "  9093/50000: episode: 396, duration: 0.174s, episode steps: 14, steps per second: 81, episode reward: 0.300, mean reward: 0.021 [-1.000, 0.100], mean action: -2.053 [-5.578, 0.046], mean observation: -0.410 [-9.810, 4.000], loss: 0.002851, mean_absolute_error: 0.050951, mean_q: 0.358859\n",
      "  9138/50000: episode: 397, duration: 0.548s, episode steps: 45, steps per second: 82, episode reward: 3.400, mean reward: 0.076 [-1.000, 0.100], mean action: -0.580 [-5.797, 2.475], mean observation: 0.044 [-6.867, 4.000], loss: 0.002431, mean_absolute_error: 0.047962, mean_q: 0.335277\n",
      "  9162/50000: episode: 398, duration: 0.307s, episode steps: 24, steps per second: 78, episode reward: 1.300, mean reward: 0.054 [-1.000, 0.100], mean action: 0.317 [-3.956, 2.448], mean observation: 0.449 [-7.848, 4.000], loss: 0.004214, mean_absolute_error: 0.048723, mean_q: 0.332662\n",
      "  9202/50000: episode: 399, duration: 0.487s, episode steps: 40, steps per second: 82, episode reward: 2.900, mean reward: 0.073 [-1.000, 0.100], mean action: -0.196 [-4.012, 1.942], mean observation: -0.101 [-5.886, 4.000], loss: 0.002865, mean_absolute_error: 0.046262, mean_q: 0.330011\n",
      "  9215/50000: episode: 400, duration: 0.161s, episode steps: 13, steps per second: 81, episode reward: 0.200, mean reward: 0.015 [-1.000, 0.100], mean action: -1.583 [-4.915, 0.198], mean observation: -0.344 [-8.829, 4.000], loss: 0.002149, mean_absolute_error: 0.044429, mean_q: 0.293284\n",
      "  9258/50000: episode: 401, duration: 0.517s, episode steps: 43, steps per second: 83, episode reward: 3.200, mean reward: 0.074 [-1.000, 0.100], mean action: -0.347 [-4.779, 2.213], mean observation: -0.180 [-5.886, 4.000], loss: 0.002535, mean_absolute_error: 0.047196, mean_q: 0.325220\n",
      "  9279/50000: episode: 402, duration: 0.262s, episode steps: 21, steps per second: 80, episode reward: 1.000, mean reward: 0.048 [-1.000, 0.100], mean action: 0.109 [-3.018, 3.260], mean observation: 0.363 [-7.848, 4.000], loss: 0.003065, mean_absolute_error: 0.052255, mean_q: 0.323555\n",
      "  9300/50000: episode: 403, duration: 0.279s, episode steps: 21, steps per second: 75, episode reward: 1.000, mean reward: 0.048 [-1.000, 0.100], mean action: -2.260 [-8.079, 3.193], mean observation: 0.358 [-7.848, 4.000], loss: 0.001995, mean_absolute_error: 0.043857, mean_q: 0.330806\n",
      "  9341/50000: episode: 404, duration: 0.495s, episode steps: 41, steps per second: 83, episode reward: 3.000, mean reward: 0.073 [-1.000, 0.100], mean action: 0.134 [-2.287, 3.273], mean observation: 0.017 [-6.867, 4.000], loss: 0.002045, mean_absolute_error: 0.042878, mean_q: 0.371641\n",
      "  9373/50000: episode: 405, duration: 0.386s, episode steps: 32, steps per second: 83, episode reward: 2.100, mean reward: 0.066 [-1.000, 0.100], mean action: 0.289 [-2.747, 3.623], mean observation: 0.408 [-7.848, 4.000], loss: 0.002471, mean_absolute_error: 0.047003, mean_q: 0.345876\n",
      "  9392/50000: episode: 406, duration: 0.228s, episode steps: 19, steps per second: 83, episode reward: 0.800, mean reward: 0.042 [-1.000, 0.100], mean action: 0.664 [-3.272, 3.420], mean observation: 0.419 [-7.848, 4.000], loss: 0.002261, mean_absolute_error: 0.044768, mean_q: 0.332161\n",
      "  9414/50000: episode: 407, duration: 0.273s, episode steps: 22, steps per second: 80, episode reward: 1.100, mean reward: 0.050 [-1.000, 0.100], mean action: 0.312 [-3.211, 2.731], mean observation: 0.434 [-6.867, 4.000], loss: 0.002350, mean_absolute_error: 0.045359, mean_q: 0.350228\n",
      "  9431/50000: episode: 408, duration: 0.198s, episode steps: 17, steps per second: 86, episode reward: 0.600, mean reward: 0.035 [-1.000, 0.100], mean action: 0.128 [-2.448, 2.560], mean observation: 0.249 [-7.848, 4.000], loss: 0.002315, mean_absolute_error: 0.042245, mean_q: 0.366136\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  9469/50000: episode: 409, duration: 0.469s, episode steps: 38, steps per second: 81, episode reward: 2.700, mean reward: 0.071 [-1.000, 0.100], mean action: 0.021 [-2.514, 3.983], mean observation: -0.098 [-5.886, 4.000], loss: 0.004038, mean_absolute_error: 0.051644, mean_q: 0.347328\n",
      "  9519/50000: episode: 410, duration: 0.599s, episode steps: 50, steps per second: 83, episode reward: 3.900, mean reward: 0.078 [-1.000, 0.100], mean action: -0.524 [-4.633, 2.280], mean observation: 0.002 [-7.647, 4.000], loss: 0.002730, mean_absolute_error: 0.049133, mean_q: 0.319492\n",
      "  9543/50000: episode: 411, duration: 0.292s, episode steps: 24, steps per second: 82, episode reward: 1.300, mean reward: 0.054 [-1.000, 0.100], mean action: 0.543 [-2.937, 3.699], mean observation: 0.428 [-7.848, 4.000], loss: 0.002836, mean_absolute_error: 0.050087, mean_q: 0.314313\n",
      "  9568/50000: episode: 412, duration: 0.303s, episode steps: 25, steps per second: 83, episode reward: 1.400, mean reward: 0.056 [-1.000, 0.100], mean action: 0.689 [-2.266, 3.217], mean observation: 0.424 [-7.848, 4.000], loss: 0.002726, mean_absolute_error: 0.050308, mean_q: 0.343892\n",
      "  9593/50000: episode: 413, duration: 0.302s, episode steps: 25, steps per second: 83, episode reward: 1.400, mean reward: 0.056 [-1.000, 0.100], mean action: -2.664 [-9.440, 2.128], mean observation: 0.085 [-7.848, 4.000], loss: 0.002022, mean_absolute_error: 0.044305, mean_q: 0.373737\n",
      "  9642/50000: episode: 414, duration: 0.605s, episode steps: 49, steps per second: 81, episode reward: 3.800, mean reward: 0.078 [-1.000, 0.100], mean action: -0.523 [-5.262, 2.595], mean observation: 0.031 [-7.917, 4.000], loss: 0.002600, mean_absolute_error: 0.046196, mean_q: 0.368815\n",
      "  9682/50000: episode: 415, duration: 0.501s, episode steps: 40, steps per second: 80, episode reward: 2.900, mean reward: 0.073 [-1.000, 0.100], mean action: 0.109 [-2.098, 4.083], mean observation: -0.014 [-6.867, 4.000], loss: 0.003556, mean_absolute_error: 0.053459, mean_q: 0.388506\n",
      "  9714/50000: episode: 416, duration: 0.391s, episode steps: 32, steps per second: 82, episode reward: 2.100, mean reward: 0.066 [-1.000, 0.100], mean action: 0.294 [-4.160, 3.756], mean observation: 0.103 [-6.867, 4.000], loss: 0.002749, mean_absolute_error: 0.049266, mean_q: 0.373159\n",
      "  9741/50000: episode: 417, duration: 0.327s, episode steps: 27, steps per second: 83, episode reward: 1.600, mean reward: 0.059 [-1.000, 0.100], mean action: -0.022 [-4.204, 2.817], mean observation: 0.398 [-7.848, 4.000], loss: 0.002485, mean_absolute_error: 0.048429, mean_q: 0.360005\n",
      "  9765/50000: episode: 418, duration: 0.286s, episode steps: 24, steps per second: 84, episode reward: 1.300, mean reward: 0.054 [-1.000, 0.100], mean action: 0.221 [-2.843, 2.558], mean observation: 0.420 [-7.848, 4.000], loss: 0.003796, mean_absolute_error: 0.048759, mean_q: 0.348629\n",
      "  9807/50000: episode: 419, duration: 0.506s, episode steps: 42, steps per second: 83, episode reward: 3.100, mean reward: 0.074 [-1.000, 0.100], mean action: -1.315 [-6.951, 2.016], mean observation: 0.141 [-7.848, 4.000], loss: 0.002505, mean_absolute_error: 0.047242, mean_q: 0.328516\n",
      "  9846/50000: episode: 420, duration: 0.558s, episode steps: 39, steps per second: 70, episode reward: 2.800, mean reward: 0.072 [-1.000, 0.100], mean action: -0.610 [-4.837, 2.336], mean observation: -0.208 [-7.848, 4.000], loss: 0.003346, mean_absolute_error: 0.049948, mean_q: 0.348365\n",
      "  9886/50000: episode: 421, duration: 0.502s, episode steps: 40, steps per second: 80, episode reward: 2.900, mean reward: 0.073 [-1.000, 0.100], mean action: -0.146 [-3.171, 2.216], mean observation: -0.051 [-4.905, 4.000], loss: 0.002472, mean_absolute_error: 0.047254, mean_q: 0.365078\n",
      "  9928/50000: episode: 422, duration: 0.593s, episode steps: 42, steps per second: 71, episode reward: 3.100, mean reward: 0.074 [-1.000, 0.100], mean action: -0.772 [-5.256, 2.236], mean observation: 0.021 [-7.848, 4.000], loss: 0.004266, mean_absolute_error: 0.050563, mean_q: 0.360391\n",
      "  9952/50000: episode: 423, duration: 0.302s, episode steps: 24, steps per second: 79, episode reward: 1.300, mean reward: 0.054 [-1.000, 0.100], mean action: -0.245 [-4.776, 3.203], mean observation: -0.179 [-6.867, 4.000], loss: 0.003433, mean_absolute_error: 0.052589, mean_q: 0.367527\n",
      "  9976/50000: episode: 424, duration: 0.308s, episode steps: 24, steps per second: 78, episode reward: 1.300, mean reward: 0.054 [-1.000, 0.100], mean action: 0.386 [-3.787, 2.712], mean observation: 0.421 [-7.848, 4.000], loss: 0.002748, mean_absolute_error: 0.050737, mean_q: 0.321362\n",
      " 10022/50000: episode: 425, duration: 0.563s, episode steps: 46, steps per second: 82, episode reward: 3.500, mean reward: 0.076 [-1.000, 0.100], mean action: -0.010 [-3.231, 3.196], mean observation: 0.052 [-5.886, 4.000], loss: 0.002604, mean_absolute_error: 0.047532, mean_q: 0.386768\n",
      " 10067/50000: episode: 426, duration: 0.607s, episode steps: 45, steps per second: 74, episode reward: 3.400, mean reward: 0.076 [-1.000, 0.100], mean action: -0.795 [-6.513, 2.405], mean observation: 0.053 [-8.320, 4.000], loss: 0.003243, mean_absolute_error: 0.052652, mean_q: 0.378368\n",
      " 10093/50000: episode: 427, duration: 0.321s, episode steps: 26, steps per second: 81, episode reward: 1.500, mean reward: 0.058 [-1.000, 0.100], mean action: 0.207 [-3.212, 2.298], mean observation: 0.355 [-8.480, 4.000], loss: 0.002640, mean_absolute_error: 0.050254, mean_q: 0.364021\n",
      " 10135/50000: episode: 428, duration: 0.503s, episode steps: 42, steps per second: 83, episode reward: 3.100, mean reward: 0.074 [-1.000, 0.100], mean action: -0.048 [-2.570, 3.855], mean observation: 0.065 [-4.905, 4.000], loss: 0.002967, mean_absolute_error: 0.047305, mean_q: 0.377873\n",
      " 10166/50000: episode: 429, duration: 0.380s, episode steps: 31, steps per second: 82, episode reward: 2.000, mean reward: 0.065 [-1.000, 0.100], mean action: -1.514 [-5.787, 4.694], mean observation: -0.488 [-8.434, 4.000], loss: 0.003934, mean_absolute_error: 0.053134, mean_q: 0.373568\n",
      " 10208/50000: episode: 430, duration: 0.503s, episode steps: 42, steps per second: 84, episode reward: 3.100, mean reward: 0.074 [-1.000, 0.100], mean action: -0.790 [-4.391, 2.463], mean observation: -0.007 [-7.848, 4.000], loss: 0.002483, mean_absolute_error: 0.048359, mean_q: 0.413314\n",
      " 10248/50000: episode: 431, duration: 0.518s, episode steps: 40, steps per second: 77, episode reward: 2.900, mean reward: 0.073 [-1.000, 0.100], mean action: -0.484 [-4.191, 4.719], mean observation: 0.033 [-6.867, 4.000], loss: 0.002507, mean_absolute_error: 0.046570, mean_q: 0.393137\n",
      " 10291/50000: episode: 432, duration: 0.534s, episode steps: 43, steps per second: 81, episode reward: 3.200, mean reward: 0.074 [-1.000, 0.100], mean action: -0.222 [-3.594, 3.927], mean observation: 0.092 [-6.867, 4.000], loss: 0.004555, mean_absolute_error: 0.054745, mean_q: 0.408671\n",
      " 10316/50000: episode: 433, duration: 0.301s, episode steps: 25, steps per second: 83, episode reward: 1.400, mean reward: 0.056 [-1.000, 0.100], mean action: 0.001 [-3.795, 2.451], mean observation: -0.174 [-5.886, 4.000], loss: 0.002728, mean_absolute_error: 0.051908, mean_q: 0.376549\n",
      " 10353/50000: episode: 434, duration: 0.443s, episode steps: 37, steps per second: 84, episode reward: 2.600, mean reward: 0.070 [-1.000, 0.100], mean action: 0.106 [-2.796, 3.322], mean observation: -0.111 [-5.886, 4.000], loss: 0.003522, mean_absolute_error: 0.053374, mean_q: 0.409252\n",
      " 10388/50000: episode: 435, duration: 0.431s, episode steps: 35, steps per second: 81, episode reward: 2.400, mean reward: 0.069 [-1.000, 0.100], mean action: -0.592 [-4.945, 2.457], mean observation: -0.030 [-6.867, 4.000], loss: 0.003609, mean_absolute_error: 0.054960, mean_q: 0.404453\n",
      " 10430/50000: episode: 436, duration: 0.570s, episode steps: 42, steps per second: 74, episode reward: 3.100, mean reward: 0.074 [-1.000, 0.100], mean action: -0.263 [-3.365, 2.555], mean observation: 0.087 [-6.867, 4.000], loss: 0.003443, mean_absolute_error: 0.054088, mean_q: 0.374511\n",
      " 10472/50000: episode: 437, duration: 0.507s, episode steps: 42, steps per second: 83, episode reward: 3.100, mean reward: 0.074 [-1.000, 0.100], mean action: -0.010 [-2.347, 2.490], mean observation: -0.025 [-6.867, 4.000], loss: 0.003242, mean_absolute_error: 0.054492, mean_q: 0.379967\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 10516/50000: episode: 438, duration: 0.559s, episode steps: 44, steps per second: 79, episode reward: 3.300, mean reward: 0.075 [-1.000, 0.100], mean action: -0.406 [-5.425, 4.727], mean observation: -0.017 [-6.867, 4.000], loss: 0.003814, mean_absolute_error: 0.053369, mean_q: 0.413866\n",
      " 10531/50000: episode: 439, duration: 0.271s, episode steps: 15, steps per second: 55, episode reward: 0.400, mean reward: 0.027 [-1.000, 0.100], mean action: -0.086 [-2.122, 2.904], mean observation: -0.081 [-8.829, 4.000], loss: 0.003229, mean_absolute_error: 0.053396, mean_q: 0.384396\n",
      " 10565/50000: episode: 440, duration: 0.464s, episode steps: 34, steps per second: 73, episode reward: 2.300, mean reward: 0.068 [-1.000, 0.100], mean action: -0.790 [-5.171, 2.769], mean observation: -0.250 [-6.867, 4.000], loss: 0.002833, mean_absolute_error: 0.047977, mean_q: 0.416815\n",
      " 10589/50000: episode: 441, duration: 0.310s, episode steps: 24, steps per second: 77, episode reward: 1.300, mean reward: 0.054 [-1.000, 0.100], mean action: 0.496 [-1.910, 2.124], mean observation: 0.404 [-7.848, 4.000], loss: 0.003298, mean_absolute_error: 0.052307, mean_q: 0.389004\n",
      " 10628/50000: episode: 442, duration: 0.461s, episode steps: 39, steps per second: 85, episode reward: 2.800, mean reward: 0.072 [-1.000, 0.100], mean action: 0.192 [-3.923, 3.373], mean observation: -0.010 [-6.867, 4.000], loss: 0.003225, mean_absolute_error: 0.052179, mean_q: 0.410632\n",
      " 10668/50000: episode: 443, duration: 0.501s, episode steps: 40, steps per second: 80, episode reward: 2.900, mean reward: 0.073 [-1.000, 0.100], mean action: -0.008 [-2.849, 3.626], mean observation: -0.061 [-7.848, 4.000], loss: 0.002720, mean_absolute_error: 0.048045, mean_q: 0.418665\n",
      " 10692/50000: episode: 444, duration: 0.411s, episode steps: 24, steps per second: 58, episode reward: 1.300, mean reward: 0.054 [-1.000, 0.100], mean action: 0.283 [-2.273, 2.513], mean observation: -0.271 [-6.867, 4.000], loss: 0.003401, mean_absolute_error: 0.052663, mean_q: 0.415519\n",
      " 10720/50000: episode: 445, duration: 0.361s, episode steps: 28, steps per second: 78, episode reward: 1.700, mean reward: 0.061 [-1.000, 0.100], mean action: 0.009 [-6.197, 8.168], mean observation: -0.002 [-7.848, 4.000], loss: 0.003129, mean_absolute_error: 0.049612, mean_q: 0.435227\n",
      " 10760/50000: episode: 446, duration: 0.516s, episode steps: 40, steps per second: 78, episode reward: 2.900, mean reward: 0.073 [-1.000, 0.100], mean action: -0.423 [-4.138, 5.460], mean observation: -0.340 [-8.103, 4.000], loss: 0.004110, mean_absolute_error: 0.054863, mean_q: 0.416291\n",
      " 10807/50000: episode: 447, duration: 0.608s, episode steps: 47, steps per second: 77, episode reward: 3.600, mean reward: 0.077 [-1.000, 0.100], mean action: 0.151 [-3.243, 2.085], mean observation: 0.074 [-6.867, 4.000], loss: 0.003733, mean_absolute_error: 0.055130, mean_q: 0.402593\n",
      " 10831/50000: episode: 448, duration: 0.401s, episode steps: 24, steps per second: 60, episode reward: 1.300, mean reward: 0.054 [-1.000, 0.100], mean action: 0.448 [-1.726, 2.229], mean observation: 0.401 [-7.848, 4.000], loss: 0.004529, mean_absolute_error: 0.053914, mean_q: 0.412813\n",
      " 10871/50000: episode: 449, duration: 0.559s, episode steps: 40, steps per second: 72, episode reward: 2.900, mean reward: 0.073 [-1.000, 0.100], mean action: -0.020 [-4.558, 3.408], mean observation: 0.014 [-6.867, 4.000], loss: 0.003882, mean_absolute_error: 0.055514, mean_q: 0.420258\n",
      " 10899/50000: episode: 450, duration: 0.369s, episode steps: 28, steps per second: 76, episode reward: 1.700, mean reward: 0.061 [-1.000, 0.100], mean action: -0.442 [-4.279, 4.142], mean observation: -0.073 [-7.466, 4.000], loss: 0.003567, mean_absolute_error: 0.054742, mean_q: 0.416361\n",
      " 10954/50000: episode: 451, duration: 0.938s, episode steps: 55, steps per second: 59, episode reward: 4.400, mean reward: 0.080 [-1.000, 0.100], mean action: -0.052 [-3.036, 4.520], mean observation: 0.164 [-6.867, 4.000], loss: 0.003861, mean_absolute_error: 0.056484, mean_q: 0.411464\n",
      " 10996/50000: episode: 452, duration: 0.488s, episode steps: 42, steps per second: 86, episode reward: 3.100, mean reward: 0.074 [-1.000, 0.100], mean action: -0.092 [-3.236, 2.091], mean observation: 0.060 [-6.867, 4.000], loss: 0.003499, mean_absolute_error: 0.055477, mean_q: 0.453489\n",
      " 11034/50000: episode: 453, duration: 0.449s, episode steps: 38, steps per second: 85, episode reward: 2.700, mean reward: 0.071 [-1.000, 0.100], mean action: -0.134 [-3.057, 2.880], mean observation: -0.034 [-5.886, 4.000], loss: 0.003157, mean_absolute_error: 0.053003, mean_q: 0.407499\n",
      " 11073/50000: episode: 454, duration: 0.475s, episode steps: 39, steps per second: 82, episode reward: 2.800, mean reward: 0.072 [-1.000, 0.100], mean action: 0.021 [-3.002, 2.478], mean observation: -0.033 [-6.867, 4.000], loss: 0.003771, mean_absolute_error: 0.056925, mean_q: 0.424030\n",
      " 11104/50000: episode: 455, duration: 0.377s, episode steps: 31, steps per second: 82, episode reward: 2.000, mean reward: 0.065 [-1.000, 0.100], mean action: -0.573 [-3.949, 3.233], mean observation: -0.132 [-7.331, 4.000], loss: 0.003179, mean_absolute_error: 0.052442, mean_q: 0.464337\n",
      " 11155/50000: episode: 456, duration: 0.611s, episode steps: 51, steps per second: 83, episode reward: 4.000, mean reward: 0.078 [-1.000, 0.100], mean action: -0.485 [-3.752, 2.966], mean observation: 0.008 [-6.867, 4.000], loss: 0.003693, mean_absolute_error: 0.056272, mean_q: 0.430790\n",
      " 11179/50000: episode: 457, duration: 0.311s, episode steps: 24, steps per second: 77, episode reward: 1.300, mean reward: 0.054 [-1.000, 0.100], mean action: 0.467 [-2.809, 2.129], mean observation: 0.395 [-7.848, 4.000], loss: 0.003302, mean_absolute_error: 0.053170, mean_q: 0.415483\n",
      " 11224/50000: episode: 458, duration: 0.583s, episode steps: 45, steps per second: 77, episode reward: 3.400, mean reward: 0.076 [-1.000, 0.100], mean action: -0.413 [-4.227, 4.138], mean observation: -0.049 [-5.886, 4.000], loss: 0.003565, mean_absolute_error: 0.054960, mean_q: 0.448168\n",
      " 11265/50000: episode: 459, duration: 0.534s, episode steps: 41, steps per second: 77, episode reward: 3.000, mean reward: 0.073 [-1.000, 0.100], mean action: -0.303 [-4.043, 3.421], mean observation: -0.023 [-5.886, 4.000], loss: 0.003049, mean_absolute_error: 0.051257, mean_q: 0.487267\n",
      " 11304/50000: episode: 460, duration: 0.449s, episode steps: 39, steps per second: 87, episode reward: 2.800, mean reward: 0.072 [-1.000, 0.100], mean action: -0.088 [-3.372, 2.501], mean observation: -0.073 [-6.867, 4.000], loss: 0.003480, mean_absolute_error: 0.055585, mean_q: 0.478075\n",
      " 11326/50000: episode: 461, duration: 0.273s, episode steps: 22, steps per second: 81, episode reward: 1.100, mean reward: 0.050 [-1.000, 0.100], mean action: -0.211 [-3.703, 2.577], mean observation: -0.356 [-6.867, 4.000], loss: 0.004145, mean_absolute_error: 0.063392, mean_q: 0.424674\n",
      " 11382/50000: episode: 462, duration: 0.685s, episode steps: 56, steps per second: 82, episode reward: 4.500, mean reward: 0.080 [-1.000, 0.100], mean action: 0.004 [-3.435, 5.902], mean observation: 0.027 [-7.848, 4.000], loss: 0.003698, mean_absolute_error: 0.054966, mean_q: 0.469438\n",
      " 11401/50000: episode: 463, duration: 0.239s, episode steps: 19, steps per second: 80, episode reward: 0.800, mean reward: 0.042 [-1.000, 0.100], mean action: 0.814 [-2.493, 4.241], mean observation: 0.406 [-7.848, 4.000], loss: 0.004180, mean_absolute_error: 0.058077, mean_q: 0.456497\n",
      " 11448/50000: episode: 464, duration: 0.671s, episode steps: 47, steps per second: 70, episode reward: 3.600, mean reward: 0.077 [-1.000, 0.100], mean action: -0.663 [-4.745, 2.528], mean observation: -0.041 [-6.867, 4.000], loss: 0.005255, mean_absolute_error: 0.062301, mean_q: 0.450324\n",
      " 11490/50000: episode: 465, duration: 0.521s, episode steps: 42, steps per second: 81, episode reward: 3.100, mean reward: 0.074 [-1.000, 0.100], mean action: -0.137 [-4.024, 3.500], mean observation: 0.018 [-5.886, 4.000], loss: 0.003996, mean_absolute_error: 0.060577, mean_q: 0.436670\n",
      " 11513/50000: episode: 466, duration: 0.522s, episode steps: 23, steps per second: 44, episode reward: 1.200, mean reward: 0.052 [-1.000, 0.100], mean action: -0.603 [-3.054, 1.484], mean observation: -0.316 [-5.886, 4.000], loss: 0.004142, mean_absolute_error: 0.058262, mean_q: 0.450759\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 11573/50000: episode: 467, duration: 0.968s, episode steps: 60, steps per second: 62, episode reward: 4.900, mean reward: 0.082 [-1.000, 0.100], mean action: -0.047 [-3.615, 5.150], mean observation: -0.050 [-6.867, 4.000], loss: 0.003528, mean_absolute_error: 0.054517, mean_q: 0.449229\n",
      " 11618/50000: episode: 468, duration: 0.664s, episode steps: 45, steps per second: 68, episode reward: 3.400, mean reward: 0.076 [-1.000, 0.100], mean action: 0.015 [-3.739, 3.740], mean observation: -0.070 [-5.886, 4.000], loss: 0.004612, mean_absolute_error: 0.057369, mean_q: 0.465316\n",
      " 11644/50000: episode: 469, duration: 0.347s, episode steps: 26, steps per second: 75, episode reward: 1.500, mean reward: 0.058 [-1.000, 0.100], mean action: 0.374 [-3.181, 2.557], mean observation: 0.360 [-7.848, 4.000], loss: 0.003870, mean_absolute_error: 0.058295, mean_q: 0.467622\n",
      " 11691/50000: episode: 470, duration: 0.632s, episode steps: 47, steps per second: 74, episode reward: 3.600, mean reward: 0.077 [-1.000, 0.100], mean action: -0.032 [-4.068, 3.570], mean observation: 0.037 [-7.848, 4.000], loss: 0.003168, mean_absolute_error: 0.052898, mean_q: 0.462412\n",
      " 11739/50000: episode: 471, duration: 0.611s, episode steps: 48, steps per second: 79, episode reward: 3.700, mean reward: 0.077 [-1.000, 0.100], mean action: 0.078 [-3.209, 3.612], mean observation: 0.003 [-6.867, 4.000], loss: 0.003778, mean_absolute_error: 0.054642, mean_q: 0.483086\n",
      " 11766/50000: episode: 472, duration: 0.361s, episode steps: 27, steps per second: 75, episode reward: 1.600, mean reward: 0.059 [-1.000, 0.100], mean action: 0.574 [-3.063, 2.618], mean observation: 0.445 [-6.867, 4.000], loss: 0.003123, mean_absolute_error: 0.053383, mean_q: 0.504040\n",
      " 11790/50000: episode: 473, duration: 0.309s, episode steps: 24, steps per second: 78, episode reward: 1.300, mean reward: 0.054 [-1.000, 0.100], mean action: 0.573 [-2.235, 2.409], mean observation: 0.479 [-6.867, 4.000], loss: 0.006078, mean_absolute_error: 0.059471, mean_q: 0.472809\n",
      " 11812/50000: episode: 474, duration: 0.308s, episode steps: 22, steps per second: 71, episode reward: 1.100, mean reward: 0.050 [-1.000, 0.100], mean action: 0.519 [-2.865, 2.643], mean observation: 0.437 [-7.848, 4.000], loss: 0.003361, mean_absolute_error: 0.052873, mean_q: 0.476255\n",
      " 11858/50000: episode: 475, duration: 0.586s, episode steps: 46, steps per second: 78, episode reward: 3.500, mean reward: 0.076 [-1.000, 0.100], mean action: -0.600 [-7.585, 3.445], mean observation: -0.101 [-8.127, 4.000], loss: 0.003871, mean_absolute_error: 0.058964, mean_q: 0.504516\n",
      " 11904/50000: episode: 476, duration: 0.544s, episode steps: 46, steps per second: 85, episode reward: 3.500, mean reward: 0.076 [-1.000, 0.100], mean action: 0.049 [-4.199, 3.577], mean observation: 0.008 [-6.867, 4.000], loss: 0.003694, mean_absolute_error: 0.054398, mean_q: 0.513672\n",
      " 11964/50000: episode: 477, duration: 0.757s, episode steps: 60, steps per second: 79, episode reward: 4.900, mean reward: 0.082 [-1.000, 0.100], mean action: 0.061 [-3.228, 4.380], mean observation: 0.055 [-6.867, 4.000], loss: 0.003523, mean_absolute_error: 0.053717, mean_q: 0.497904\n",
      " 12022/50000: episode: 478, duration: 0.830s, episode steps: 58, steps per second: 70, episode reward: 4.700, mean reward: 0.081 [-1.000, 0.100], mean action: -0.136 [-4.097, 3.203], mean observation: 0.240 [-6.867, 4.000], loss: 0.003589, mean_absolute_error: 0.054478, mean_q: 0.511905\n",
      " 12043/50000: episode: 479, duration: 0.296s, episode steps: 21, steps per second: 71, episode reward: 1.000, mean reward: 0.048 [-1.000, 0.100], mean action: 0.661 [-2.795, 3.862], mean observation: 0.369 [-7.848, 4.000], loss: 0.003722, mean_absolute_error: 0.054410, mean_q: 0.523984\n",
      " 12068/50000: episode: 480, duration: 0.300s, episode steps: 25, steps per second: 83, episode reward: 1.400, mean reward: 0.056 [-1.000, 0.100], mean action: 0.146 [-2.557, 4.096], mean observation: -0.273 [-5.886, 4.000], loss: 0.003935, mean_absolute_error: 0.054494, mean_q: 0.501360\n",
      " 12109/50000: episode: 481, duration: 0.511s, episode steps: 41, steps per second: 80, episode reward: 3.000, mean reward: 0.073 [-1.000, 0.100], mean action: 0.293 [-2.402, 4.055], mean observation: -0.126 [-6.867, 4.000], loss: 0.003637, mean_absolute_error: 0.055031, mean_q: 0.484871\n",
      " 12171/50000: episode: 482, duration: 0.842s, episode steps: 62, steps per second: 74, episode reward: 5.100, mean reward: 0.082 [-1.000, 0.100], mean action: -0.065 [-5.272, 4.532], mean observation: 0.209 [-5.886, 4.000], loss: 0.003398, mean_absolute_error: 0.054673, mean_q: 0.514720\n",
      " 12215/50000: episode: 483, duration: 0.537s, episode steps: 44, steps per second: 82, episode reward: 3.300, mean reward: 0.075 [-1.000, 0.100], mean action: -0.009 [-6.006, 4.770], mean observation: 0.203 [-6.867, 4.000], loss: 0.003960, mean_absolute_error: 0.055693, mean_q: 0.557400\n",
      " 12236/50000: episode: 484, duration: 0.349s, episode steps: 21, steps per second: 60, episode reward: 1.000, mean reward: 0.048 [-1.000, 0.100], mean action: -0.091 [-2.267, 2.704], mean observation: -0.301 [-7.848, 4.000], loss: 0.003622, mean_absolute_error: 0.055737, mean_q: 0.516252\n",
      " 12252/50000: episode: 485, duration: 0.211s, episode steps: 16, steps per second: 76, episode reward: 0.500, mean reward: 0.031 [-1.000, 0.100], mean action: 0.483 [-3.456, 5.316], mean observation: 0.093 [-6.867, 4.000], loss: 0.004112, mean_absolute_error: 0.060882, mean_q: 0.556473\n",
      " 12276/50000: episode: 486, duration: 0.333s, episode steps: 24, steps per second: 72, episode reward: 1.300, mean reward: 0.054 [-1.000, 0.100], mean action: 0.251 [-4.883, 3.131], mean observation: 0.536 [-6.867, 4.000], loss: 0.005085, mean_absolute_error: 0.060432, mean_q: 0.484625\n",
      " 12296/50000: episode: 487, duration: 0.299s, episode steps: 20, steps per second: 67, episode reward: 0.900, mean reward: 0.045 [-1.000, 0.100], mean action: 0.371 [-3.387, 3.915], mean observation: 0.455 [-7.848, 4.000], loss: 0.004685, mean_absolute_error: 0.059758, mean_q: 0.477225\n",
      " 12323/50000: episode: 488, duration: 0.347s, episode steps: 27, steps per second: 78, episode reward: 1.600, mean reward: 0.059 [-1.000, 0.100], mean action: 0.541 [-3.710, 4.079], mean observation: 0.479 [-6.867, 4.000], loss: 0.004508, mean_absolute_error: 0.058065, mean_q: 0.508044\n",
      " 12351/50000: episode: 489, duration: 0.419s, episode steps: 28, steps per second: 67, episode reward: 1.700, mean reward: 0.061 [-1.000, 0.100], mean action: 0.018 [-9.308, 6.828], mean observation: 0.510 [-6.867, 4.000], loss: 0.003414, mean_absolute_error: 0.054755, mean_q: 0.572058\n",
      " 12376/50000: episode: 490, duration: 0.315s, episode steps: 25, steps per second: 79, episode reward: 1.400, mean reward: 0.056 [-1.000, 0.100], mean action: -0.420 [-2.961, 2.790], mean observation: -0.615 [-5.886, 4.000], loss: 0.002784, mean_absolute_error: 0.050259, mean_q: 0.520043\n",
      " 12429/50000: episode: 491, duration: 0.663s, episode steps: 53, steps per second: 80, episode reward: 4.200, mean reward: 0.079 [-1.000, 0.100], mean action: -0.587 [-4.822, 3.818], mean observation: -0.049 [-8.125, 4.000], loss: 0.004138, mean_absolute_error: 0.057103, mean_q: 0.523484\n",
      " 12452/50000: episode: 492, duration: 0.372s, episode steps: 23, steps per second: 62, episode reward: 1.200, mean reward: 0.052 [-1.000, 0.100], mean action: 0.672 [-3.419, 3.052], mean observation: 0.384 [-6.867, 4.000], loss: 0.005827, mean_absolute_error: 0.057402, mean_q: 0.541903\n",
      " 12493/50000: episode: 493, duration: 0.536s, episode steps: 41, steps per second: 77, episode reward: 3.000, mean reward: 0.073 [-1.000, 0.100], mean action: -0.081 [-3.565, 2.742], mean observation: -0.110 [-6.867, 4.000], loss: 0.005025, mean_absolute_error: 0.063721, mean_q: 0.534015\n",
      " 12509/50000: episode: 494, duration: 0.203s, episode steps: 16, steps per second: 79, episode reward: 0.500, mean reward: 0.031 [-1.000, 0.100], mean action: -0.222 [-3.433, 4.140], mean observation: 0.097 [-6.867, 4.000], loss: 0.004508, mean_absolute_error: 0.066470, mean_q: 0.550158\n",
      " 12551/50000: episode: 495, duration: 0.552s, episode steps: 42, steps per second: 76, episode reward: 3.100, mean reward: 0.074 [-1.000, 0.100], mean action: 0.377 [-5.296, 4.308], mean observation: 0.075 [-7.129, 4.000], loss: 0.004492, mean_absolute_error: 0.059786, mean_q: 0.518649\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 12630/50000: episode: 496, duration: 1.223s, episode steps: 79, steps per second: 65, episode reward: 6.800, mean reward: 0.086 [-1.000, 0.100], mean action: -0.298 [-4.499, 4.216], mean observation: -0.012 [-5.886, 4.000], loss: 0.003472, mean_absolute_error: 0.055519, mean_q: 0.536090\n",
      " 12668/50000: episode: 497, duration: 0.501s, episode steps: 38, steps per second: 76, episode reward: 2.700, mean reward: 0.071 [-1.000, 0.100], mean action: -0.142 [-3.510, 4.357], mean observation: -0.230 [-6.867, 4.000], loss: 0.003991, mean_absolute_error: 0.057848, mean_q: 0.541203\n",
      " 12708/50000: episode: 498, duration: 0.501s, episode steps: 40, steps per second: 80, episode reward: 2.900, mean reward: 0.073 [-1.000, 0.100], mean action: 0.235 [-3.811, 6.886], mean observation: -0.283 [-6.867, 4.000], loss: 0.004902, mean_absolute_error: 0.058685, mean_q: 0.582951\n",
      " 12731/50000: episode: 499, duration: 0.282s, episode steps: 23, steps per second: 82, episode reward: 1.200, mean reward: 0.052 [-1.000, 0.100], mean action: 0.554 [-1.738, 2.787], mean observation: -0.268 [-6.867, 4.000], loss: 0.005566, mean_absolute_error: 0.061254, mean_q: 0.522799\n",
      " 12771/50000: episode: 500, duration: 0.484s, episode steps: 40, steps per second: 83, episode reward: 2.900, mean reward: 0.073 [-1.000, 0.100], mean action: 0.064 [-3.567, 3.387], mean observation: -0.046 [-7.848, 4.000], loss: 0.003599, mean_absolute_error: 0.056348, mean_q: 0.596288\n",
      " 12802/50000: episode: 501, duration: 0.365s, episode steps: 31, steps per second: 85, episode reward: 2.000, mean reward: 0.065 [-1.000, 0.100], mean action: -0.058 [-3.216, 3.413], mean observation: -0.071 [-7.848, 4.000], loss: 0.004917, mean_absolute_error: 0.057638, mean_q: 0.569417\n",
      " 12843/50000: episode: 502, duration: 0.498s, episode steps: 41, steps per second: 82, episode reward: 3.000, mean reward: 0.073 [-1.000, 0.100], mean action: 0.375 [-4.303, 3.463], mean observation: 0.030 [-6.867, 4.000], loss: 0.003815, mean_absolute_error: 0.056825, mean_q: 0.549469\n",
      " 12887/50000: episode: 503, duration: 0.519s, episode steps: 44, steps per second: 85, episode reward: 3.300, mean reward: 0.075 [-1.000, 0.100], mean action: -0.120 [-5.661, 4.604], mean observation: -0.051 [-5.886, 4.000], loss: 0.005645, mean_absolute_error: 0.065705, mean_q: 0.546217\n",
      " 12927/50000: episode: 504, duration: 0.471s, episode steps: 40, steps per second: 85, episode reward: 2.900, mean reward: 0.073 [-1.000, 0.100], mean action: 0.533 [-4.010, 3.072], mean observation: 0.050 [-6.867, 4.000], loss: 0.005138, mean_absolute_error: 0.064038, mean_q: 0.602521\n",
      " 12949/50000: episode: 505, duration: 0.251s, episode steps: 22, steps per second: 88, episode reward: 1.100, mean reward: 0.050 [-1.000, 0.100], mean action: 0.774 [-3.011, 3.075], mean observation: -0.322 [-7.848, 4.000], loss: 0.004026, mean_absolute_error: 0.059815, mean_q: 0.611935\n",
      " 12972/50000: episode: 506, duration: 0.283s, episode steps: 23, steps per second: 81, episode reward: 1.200, mean reward: 0.052 [-1.000, 0.100], mean action: -0.042 [-4.217, 2.938], mean observation: -0.512 [-7.848, 4.000], loss: 0.005191, mean_absolute_error: 0.058732, mean_q: 0.527718\n",
      " 13026/50000: episode: 507, duration: 0.660s, episode steps: 54, steps per second: 82, episode reward: 4.300, mean reward: 0.080 [-1.000, 0.100], mean action: 0.053 [-2.486, 3.556], mean observation: -0.137 [-6.867, 4.000], loss: 0.004908, mean_absolute_error: 0.060635, mean_q: 0.579409\n",
      " 13054/50000: episode: 508, duration: 0.331s, episode steps: 28, steps per second: 85, episode reward: 1.700, mean reward: 0.061 [-1.000, 0.100], mean action: 0.619 [-3.696, 3.036], mean observation: -0.283 [-6.867, 4.000], loss: 0.004771, mean_absolute_error: 0.062892, mean_q: 0.549782\n",
      " 13099/50000: episode: 509, duration: 0.550s, episode steps: 45, steps per second: 82, episode reward: 3.400, mean reward: 0.076 [-1.000, 0.100], mean action: -0.052 [-4.192, 2.677], mean observation: -0.167 [-6.867, 4.000], loss: 0.004348, mean_absolute_error: 0.058136, mean_q: 0.592460\n",
      " 13122/50000: episode: 510, duration: 0.279s, episode steps: 23, steps per second: 82, episode reward: 1.200, mean reward: 0.052 [-1.000, 0.100], mean action: 1.153 [-1.496, 7.464], mean observation: -0.152 [-5.886, 4.000], loss: 0.003440, mean_absolute_error: 0.055030, mean_q: 0.583212\n",
      " 13142/50000: episode: 511, duration: 0.254s, episode steps: 20, steps per second: 79, episode reward: 0.900, mean reward: 0.045 [-1.000, 0.100], mean action: 0.902 [-2.420, 3.537], mean observation: -0.401 [-6.867, 4.000], loss: 0.004772, mean_absolute_error: 0.058830, mean_q: 0.563312\n",
      " 13182/50000: episode: 512, duration: 0.472s, episode steps: 40, steps per second: 85, episode reward: 2.900, mean reward: 0.073 [-1.000, 0.100], mean action: 0.512 [-3.028, 3.671], mean observation: -0.158 [-5.886, 4.000], loss: 0.003977, mean_absolute_error: 0.056227, mean_q: 0.600552\n",
      " 13205/50000: episode: 513, duration: 0.277s, episode steps: 23, steps per second: 83, episode reward: 1.200, mean reward: 0.052 [-1.000, 0.100], mean action: 0.690 [-2.720, 3.051], mean observation: 0.373 [-6.867, 4.000], loss: 0.003799, mean_absolute_error: 0.058012, mean_q: 0.568180\n",
      " 13255/50000: episode: 514, duration: 0.623s, episode steps: 50, steps per second: 80, episode reward: 3.900, mean reward: 0.078 [-1.000, 0.100], mean action: 0.040 [-2.938, 4.335], mean observation: 0.049 [-6.867, 4.000], loss: 0.005234, mean_absolute_error: 0.061805, mean_q: 0.591793\n",
      " 13288/50000: episode: 515, duration: 0.393s, episode steps: 33, steps per second: 84, episode reward: 2.200, mean reward: 0.067 [-1.000, 0.100], mean action: 0.371 [-6.447, 7.500], mean observation: 0.080 [-4.934, 4.000], loss: 0.005750, mean_absolute_error: 0.067305, mean_q: 0.561162\n",
      " 13328/50000: episode: 516, duration: 0.471s, episode steps: 40, steps per second: 85, episode reward: 2.900, mean reward: 0.073 [-1.000, 0.100], mean action: 0.360 [-3.083, 5.808], mean observation: -0.443 [-6.316, 4.000], loss: 0.004096, mean_absolute_error: 0.057704, mean_q: 0.581473\n",
      " 13368/50000: episode: 517, duration: 0.485s, episode steps: 40, steps per second: 82, episode reward: 2.900, mean reward: 0.073 [-1.000, 0.100], mean action: -0.129 [-5.264, 3.703], mean observation: -0.266 [-6.867, 4.000], loss: 0.004837, mean_absolute_error: 0.061112, mean_q: 0.607252\n",
      " 13390/50000: episode: 518, duration: 0.272s, episode steps: 22, steps per second: 81, episode reward: 1.100, mean reward: 0.050 [-1.000, 0.100], mean action: 0.327 [-2.158, 3.244], mean observation: -0.291 [-6.867, 4.000], loss: 0.003321, mean_absolute_error: 0.052628, mean_q: 0.540310\n",
      " 13443/50000: episode: 519, duration: 0.632s, episode steps: 53, steps per second: 84, episode reward: 4.200, mean reward: 0.079 [-1.000, 0.100], mean action: 0.162 [-4.374, 4.214], mean observation: -0.054 [-6.867, 4.000], loss: 0.005207, mean_absolute_error: 0.060102, mean_q: 0.592120\n",
      " 13493/50000: episode: 520, duration: 0.582s, episode steps: 50, steps per second: 86, episode reward: 3.900, mean reward: 0.078 [-1.000, 0.100], mean action: 0.105 [-2.996, 4.023], mean observation: -0.130 [-6.867, 4.000], loss: 0.005391, mean_absolute_error: 0.062821, mean_q: 0.599547\n",
      " 13510/50000: episode: 521, duration: 0.219s, episode steps: 17, steps per second: 77, episode reward: 0.600, mean reward: 0.035 [-1.000, 0.100], mean action: 0.011 [-7.156, 5.083], mean observation: -0.516 [-6.867, 4.000], loss: 0.004307, mean_absolute_error: 0.059222, mean_q: 0.662291\n",
      " 13542/50000: episode: 522, duration: 0.388s, episode steps: 32, steps per second: 83, episode reward: 2.100, mean reward: 0.066 [-1.000, 0.100], mean action: 0.525 [-2.982, 4.071], mean observation: 0.522 [-6.867, 4.000], loss: 0.003721, mean_absolute_error: 0.056982, mean_q: 0.603923\n",
      " 13584/50000: episode: 523, duration: 0.503s, episode steps: 42, steps per second: 84, episode reward: 3.100, mean reward: 0.074 [-1.000, 0.100], mean action: 0.047 [-4.262, 3.817], mean observation: 0.050 [-6.867, 4.000], loss: 0.003558, mean_absolute_error: 0.055843, mean_q: 0.611574\n",
      " 13619/50000: episode: 524, duration: 0.420s, episode steps: 35, steps per second: 83, episode reward: 2.400, mean reward: 0.069 [-1.000, 0.100], mean action: 0.005 [-4.132, 3.810], mean observation: -0.168 [-6.867, 4.000], loss: 0.004745, mean_absolute_error: 0.059233, mean_q: 0.617199\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 13636/50000: episode: 525, duration: 0.219s, episode steps: 17, steps per second: 77, episode reward: 0.600, mean reward: 0.035 [-1.000, 0.100], mean action: 0.263 [-3.380, 4.839], mean observation: -0.664 [-7.848, 4.000], loss: 0.005684, mean_absolute_error: 0.061124, mean_q: 0.615824\n",
      " 13658/50000: episode: 526, duration: 0.257s, episode steps: 22, steps per second: 86, episode reward: 1.100, mean reward: 0.050 [-1.000, 0.100], mean action: 0.004 [-4.218, 3.403], mean observation: -0.559 [-5.886, 4.000], loss: 0.004939, mean_absolute_error: 0.065661, mean_q: 0.562076\n",
      " 13755/50000: episode: 527, duration: 1.173s, episode steps: 97, steps per second: 83, episode reward: 8.600, mean reward: 0.089 [-1.000, 0.100], mean action: -0.184 [-6.617, 3.957], mean observation: -0.095 [-7.799, 4.000], loss: 0.005646, mean_absolute_error: 0.065017, mean_q: 0.605561\n",
      " 13858/50000: episode: 528, duration: 1.235s, episode steps: 103, steps per second: 83, episode reward: 9.200, mean reward: 0.089 [-1.000, 0.100], mean action: -0.474 [-8.034, 3.035], mean observation: 0.090 [-6.867, 4.000], loss: 0.004750, mean_absolute_error: 0.064898, mean_q: 0.611844\n",
      " 13896/50000: episode: 529, duration: 0.460s, episode steps: 38, steps per second: 83, episode reward: 2.700, mean reward: 0.071 [-1.000, 0.100], mean action: -0.672 [-6.923, 3.788], mean observation: -0.097 [-6.867, 4.000], loss: 0.006371, mean_absolute_error: 0.067848, mean_q: 0.656775\n",
      " 13946/50000: episode: 530, duration: 0.600s, episode steps: 50, steps per second: 83, episode reward: 3.900, mean reward: 0.078 [-1.000, 0.100], mean action: 0.208 [-4.528, 4.778], mean observation: -0.292 [-6.867, 4.000], loss: 0.006070, mean_absolute_error: 0.064174, mean_q: 0.639039\n",
      " 13984/50000: episode: 531, duration: 0.472s, episode steps: 38, steps per second: 81, episode reward: 2.700, mean reward: 0.071 [-1.000, 0.100], mean action: -0.587 [-5.952, 3.727], mean observation: -0.149 [-6.867, 4.000], loss: 0.005632, mean_absolute_error: 0.063709, mean_q: 0.640895\n",
      " 14079/50000: episode: 532, duration: 1.148s, episode steps: 95, steps per second: 83, episode reward: 8.400, mean reward: 0.088 [-1.000, 0.100], mean action: -0.250 [-4.443, 4.137], mean observation: -0.248 [-6.867, 4.000], loss: 0.004788, mean_absolute_error: 0.059604, mean_q: 0.666057\n",
      " 14151/50000: episode: 533, duration: 0.902s, episode steps: 72, steps per second: 80, episode reward: 6.100, mean reward: 0.085 [-1.000, 0.100], mean action: -0.146 [-6.295, 4.160], mean observation: -0.242 [-9.024, 4.000], loss: 0.004710, mean_absolute_error: 0.057947, mean_q: 0.676799\n",
      " 14291/50000: episode: 534, duration: 1.721s, episode steps: 140, steps per second: 81, episode reward: 12.900, mean reward: 0.092 [-1.000, 0.100], mean action: -0.600 [-8.989, 4.855], mean observation: -0.025 [-6.867, 4.000], loss: 0.004869, mean_absolute_error: 0.061196, mean_q: 0.669100\n",
      " 14358/50000: episode: 535, duration: 0.849s, episode steps: 67, steps per second: 79, episode reward: 5.600, mean reward: 0.084 [-1.000, 0.100], mean action: -0.710 [-6.251, 4.471], mean observation: -0.222 [-8.643, 4.000], loss: 0.004887, mean_absolute_error: 0.061164, mean_q: 0.686471\n",
      " 14721/50000: episode: 536, duration: 4.498s, episode steps: 363, steps per second: 81, episode reward: 35.200, mean reward: 0.097 [-1.000, 0.100], mean action: -0.280 [-7.848, 5.685], mean observation: 0.084 [-6.357, 4.000], loss: 0.005229, mean_absolute_error: 0.062611, mean_q: 0.693759\n",
      " 14750/50000: episode: 537, duration: 0.360s, episode steps: 29, steps per second: 80, episode reward: 1.800, mean reward: 0.062 [-1.000, 0.100], mean action: -0.829 [-6.492, 4.215], mean observation: -0.205 [-4.905, 4.000], loss: 0.005918, mean_absolute_error: 0.063668, mean_q: 0.716271\n",
      " 14807/50000: episode: 538, duration: 0.746s, episode steps: 57, steps per second: 76, episode reward: 4.600, mean reward: 0.081 [-1.000, 0.100], mean action: 0.261 [-4.973, 4.739], mean observation: 0.055 [-7.848, 4.000], loss: 0.005059, mean_absolute_error: 0.062722, mean_q: 0.697563\n",
      " 14857/50000: episode: 539, duration: 0.698s, episode steps: 50, steps per second: 72, episode reward: 3.900, mean reward: 0.078 [-1.000, 0.100], mean action: 0.210 [-6.913, 9.397], mean observation: -0.303 [-6.867, 4.000], loss: 0.005347, mean_absolute_error: 0.065159, mean_q: 0.743463\n",
      " 15145/50000: episode: 540, duration: 3.622s, episode steps: 288, steps per second: 80, episode reward: 27.700, mean reward: 0.096 [-1.000, 0.100], mean action: -0.340 [-9.475, 7.048], mean observation: -0.016 [-6.867, 4.000], loss: 0.005063, mean_absolute_error: 0.063442, mean_q: 0.726508\n",
      " 15521/50000: episode: 541, duration: 4.913s, episode steps: 376, steps per second: 77, episode reward: 36.500, mean reward: 0.097 [-1.000, 0.100], mean action: -0.494 [-9.032, 4.650], mean observation: 0.082 [-6.867, 4.000], loss: 0.005194, mean_absolute_error: 0.063088, mean_q: 0.764356\n",
      " 15703/50000: episode: 542, duration: 2.392s, episode steps: 182, steps per second: 76, episode reward: 17.100, mean reward: 0.094 [-1.000, 0.100], mean action: -0.304 [-5.499, 3.865], mean observation: 0.058 [-5.886, 4.000], loss: 0.005434, mean_absolute_error: 0.063241, mean_q: 0.786103\n",
      " 15738/50000: episode: 543, duration: 0.433s, episode steps: 35, steps per second: 81, episode reward: 2.400, mean reward: 0.069 [-1.000, 0.100], mean action: 0.342 [-4.250, 8.087], mean observation: -0.466 [-7.848, 4.000], loss: 0.005026, mean_absolute_error: 0.061037, mean_q: 0.774587\n",
      " 15768/50000: episode: 544, duration: 0.374s, episode steps: 30, steps per second: 80, episode reward: 1.900, mean reward: 0.063 [-1.000, 0.100], mean action: 0.344 [-3.544, 2.943], mean observation: 0.493 [-6.867, 4.000], loss: 0.004874, mean_absolute_error: 0.061505, mean_q: 0.823148\n",
      " 15957/50000: episode: 545, duration: 2.217s, episode steps: 189, steps per second: 85, episode reward: 17.800, mean reward: 0.094 [-1.000, 0.100], mean action: -0.428 [-6.620, 7.608], mean observation: 0.016 [-6.867, 4.000], loss: 0.004719, mean_absolute_error: 0.061836, mean_q: 0.807732\n",
      " 16115/50000: episode: 546, duration: 1.869s, episode steps: 158, steps per second: 85, episode reward: 14.700, mean reward: 0.093 [-1.000, 0.100], mean action: -0.313 [-7.358, 3.379], mean observation: 0.089 [-5.886, 4.000], loss: 0.004951, mean_absolute_error: 0.062117, mean_q: 0.841099\n",
      " 16161/50000: episode: 547, duration: 0.546s, episode steps: 46, steps per second: 84, episode reward: 3.500, mean reward: 0.076 [-1.000, 0.100], mean action: 0.180 [-3.405, 8.384], mean observation: -0.212 [-7.848, 4.000], loss: 0.005437, mean_absolute_error: 0.061883, mean_q: 0.834394\n",
      " 16225/50000: episode: 548, duration: 0.810s, episode steps: 64, steps per second: 79, episode reward: 5.300, mean reward: 0.083 [-1.000, 0.100], mean action: 0.111 [-6.745, 2.592], mean observation: 0.262 [-5.886, 4.000], loss: 0.005173, mean_absolute_error: 0.062247, mean_q: 0.813105\n",
      " 16283/50000: episode: 549, duration: 0.693s, episode steps: 58, steps per second: 84, episode reward: 4.700, mean reward: 0.081 [-1.000, 0.100], mean action: 0.044 [-5.909, 3.524], mean observation: 0.099 [-5.915, 4.000], loss: 0.005465, mean_absolute_error: 0.065367, mean_q: 0.862568\n",
      " 16711/50000: episode: 550, duration: 5.032s, episode steps: 428, steps per second: 85, episode reward: 41.700, mean reward: 0.097 [-1.000, 0.100], mean action: -0.337 [-7.155, 6.767], mean observation: 0.019 [-8.143, 4.000], loss: 0.005447, mean_absolute_error: 0.065442, mean_q: 0.873533\n",
      " 16737/50000: episode: 551, duration: 0.310s, episode steps: 26, steps per second: 84, episode reward: 1.500, mean reward: 0.058 [-1.000, 0.100], mean action: 0.024 [-7.262, 6.513], mean observation: -0.564 [-7.626, 4.000], loss: 0.005688, mean_absolute_error: 0.069462, mean_q: 0.902832\n",
      " 16928/50000: episode: 552, duration: 2.247s, episode steps: 191, steps per second: 85, episode reward: 18.000, mean reward: 0.094 [-1.000, 0.100], mean action: -0.285 [-4.677, 4.126], mean observation: 0.082 [-5.886, 4.000], loss: 0.004817, mean_absolute_error: 0.062528, mean_q: 0.896828\n",
      " 17139/50000: episode: 553, duration: 2.592s, episode steps: 211, steps per second: 81, episode reward: 20.000, mean reward: 0.095 [-1.000, 0.100], mean action: -0.834 [-9.320, 3.660], mean observation: 0.091 [-6.867, 4.000], loss: 0.005091, mean_absolute_error: 0.062627, mean_q: 0.897749\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 17207/50000: episode: 554, duration: 0.885s, episode steps: 68, steps per second: 77, episode reward: 5.700, mean reward: 0.084 [-1.000, 0.100], mean action: -1.541 [-11.890, 3.219], mean observation: 0.196 [-7.840, 4.000], loss: 0.004788, mean_absolute_error: 0.061731, mean_q: 0.939795\n",
      " 17411/50000: episode: 555, duration: 2.620s, episode steps: 204, steps per second: 78, episode reward: 19.300, mean reward: 0.095 [-1.000, 0.100], mean action: -0.618 [-8.501, 3.657], mean observation: 0.139 [-7.848, 4.000], loss: 0.005130, mean_absolute_error: 0.062289, mean_q: 0.935893\n",
      " 17433/50000: episode: 556, duration: 0.271s, episode steps: 22, steps per second: 81, episode reward: 1.100, mean reward: 0.050 [-1.000, 0.100], mean action: 0.431 [-4.175, 3.564], mean observation: 0.401 [-6.867, 4.000], loss: 0.006855, mean_absolute_error: 0.069155, mean_q: 0.970358\n",
      " 17543/50000: episode: 557, duration: 1.374s, episode steps: 110, steps per second: 80, episode reward: 9.900, mean reward: 0.090 [-1.000, 0.100], mean action: -0.918 [-12.452, 5.254], mean observation: 0.056 [-6.867, 4.000], loss: 0.004750, mean_absolute_error: 0.063003, mean_q: 0.956996\n",
      " 17595/50000: episode: 558, duration: 0.603s, episode steps: 52, steps per second: 86, episode reward: 4.100, mean reward: 0.079 [-1.000, 0.100], mean action: -1.432 [-9.964, 4.241], mean observation: -0.058 [-8.251, 4.000], loss: 0.004984, mean_absolute_error: 0.063715, mean_q: 0.989665\n",
      " 17651/50000: episode: 559, duration: 0.740s, episode steps: 56, steps per second: 76, episode reward: 4.500, mean reward: 0.080 [-1.000, 0.100], mean action: -0.114 [-5.845, 7.532], mean observation: -0.037 [-7.848, 4.000], loss: 0.006208, mean_absolute_error: 0.066911, mean_q: 0.968897\n",
      " 17674/50000: episode: 560, duration: 0.353s, episode steps: 23, steps per second: 65, episode reward: 1.200, mean reward: 0.052 [-1.000, 0.100], mean action: -1.155 [-13.008, 5.470], mean observation: 0.020 [-7.848, 4.000], loss: 0.005963, mean_absolute_error: 0.065533, mean_q: 0.983532\n",
      " 17763/50000: episode: 561, duration: 1.301s, episode steps: 89, steps per second: 68, episode reward: 7.800, mean reward: 0.088 [-1.000, 0.100], mean action: -0.840 [-12.000, 3.310], mean observation: 0.093 [-6.867, 4.000], loss: 0.005801, mean_absolute_error: 0.068822, mean_q: 0.943129\n",
      " 17868/50000: episode: 562, duration: 1.292s, episode steps: 105, steps per second: 81, episode reward: 9.400, mean reward: 0.090 [-1.000, 0.100], mean action: -0.626 [-7.906, 6.431], mean observation: -0.007 [-5.886, 4.000], loss: 0.005003, mean_absolute_error: 0.064971, mean_q: 0.983998\n",
      " 18104/50000: episode: 563, duration: 3.094s, episode steps: 236, steps per second: 76, episode reward: 22.500, mean reward: 0.095 [-1.000, 0.100], mean action: -0.239 [-6.173, 4.581], mean observation: 0.140 [-5.886, 4.000], loss: 0.005182, mean_absolute_error: 0.065245, mean_q: 0.997281\n",
      " 18197/50000: episode: 564, duration: 1.304s, episode steps: 93, steps per second: 71, episode reward: 8.200, mean reward: 0.088 [-1.000, 0.100], mean action: -0.694 [-11.381, 4.441], mean observation: 0.187 [-7.624, 4.000], loss: 0.005899, mean_absolute_error: 0.067415, mean_q: 1.004331\n",
      " 18247/50000: episode: 565, duration: 0.700s, episode steps: 50, steps per second: 71, episode reward: 3.900, mean reward: 0.078 [-1.000, 0.100], mean action: -1.457 [-8.591, 3.313], mean observation: 0.124 [-6.988, 4.000], loss: 0.005755, mean_absolute_error: 0.068664, mean_q: 0.971688\n",
      " 18417/50000: episode: 566, duration: 2.297s, episode steps: 170, steps per second: 74, episode reward: 15.900, mean reward: 0.094 [-1.000, 0.100], mean action: -0.219 [-7.062, 2.766], mean observation: 0.173 [-5.886, 4.000], loss: 0.005147, mean_absolute_error: 0.063584, mean_q: 1.007995\n",
      " 18504/50000: episode: 567, duration: 1.118s, episode steps: 87, steps per second: 78, episode reward: 7.600, mean reward: 0.087 [-1.000, 0.100], mean action: -0.492 [-6.081, 4.755], mean observation: -0.021 [-5.886, 4.000], loss: 0.006068, mean_absolute_error: 0.066534, mean_q: 1.038583\n",
      " 18807/50000: episode: 568, duration: 3.995s, episode steps: 303, steps per second: 76, episode reward: 29.200, mean reward: 0.096 [-1.000, 0.100], mean action: -0.265 [-4.793, 3.663], mean observation: 0.148 [-5.886, 4.000], loss: 0.006418, mean_absolute_error: 0.070744, mean_q: 1.039512\n",
      " 18868/50000: episode: 569, duration: 0.827s, episode steps: 61, steps per second: 74, episode reward: 5.000, mean reward: 0.082 [-1.000, 0.100], mean action: -1.041 [-8.709, 2.274], mean observation: 0.183 [-6.867, 4.000], loss: 0.005005, mean_absolute_error: 0.064320, mean_q: 1.040924\n",
      " 18885/50000: episode: 570, duration: 0.229s, episode steps: 17, steps per second: 74, episode reward: 0.600, mean reward: 0.035 [-1.000, 0.100], mean action: -1.246 [-5.906, 4.638], mean observation: -0.387 [-7.848, 4.000], loss: 0.003934, mean_absolute_error: 0.055578, mean_q: 1.049200\n",
      " 18915/50000: episode: 571, duration: 0.382s, episode steps: 30, steps per second: 79, episode reward: 1.900, mean reward: 0.063 [-1.000, 0.100], mean action: 0.663 [-7.488, 4.026], mean observation: 0.115 [-7.848, 4.000], loss: 0.005059, mean_absolute_error: 0.064159, mean_q: 1.057549\n",
      " 19148/50000: episode: 572, duration: 3.278s, episode steps: 233, steps per second: 71, episode reward: 22.200, mean reward: 0.095 [-1.000, 0.100], mean action: -0.421 [-8.688, 4.782], mean observation: 0.045 [-6.867, 4.000], loss: 0.006105, mean_absolute_error: 0.068541, mean_q: 1.078058\n",
      " 19550/50000: episode: 573, duration: 5.138s, episode steps: 402, steps per second: 78, episode reward: 39.100, mean reward: 0.097 [-1.000, 0.100], mean action: -0.446 [-9.801, 5.243], mean observation: 0.069 [-8.711, 4.000], loss: 0.006362, mean_absolute_error: 0.069408, mean_q: 1.089400\n",
      " 19572/50000: episode: 574, duration: 0.272s, episode steps: 22, steps per second: 81, episode reward: 1.100, mean reward: 0.050 [-1.000, 0.100], mean action: -0.747 [-8.770, 5.350], mean observation: -0.418 [-6.867, 4.000], loss: 0.006144, mean_absolute_error: 0.072068, mean_q: 1.104630\n",
      " 19606/50000: episode: 575, duration: 0.402s, episode steps: 34, steps per second: 85, episode reward: 2.300, mean reward: 0.068 [-1.000, 0.100], mean action: -0.012 [-4.505, 5.084], mean observation: 0.313 [-6.943, 4.000], loss: 0.006462, mean_absolute_error: 0.070164, mean_q: 1.095394\n",
      " 19659/50000: episode: 576, duration: 0.631s, episode steps: 53, steps per second: 84, episode reward: 4.200, mean reward: 0.079 [-1.000, 0.100], mean action: -1.038 [-8.762, 3.787], mean observation: 0.186 [-6.867, 4.000], loss: 0.006737, mean_absolute_error: 0.069938, mean_q: 1.104163\n",
      " 19674/50000: episode: 577, duration: 0.186s, episode steps: 15, steps per second: 81, episode reward: 0.400, mean reward: 0.027 [-1.000, 0.100], mean action: -1.499 [-6.078, 4.371], mean observation: -0.358 [-7.848, 4.000], loss: 0.005680, mean_absolute_error: 0.070471, mean_q: 1.088501\n",
      " 19757/50000: episode: 578, duration: 1.078s, episode steps: 83, steps per second: 77, episode reward: 7.200, mean reward: 0.087 [-1.000, 0.100], mean action: -0.594 [-10.196, 2.627], mean observation: 0.179 [-7.848, 4.000], loss: 0.005999, mean_absolute_error: 0.068054, mean_q: 1.109251\n",
      " 19884/50000: episode: 579, duration: 1.612s, episode steps: 127, steps per second: 79, episode reward: 11.600, mean reward: 0.091 [-1.000, 0.100], mean action: -0.416 [-10.578, 2.789], mean observation: 0.228 [-7.848, 4.000], loss: 0.006912, mean_absolute_error: 0.071901, mean_q: 1.118472\n",
      " 20271/50000: episode: 580, duration: 5.056s, episode steps: 387, steps per second: 77, episode reward: 37.600, mean reward: 0.097 [-1.000, 0.100], mean action: -0.483 [-10.485, 5.225], mean observation: 0.186 [-8.741, 4.000], loss: 0.005817, mean_absolute_error: 0.067029, mean_q: 1.138698\n",
      " 20422/50000: episode: 581, duration: 1.850s, episode steps: 151, steps per second: 82, episode reward: 14.000, mean reward: 0.093 [-1.000, 0.100], mean action: -0.632 [-8.063, 5.042], mean observation: 0.087 [-4.905, 4.000], loss: 0.007271, mean_absolute_error: 0.073541, mean_q: 1.148461\n",
      " 20773/50000: episode: 582, duration: 4.587s, episode steps: 351, steps per second: 77, episode reward: 34.000, mean reward: 0.097 [-1.000, 0.100], mean action: -0.494 [-8.059, 5.353], mean observation: 0.096 [-7.848, 4.000], loss: 0.006926, mean_absolute_error: 0.072443, mean_q: 1.185874\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 20867/50000: episode: 583, duration: 1.292s, episode steps: 94, steps per second: 73, episode reward: 8.300, mean reward: 0.088 [-1.000, 0.100], mean action: -0.714 [-10.355, 4.059], mean observation: 0.187 [-8.397, 4.000], loss: 0.007652, mean_absolute_error: 0.074607, mean_q: 1.212699\n",
      " 20980/50000: episode: 584, duration: 1.482s, episode steps: 113, steps per second: 76, episode reward: 10.200, mean reward: 0.090 [-1.000, 0.100], mean action: -1.098 [-10.217, 7.260], mean observation: -0.093 [-7.848, 4.000], loss: 0.006833, mean_absolute_error: 0.070672, mean_q: 1.220605\n",
      " 21080/50000: episode: 585, duration: 1.332s, episode steps: 100, steps per second: 75, episode reward: 8.900, mean reward: 0.089 [-1.000, 0.100], mean action: -1.372 [-9.992, 3.729], mean observation: 0.067 [-7.848, 4.000], loss: 0.006614, mean_absolute_error: 0.070518, mean_q: 1.234887\n",
      " 21115/50000: episode: 586, duration: 0.514s, episode steps: 35, steps per second: 68, episode reward: 2.400, mean reward: 0.069 [-1.000, 0.100], mean action: 0.012 [-9.341, 4.651], mean observation: -0.142 [-6.867, 4.000], loss: 0.007557, mean_absolute_error: 0.069314, mean_q: 1.192681\n",
      " 21586/50000: episode: 587, duration: 5.966s, episode steps: 471, steps per second: 79, episode reward: 46.000, mean reward: 0.098 [-1.000, 0.100], mean action: -0.625 [-11.292, 3.908], mean observation: 0.003 [-7.848, 4.000], loss: 0.007366, mean_absolute_error: 0.073248, mean_q: 1.242126\n",
      " 21684/50000: episode: 588, duration: 1.241s, episode steps: 98, steps per second: 79, episode reward: 8.700, mean reward: 0.089 [-1.000, 0.100], mean action: -0.337 [-7.985, 3.373], mean observation: 0.119 [-6.867, 4.000], loss: 0.007807, mean_absolute_error: 0.077438, mean_q: 1.261982\n",
      " 21824/50000: episode: 589, duration: 2.211s, episode steps: 140, steps per second: 63, episode reward: 12.900, mean reward: 0.092 [-1.000, 0.100], mean action: -0.768 [-12.706, 3.976], mean observation: -0.022 [-5.886, 4.000], loss: 0.007071, mean_absolute_error: 0.071490, mean_q: 1.282507\n",
      " 22433/50000: episode: 590, duration: 7.775s, episode steps: 609, steps per second: 78, episode reward: 59.800, mean reward: 0.098 [-1.000, 0.100], mean action: -0.548 [-11.865, 4.944], mean observation: 0.008 [-7.848, 4.000], loss: 0.007763, mean_absolute_error: 0.073981, mean_q: 1.309022\n",
      " 23434/50000: episode: 591, duration: 12.593s, episode steps: 1001, steps per second: 79, episode reward: 100.100, mean reward: 0.100 [0.100, 0.100], mean action: -0.423 [-9.909, 4.077], mean observation: 0.059 [-6.867, 4.000], loss: 0.007749, mean_absolute_error: 0.073838, mean_q: 1.379100\n",
      " 24362/50000: episode: 592, duration: 11.294s, episode steps: 928, steps per second: 82, episode reward: 91.700, mean reward: 0.099 [-1.000, 0.100], mean action: -0.611 [-10.282, 5.319], mean observation: 0.066 [-6.867, 4.000], loss: 0.008589, mean_absolute_error: 0.076586, mean_q: 1.457519\n",
      " 24394/50000: episode: 593, duration: 0.399s, episode steps: 32, steps per second: 80, episode reward: 2.100, mean reward: 0.066 [-1.000, 0.100], mean action: -0.520 [-8.726, 5.792], mean observation: -0.355 [-6.867, 4.000], loss: 0.010101, mean_absolute_error: 0.076120, mean_q: 1.505937\n",
      " 24767/50000: episode: 594, duration: 5.036s, episode steps: 373, steps per second: 74, episode reward: 36.200, mean reward: 0.097 [-1.000, 0.100], mean action: -0.476 [-10.622, 4.643], mean observation: 0.105 [-7.848, 4.000], loss: 0.009438, mean_absolute_error: 0.077615, mean_q: 1.539752\n",
      " 25052/50000: episode: 595, duration: 3.444s, episode steps: 285, steps per second: 83, episode reward: 27.400, mean reward: 0.096 [-1.000, 0.100], mean action: -0.342 [-8.026, 8.721], mean observation: 0.034 [-6.867, 4.000], loss: 0.009415, mean_absolute_error: 0.079384, mean_q: 1.569994\n",
      " 25068/50000: episode: 596, duration: 0.195s, episode steps: 16, steps per second: 82, episode reward: 0.500, mean reward: 0.031 [-1.000, 0.100], mean action: -0.120 [-6.333, 5.389], mean observation: -0.475 [-7.848, 4.000], loss: 0.007780, mean_absolute_error: 0.075688, mean_q: 1.544443\n",
      " 25414/50000: episode: 597, duration: 4.066s, episode steps: 346, steps per second: 85, episode reward: 33.500, mean reward: 0.097 [-1.000, 0.100], mean action: -0.444 [-8.534, 8.968], mean observation: 0.004 [-5.886, 4.000], loss: 0.009632, mean_absolute_error: 0.079667, mean_q: 1.596562\n",
      " 26415/50000: episode: 598, duration: 12.437s, episode steps: 1001, steps per second: 80, episode reward: 100.100, mean reward: 0.100 [0.100, 0.100], mean action: -0.228 [-9.974, 9.871], mean observation: 0.107 [-7.848, 4.000], loss: 0.009565, mean_absolute_error: 0.080029, mean_q: 1.651645\n",
      " 26435/50000: episode: 599, duration: 0.245s, episode steps: 20, steps per second: 82, episode reward: 0.900, mean reward: 0.045 [-1.000, 0.100], mean action: -0.891 [-6.258, 5.411], mean observation: -0.550 [-7.848, 4.000], loss: 0.008726, mean_absolute_error: 0.077218, mean_q: 1.616296\n",
      " 26467/50000: episode: 600, duration: 0.409s, episode steps: 32, steps per second: 78, episode reward: 2.100, mean reward: 0.066 [-1.000, 0.100], mean action: 0.450 [-9.305, 10.314], mean observation: -0.525 [-8.660, 4.000], loss: 0.009638, mean_absolute_error: 0.081879, mean_q: 1.670998\n",
      " 26567/50000: episode: 601, duration: 1.182s, episode steps: 100, steps per second: 85, episode reward: 8.900, mean reward: 0.089 [-1.000, 0.100], mean action: -1.089 [-9.198, 6.956], mean observation: -0.016 [-5.255, 4.000], loss: 0.009387, mean_absolute_error: 0.079641, mean_q: 1.734221\n",
      " 27265/50000: episode: 602, duration: 8.219s, episode steps: 698, steps per second: 85, episode reward: 68.700, mean reward: 0.098 [-1.000, 0.100], mean action: -0.094 [-12.756, 11.784], mean observation: 0.071 [-5.299, 4.000], loss: 0.009106, mean_absolute_error: 0.078689, mean_q: 1.758830\n",
      " 27293/50000: episode: 603, duration: 0.329s, episode steps: 28, steps per second: 85, episode reward: 1.700, mean reward: 0.061 [-1.000, 0.100], mean action: -0.265 [-7.028, 6.716], mean observation: -0.393 [-6.867, 4.000], loss: 0.010785, mean_absolute_error: 0.084706, mean_q: 1.785964\n",
      " 27307/50000: episode: 604, duration: 0.177s, episode steps: 14, steps per second: 79, episode reward: 0.300, mean reward: 0.021 [-1.000, 0.100], mean action: -0.427 [-8.583, 9.272], mean observation: -0.618 [-8.829, 4.000], loss: 0.011708, mean_absolute_error: 0.085380, mean_q: 1.829845\n",
      " 27922/50000: episode: 605, duration: 7.211s, episode steps: 615, steps per second: 85, episode reward: 60.400, mean reward: 0.098 [-1.000, 0.100], mean action: -0.060 [-14.077, 13.555], mean observation: 0.065 [-6.867, 4.000], loss: 0.009837, mean_absolute_error: 0.080471, mean_q: 1.837462\n",
      " 27942/50000: episode: 606, duration: 0.252s, episode steps: 20, steps per second: 79, episode reward: 0.900, mean reward: 0.045 [-1.000, 0.100], mean action: 0.089 [-5.703, 14.093], mean observation: -0.571 [-6.867, 4.000], loss: 0.012398, mean_absolute_error: 0.084979, mean_q: 1.830572\n",
      " 28150/50000: episode: 607, duration: 2.480s, episode steps: 208, steps per second: 84, episode reward: 19.700, mean reward: 0.095 [-1.000, 0.100], mean action: -0.262 [-12.419, 12.638], mean observation: 0.039 [-6.867, 4.000], loss: 0.011832, mean_absolute_error: 0.086993, mean_q: 1.877172\n",
      " 28174/50000: episode: 608, duration: 0.296s, episode steps: 24, steps per second: 81, episode reward: 1.300, mean reward: 0.054 [-1.000, 0.100], mean action: -0.949 [-9.280, 6.929], mean observation: -0.349 [-6.867, 4.000], loss: 0.012072, mean_absolute_error: 0.088558, mean_q: 1.930041\n",
      " 28199/50000: episode: 609, duration: 0.316s, episode steps: 25, steps per second: 79, episode reward: 1.400, mean reward: 0.056 [-1.000, 0.100], mean action: 1.286 [-5.041, 11.170], mean observation: -0.572 [-7.848, 4.000], loss: 0.009993, mean_absolute_error: 0.081414, mean_q: 1.885832\n",
      " 28268/50000: episode: 610, duration: 0.808s, episode steps: 69, steps per second: 85, episode reward: 5.800, mean reward: 0.084 [-1.000, 0.100], mean action: -0.140 [-11.285, 13.082], mean observation: -0.202 [-7.848, 4.000], loss: 0.009564, mean_absolute_error: 0.076465, mean_q: 1.890761\n",
      " 28295/50000: episode: 611, duration: 0.338s, episode steps: 27, steps per second: 80, episode reward: 1.600, mean reward: 0.059 [-1.000, 0.100], mean action: 0.671 [-6.007, 13.833], mean observation: -0.432 [-5.886, 4.000], loss: 0.009650, mean_absolute_error: 0.079635, mean_q: 1.946689\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 28318/50000: episode: 612, duration: 0.281s, episode steps: 23, steps per second: 82, episode reward: 1.200, mean reward: 0.052 [-1.000, 0.100], mean action: 1.276 [-4.464, 12.024], mean observation: -0.503 [-7.848, 4.000], loss: 0.010114, mean_absolute_error: 0.077654, mean_q: 1.926618\n",
      " 29191/50000: episode: 613, duration: 10.284s, episode steps: 873, steps per second: 85, episode reward: 86.200, mean reward: 0.099 [-1.000, 0.100], mean action: -0.503 [-17.255, 11.090], mean observation: 0.116 [-5.886, 4.000], loss: 0.010875, mean_absolute_error: 0.083014, mean_q: 1.952337\n",
      " 29212/50000: episode: 614, duration: 0.256s, episode steps: 21, steps per second: 82, episode reward: 1.000, mean reward: 0.048 [-1.000, 0.100], mean action: 0.035 [-6.421, 6.875], mean observation: -0.477 [-6.867, 4.000], loss: 0.008374, mean_absolute_error: 0.076653, mean_q: 1.967785\n",
      " 29237/50000: episode: 615, duration: 0.301s, episode steps: 25, steps per second: 83, episode reward: 1.400, mean reward: 0.056 [-1.000, 0.100], mean action: -0.249 [-6.291, 7.241], mean observation: -0.423 [-6.867, 4.000], loss: 0.012408, mean_absolute_error: 0.085409, mean_q: 1.942593\n",
      " 29596/50000: episode: 616, duration: 4.296s, episode steps: 359, steps per second: 84, episode reward: 34.800, mean reward: 0.097 [-1.000, 0.100], mean action: -0.411 [-15.349, 8.633], mean observation: 0.098 [-5.886, 4.000], loss: 0.010412, mean_absolute_error: 0.080595, mean_q: 2.018579\n",
      " 29739/50000: episode: 617, duration: 1.711s, episode steps: 143, steps per second: 84, episode reward: 13.200, mean reward: 0.092 [-1.000, 0.100], mean action: -1.079 [-19.185, 6.726], mean observation: 0.152 [-6.867, 4.000], loss: 0.012781, mean_absolute_error: 0.092064, mean_q: 2.009935\n",
      " 30019/50000: episode: 618, duration: 3.638s, episode steps: 280, steps per second: 77, episode reward: 26.900, mean reward: 0.096 [-1.000, 0.100], mean action: -0.828 [-12.890, 6.455], mean observation: 0.187 [-5.973, 4.000], loss: 0.011959, mean_absolute_error: 0.085934, mean_q: 2.050045\n",
      " 30101/50000: episode: 619, duration: 1.119s, episode steps: 82, steps per second: 73, episode reward: 7.100, mean reward: 0.087 [-1.000, 0.100], mean action: -1.607 [-14.108, 6.243], mean observation: 0.204 [-7.046, 4.000], loss: 0.010504, mean_absolute_error: 0.084002, mean_q: 2.085782\n",
      " 30214/50000: episode: 620, duration: 1.369s, episode steps: 113, steps per second: 83, episode reward: 10.200, mean reward: 0.090 [-1.000, 0.100], mean action: -1.425 [-17.940, 6.820], mean observation: 0.194 [-6.867, 4.000], loss: 0.010909, mean_absolute_error: 0.082009, mean_q: 2.107998\n",
      " 30287/50000: episode: 621, duration: 0.861s, episode steps: 73, steps per second: 85, episode reward: 6.200, mean reward: 0.085 [-1.000, 0.100], mean action: -1.419 [-17.502, 5.471], mean observation: 0.199 [-4.905, 4.000], loss: 0.012977, mean_absolute_error: 0.090525, mean_q: 2.077419\n",
      " 30311/50000: episode: 622, duration: 0.292s, episode steps: 24, steps per second: 82, episode reward: 1.300, mean reward: 0.054 [-1.000, 0.100], mean action: -3.012 [-18.127, 6.485], mean observation: 0.255 [-6.867, 4.000], loss: 0.015560, mean_absolute_error: 0.094194, mean_q: 1.982965\n",
      " 30335/50000: episode: 623, duration: 0.287s, episode steps: 24, steps per second: 84, episode reward: 1.300, mean reward: 0.054 [-1.000, 0.100], mean action: -2.754 [-16.934, 7.404], mean observation: 0.313 [-5.886, 4.000], loss: 0.015084, mean_absolute_error: 0.088896, mean_q: 2.091923\n",
      " 30364/50000: episode: 624, duration: 0.345s, episode steps: 29, steps per second: 84, episode reward: 1.800, mean reward: 0.062 [-1.000, 0.100], mean action: -3.371 [-17.584, 7.275], mean observation: 0.291 [-5.886, 4.000], loss: 0.010005, mean_absolute_error: 0.079327, mean_q: 2.125520\n",
      " 30506/50000: episode: 625, duration: 1.686s, episode steps: 142, steps per second: 84, episode reward: 13.100, mean reward: 0.092 [-1.000, 0.100], mean action: -0.569 [-16.609, 8.264], mean observation: 0.182 [-6.652, 4.000], loss: 0.012385, mean_absolute_error: 0.087191, mean_q: 2.081730\n",
      " 30720/50000: episode: 626, duration: 2.526s, episode steps: 214, steps per second: 85, episode reward: 20.300, mean reward: 0.095 [-1.000, 0.100], mean action: -0.928 [-16.728, 7.834], mean observation: 0.171 [-6.867, 4.000], loss: 0.011256, mean_absolute_error: 0.081507, mean_q: 2.119852\n",
      " 30949/50000: episode: 627, duration: 2.711s, episode steps: 229, steps per second: 84, episode reward: 21.800, mean reward: 0.095 [-1.000, 0.100], mean action: -0.116 [-14.350, 17.394], mean observation: 0.091 [-7.848, 4.000], loss: 0.011721, mean_absolute_error: 0.085539, mean_q: 2.118720\n",
      " 30978/50000: episode: 628, duration: 0.344s, episode steps: 29, steps per second: 84, episode reward: 1.800, mean reward: 0.062 [-1.000, 0.100], mean action: -1.091 [-9.887, 7.349], mean observation: -0.383 [-6.867, 4.000], loss: 0.012190, mean_absolute_error: 0.089617, mean_q: 2.133315\n",
      " 31979/50000: episode: 629, duration: 11.773s, episode steps: 1001, steps per second: 85, episode reward: 100.100, mean reward: 0.100 [0.100, 0.100], mean action: 0.056 [-11.325, 7.513], mean observation: 0.163 [-6.867, 4.000], loss: 0.012064, mean_absolute_error: 0.084550, mean_q: 2.194413\n",
      " 32060/50000: episode: 630, duration: 0.956s, episode steps: 81, steps per second: 85, episode reward: 7.000, mean reward: 0.086 [-1.000, 0.100], mean action: -1.332 [-70.809, 14.874], mean observation: 0.237 [-5.886, 4.000], loss: 0.015305, mean_absolute_error: 0.092395, mean_q: 2.270461\n",
      " 32123/50000: episode: 631, duration: 0.749s, episode steps: 63, steps per second: 84, episode reward: 5.200, mean reward: 0.083 [-1.000, 0.100], mean action: -16.301 [-274.865, 10.097], mean observation: 0.205 [-6.867, 4.000], loss: 0.014970, mean_absolute_error: 0.091394, mean_q: 2.311440\n",
      " 32155/50000: episode: 632, duration: 0.390s, episode steps: 32, steps per second: 82, episode reward: 2.100, mean reward: 0.066 [-1.000, 0.100], mean action: -13.333 [-103.608, 7.959], mean observation: 0.032 [-7.542, 4.000], loss: 0.021262, mean_absolute_error: 0.112315, mean_q: 2.275591\n",
      " 32175/50000: episode: 633, duration: 0.250s, episode steps: 20, steps per second: 80, episode reward: 0.900, mean reward: 0.045 [-1.000, 0.100], mean action: -47.664 [-276.376, 7.297], mean observation: 0.269 [-5.886, 4.000], loss: 0.028874, mean_absolute_error: 0.115379, mean_q: 2.286951\n",
      " 32209/50000: episode: 634, duration: 0.408s, episode steps: 34, steps per second: 83, episode reward: 2.300, mean reward: 0.068 [-1.000, 0.100], mean action: -24.934 [-352.060, 6.833], mean observation: -0.111 [-6.867, 4.000], loss: 0.026065, mean_absolute_error: 0.115624, mean_q: 2.273549\n",
      " 32223/50000: episode: 635, duration: 0.175s, episode steps: 14, steps per second: 80, episode reward: 0.300, mean reward: 0.021 [-1.000, 0.100], mean action: -50.154 [-173.224, 1.645], mean observation: -0.284 [-8.829, 4.000], loss: 0.024721, mean_absolute_error: 0.111722, mean_q: 2.292691\n",
      " 32251/50000: episode: 636, duration: 0.350s, episode steps: 28, steps per second: 80, episode reward: 1.700, mean reward: 0.061 [-1.000, 0.100], mean action: -24.380 [-196.380, 14.999], mean observation: -0.064 [-6.867, 4.000], loss: 0.039972, mean_absolute_error: 0.139090, mean_q: 2.255183\n",
      " 32271/50000: episode: 637, duration: 0.250s, episode steps: 20, steps per second: 80, episode reward: 0.900, mean reward: 0.045 [-1.000, 0.100], mean action: -73.352 [-289.183, 7.935], mean observation: 0.161 [-5.886, 4.000], loss: 0.037807, mean_absolute_error: 0.130583, mean_q: 2.271864\n",
      " 32330/50000: episode: 638, duration: 0.709s, episode steps: 59, steps per second: 83, episode reward: 4.800, mean reward: 0.081 [-1.000, 0.100], mean action: -27.593 [-339.345, 11.346], mean observation: 0.196 [-6.867, 4.000], loss: 0.050310, mean_absolute_error: 0.140723, mean_q: 2.296383\n",
      " 32368/50000: episode: 639, duration: 0.448s, episode steps: 38, steps per second: 85, episode reward: 2.700, mean reward: 0.071 [-1.000, 0.100], mean action: -30.329 [-218.698, 6.026], mean observation: 0.037 [-6.867, 4.000], loss: 0.044184, mean_absolute_error: 0.142912, mean_q: 2.330369\n",
      " 32417/50000: episode: 640, duration: 0.591s, episode steps: 49, steps per second: 83, episode reward: 3.800, mean reward: 0.078 [-1.000, 0.100], mean action: -27.910 [-271.123, 3.827], mean observation: 0.066 [-7.848, 4.000], loss: 0.046383, mean_absolute_error: 0.150719, mean_q: 2.273411\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 32523/50000: episode: 641, duration: 1.248s, episode steps: 106, steps per second: 85, episode reward: 9.500, mean reward: 0.090 [-1.000, 0.100], mean action: -0.318 [-10.423, 8.796], mean observation: 0.186 [-6.867, 4.000], loss: 0.067069, mean_absolute_error: 0.159861, mean_q: 2.273108\n",
      " 33360/50000: episode: 642, duration: 10.412s, episode steps: 837, steps per second: 80, episode reward: 82.600, mean reward: 0.099 [-1.000, 0.100], mean action: 0.013 [-9.574, 9.684], mean observation: 0.204 [-5.886, 4.000], loss: 0.040729, mean_absolute_error: 0.126971, mean_q: 2.323241\n",
      " 33401/50000: episode: 643, duration: 0.549s, episode steps: 41, steps per second: 75, episode reward: 3.000, mean reward: 0.073 [-1.000, 0.100], mean action: 1.488 [-8.198, 18.514], mean observation: 0.110 [-5.886, 4.000], loss: 0.034498, mean_absolute_error: 0.130672, mean_q: 2.367242\n",
      " 33467/50000: episode: 644, duration: 0.880s, episode steps: 66, steps per second: 75, episode reward: 5.500, mean reward: 0.083 [-1.000, 0.100], mean action: 0.042 [-8.303, 11.805], mean observation: 0.030 [-6.867, 4.000], loss: 0.025799, mean_absolute_error: 0.109576, mean_q: 2.396421\n",
      " 33511/50000: episode: 645, duration: 0.571s, episode steps: 44, steps per second: 77, episode reward: 3.300, mean reward: 0.075 [-1.000, 0.100], mean action: -0.549 [-9.701, 10.207], mean observation: 0.071 [-4.905, 4.000], loss: 0.037554, mean_absolute_error: 0.133046, mean_q: 2.389224\n",
      " 33567/50000: episode: 646, duration: 0.700s, episode steps: 56, steps per second: 80, episode reward: 4.500, mean reward: 0.080 [-1.000, 0.100], mean action: -1.519 [-15.769, 8.504], mean observation: -0.060 [-5.886, 4.000], loss: 0.027711, mean_absolute_error: 0.119863, mean_q: 2.372019\n",
      " 33620/50000: episode: 647, duration: 0.662s, episode steps: 53, steps per second: 80, episode reward: 4.200, mean reward: 0.079 [-1.000, 0.100], mean action: -1.293 [-15.335, 16.894], mean observation: -0.144 [-8.180, 4.000], loss: 0.030919, mean_absolute_error: 0.121835, mean_q: 2.380793\n",
      " 33716/50000: episode: 648, duration: 1.229s, episode steps: 96, steps per second: 78, episode reward: 8.500, mean reward: 0.089 [-1.000, 0.100], mean action: -0.437 [-12.565, 9.428], mean observation: 0.042 [-6.294, 4.000], loss: 0.037566, mean_absolute_error: 0.126795, mean_q: 2.387895\n",
      " 33765/50000: episode: 649, duration: 0.642s, episode steps: 49, steps per second: 76, episode reward: 3.800, mean reward: 0.078 [-1.000, 0.100], mean action: -1.111 [-13.708, 5.005], mean observation: -0.031 [-6.867, 4.000], loss: 0.031180, mean_absolute_error: 0.117274, mean_q: 2.404140\n",
      " 33806/50000: episode: 650, duration: 0.575s, episode steps: 41, steps per second: 71, episode reward: 3.000, mean reward: 0.073 [-1.000, 0.100], mean action: -0.770 [-9.311, 7.042], mean observation: -0.072 [-7.848, 4.000], loss: 0.035612, mean_absolute_error: 0.122861, mean_q: 2.414255\n",
      " 33924/50000: episode: 651, duration: 2.150s, episode steps: 118, steps per second: 55, episode reward: 10.700, mean reward: 0.091 [-1.000, 0.100], mean action: -0.430 [-16.939, 16.147], mean observation: 0.066 [-6.867, 4.000], loss: 0.032391, mean_absolute_error: 0.119892, mean_q: 2.403895\n",
      " 33992/50000: episode: 652, duration: 0.890s, episode steps: 68, steps per second: 76, episode reward: 5.700, mean reward: 0.084 [-1.000, 0.100], mean action: -2.175 [-23.839, 14.515], mean observation: -0.004 [-6.867, 4.000], loss: 0.032312, mean_absolute_error: 0.121376, mean_q: 2.405396\n",
      " 34077/50000: episode: 653, duration: 1.425s, episode steps: 85, steps per second: 60, episode reward: 7.400, mean reward: 0.087 [-1.000, 0.100], mean action: -0.871 [-25.785, 11.287], mean observation: 0.028 [-8.293, 4.000], loss: 0.027133, mean_absolute_error: 0.116577, mean_q: 2.445941\n",
      " 34097/50000: episode: 654, duration: 0.404s, episode steps: 20, steps per second: 49, episode reward: 0.900, mean reward: 0.045 [-1.000, 0.100], mean action: -2.641 [-25.158, 10.323], mean observation: -0.066 [-8.829, 4.000], loss: 0.037688, mean_absolute_error: 0.126092, mean_q: 2.419105\n",
      " 34160/50000: episode: 655, duration: 0.756s, episode steps: 63, steps per second: 83, episode reward: 5.200, mean reward: 0.083 [-1.000, 0.100], mean action: 0.169 [-13.894, 16.450], mean observation: 0.094 [-6.867, 4.000], loss: 0.029763, mean_absolute_error: 0.114073, mean_q: 2.480968\n",
      " 34185/50000: episode: 656, duration: 0.318s, episode steps: 25, steps per second: 79, episode reward: 1.400, mean reward: 0.056 [-1.000, 0.100], mean action: -1.118 [-18.615, 13.157], mean observation: -0.174 [-7.848, 4.000], loss: 0.033094, mean_absolute_error: 0.124578, mean_q: 2.460474\n",
      " 34206/50000: episode: 657, duration: 0.264s, episode steps: 21, steps per second: 79, episode reward: 1.000, mean reward: 0.048 [-1.000, 0.100], mean action: -4.334 [-28.075, 8.517], mean observation: 0.125 [-7.848, 4.000], loss: 0.032920, mean_absolute_error: 0.117015, mean_q: 2.447533\n",
      " 34223/50000: episode: 658, duration: 0.211s, episode steps: 17, steps per second: 81, episode reward: 0.600, mean reward: 0.035 [-1.000, 0.100], mean action: -6.903 [-36.336, 5.045], mean observation: -0.029 [-8.829, 4.000], loss: 0.035416, mean_absolute_error: 0.127164, mean_q: 2.445351\n",
      " 34341/50000: episode: 659, duration: 1.496s, episode steps: 118, steps per second: 79, episode reward: 10.700, mean reward: 0.091 [-1.000, 0.100], mean action: -5.463 [-164.216, 12.934], mean observation: 0.170 [-8.322, 4.000], loss: 0.033097, mean_absolute_error: 0.123397, mean_q: 2.446045\n",
      " 34361/50000: episode: 660, duration: 0.243s, episode steps: 20, steps per second: 82, episode reward: 0.900, mean reward: 0.045 [-1.000, 0.100], mean action: -3.859 [-25.537, 5.752], mean observation: 0.194 [-8.829, 4.000], loss: 0.038252, mean_absolute_error: 0.132099, mean_q: 2.464573\n",
      " 34388/50000: episode: 661, duration: 0.366s, episode steps: 27, steps per second: 74, episode reward: 1.600, mean reward: 0.059 [-1.000, 0.100], mean action: -2.076 [-49.023, 18.160], mean observation: 0.287 [-7.848, 4.000], loss: 0.026863, mean_absolute_error: 0.113698, mean_q: 2.414233\n",
      " 34477/50000: episode: 662, duration: 1.119s, episode steps: 89, steps per second: 80, episode reward: 7.800, mean reward: 0.088 [-1.000, 0.100], mean action: -0.780 [-46.545, 14.049], mean observation: 0.022 [-6.747, 4.000], loss: 0.026322, mean_absolute_error: 0.117480, mean_q: 2.459659\n",
      " 34506/50000: episode: 663, duration: 0.357s, episode steps: 29, steps per second: 81, episode reward: 1.800, mean reward: 0.062 [-1.000, 0.100], mean action: 1.877 [-12.180, 25.535], mean observation: -0.408 [-7.848, 4.000], loss: 0.032511, mean_absolute_error: 0.116321, mean_q: 2.441716\n",
      " 34542/50000: episode: 664, duration: 0.514s, episode steps: 36, steps per second: 70, episode reward: 2.500, mean reward: 0.069 [-1.000, 0.100], mean action: -0.375 [-14.606, 26.396], mean observation: -0.574 [-9.427, 4.000], loss: 0.025422, mean_absolute_error: 0.112548, mean_q: 2.473899\n",
      " 34573/50000: episode: 665, duration: 0.391s, episode steps: 31, steps per second: 79, episode reward: 2.000, mean reward: 0.065 [-1.000, 0.100], mean action: -2.558 [-18.550, 11.780], mean observation: -0.357 [-6.867, 4.000], loss: 0.020713, mean_absolute_error: 0.111257, mean_q: 2.516021\n",
      " 34713/50000: episode: 666, duration: 1.856s, episode steps: 140, steps per second: 75, episode reward: 12.900, mean reward: 0.092 [-1.000, 0.100], mean action: -1.324 [-18.257, 8.206], mean observation: -0.031 [-8.493, 4.000], loss: 0.026336, mean_absolute_error: 0.114725, mean_q: 2.473181\n",
      " 34741/50000: episode: 667, duration: 0.418s, episode steps: 28, steps per second: 67, episode reward: 1.700, mean reward: 0.061 [-1.000, 0.100], mean action: -3.494 [-26.972, 13.407], mean observation: 0.320 [-5.886, 4.000], loss: 0.027516, mean_absolute_error: 0.122752, mean_q: 2.464435\n",
      " 34763/50000: episode: 668, duration: 0.302s, episode steps: 22, steps per second: 73, episode reward: 1.100, mean reward: 0.050 [-1.000, 0.100], mean action: -1.245 [-15.250, 22.893], mean observation: -0.530 [-7.848, 4.000], loss: 0.036146, mean_absolute_error: 0.125551, mean_q: 2.482363\n",
      " 34895/50000: episode: 669, duration: 1.754s, episode steps: 132, steps per second: 75, episode reward: 12.100, mean reward: 0.092 [-1.000, 0.100], mean action: 0.070 [-18.451, 21.705], mean observation: 0.023 [-6.867, 4.000], loss: 0.023972, mean_absolute_error: 0.115819, mean_q: 2.479457\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 34918/50000: episode: 670, duration: 0.333s, episode steps: 23, steps per second: 69, episode reward: 1.200, mean reward: 0.052 [-1.000, 0.100], mean action: -6.483 [-35.002, 13.720], mean observation: 0.163 [-7.848, 4.000], loss: 0.023502, mean_absolute_error: 0.113261, mean_q: 2.485149\n",
      " 34994/50000: episode: 671, duration: 1.097s, episode steps: 76, steps per second: 69, episode reward: 6.500, mean reward: 0.086 [-1.000, 0.100], mean action: -0.929 [-18.343, 15.518], mean observation: -0.189 [-8.194, 4.000], loss: 0.031065, mean_absolute_error: 0.120190, mean_q: 2.476488\n",
      " 35056/50000: episode: 672, duration: 0.851s, episode steps: 62, steps per second: 73, episode reward: 5.100, mean reward: 0.082 [-1.000, 0.100], mean action: 0.368 [-13.429, 19.363], mean observation: -0.210 [-8.178, 4.000], loss: 0.033603, mean_absolute_error: 0.122716, mean_q: 2.509124\n",
      " 35085/50000: episode: 673, duration: 0.372s, episode steps: 29, steps per second: 78, episode reward: 1.800, mean reward: 0.062 [-1.000, 0.100], mean action: -18.727 [-131.984, 7.951], mean observation: 0.122 [-8.412, 4.000], loss: 0.019896, mean_absolute_error: 0.101602, mean_q: 2.562367\n",
      " 35224/50000: episode: 674, duration: 1.743s, episode steps: 139, steps per second: 80, episode reward: 12.800, mean reward: 0.092 [-1.000, 0.100], mean action: -1.763 [-79.315, 18.445], mean observation: 0.130 [-7.848, 4.000], loss: 0.025494, mean_absolute_error: 0.113864, mean_q: 2.505106\n",
      " 35327/50000: episode: 675, duration: 1.445s, episode steps: 103, steps per second: 71, episode reward: 9.200, mean reward: 0.089 [-1.000, 0.100], mean action: -3.376 [-81.681, 15.515], mean observation: 0.124 [-6.867, 4.000], loss: 0.028912, mean_absolute_error: 0.117245, mean_q: 2.529244\n",
      " 35383/50000: episode: 676, duration: 0.751s, episode steps: 56, steps per second: 75, episode reward: 4.500, mean reward: 0.080 [-1.000, 0.100], mean action: -11.964 [-200.915, 12.122], mean observation: 0.043 [-6.350, 4.000], loss: 0.026742, mean_absolute_error: 0.110216, mean_q: 2.519903\n",
      " 35402/50000: episode: 677, duration: 0.298s, episode steps: 19, steps per second: 64, episode reward: 0.800, mean reward: 0.042 [-1.000, 0.100], mean action: -16.298 [-98.160, 2.527], mean observation: -0.208 [-5.886, 4.000], loss: 0.037099, mean_absolute_error: 0.132446, mean_q: 2.479968\n",
      " 35434/50000: episode: 678, duration: 0.603s, episode steps: 32, steps per second: 53, episode reward: 2.100, mean reward: 0.066 [-1.000, 0.100], mean action: -15.021 [-142.135, 8.713], mean observation: -0.014 [-6.867, 4.000], loss: 0.026645, mean_absolute_error: 0.117982, mean_q: 2.550496\n",
      " 35524/50000: episode: 679, duration: 1.264s, episode steps: 90, steps per second: 71, episode reward: 7.900, mean reward: 0.088 [-1.000, 0.100], mean action: -7.466 [-195.884, 18.038], mean observation: 0.191 [-6.867, 4.000], loss: 0.033017, mean_absolute_error: 0.128274, mean_q: 2.503096\n",
      " 35629/50000: episode: 680, duration: 1.511s, episode steps: 105, steps per second: 69, episode reward: 9.400, mean reward: 0.090 [-1.000, 0.100], mean action: -10.652 [-245.197, 14.806], mean observation: 0.133 [-5.598, 4.000], loss: 0.027776, mean_absolute_error: 0.113429, mean_q: 2.592269\n",
      " 35660/50000: episode: 681, duration: 0.377s, episode steps: 31, steps per second: 82, episode reward: 2.000, mean reward: 0.065 [-1.000, 0.100], mean action: 3.963 [-30.227, 38.526], mean observation: -0.354 [-6.867, 4.000], loss: 0.039261, mean_absolute_error: 0.125571, mean_q: 2.598930\n",
      " 35708/50000: episode: 682, duration: 0.606s, episode steps: 48, steps per second: 79, episode reward: 3.700, mean reward: 0.077 [-1.000, 0.100], mean action: 2.180 [-14.440, 34.743], mean observation: -0.097 [-6.867, 4.000], loss: 0.027214, mean_absolute_error: 0.114193, mean_q: 2.582245\n",
      " 35804/50000: episode: 683, duration: 1.156s, episode steps: 96, steps per second: 83, episode reward: 8.500, mean reward: 0.089 [-1.000, 0.100], mean action: -4.098 [-96.104, 16.517], mean observation: 0.205 [-6.867, 4.000], loss: 0.035368, mean_absolute_error: 0.123134, mean_q: 2.568337\n",
      " 35841/50000: episode: 684, duration: 0.524s, episode steps: 37, steps per second: 71, episode reward: 2.600, mean reward: 0.070 [-1.000, 0.100], mean action: -6.068 [-82.500, 7.259], mean observation: 0.251 [-6.867, 4.000], loss: 0.036429, mean_absolute_error: 0.130224, mean_q: 2.597189\n",
      " 35890/50000: episode: 685, duration: 0.575s, episode steps: 49, steps per second: 85, episode reward: 3.800, mean reward: 0.078 [-1.000, 0.100], mean action: -3.265 [-85.769, 22.470], mean observation: 0.132 [-7.848, 4.000], loss: 0.040410, mean_absolute_error: 0.136661, mean_q: 2.583942\n",
      " 35962/50000: episode: 686, duration: 0.903s, episode steps: 72, steps per second: 80, episode reward: 6.100, mean reward: 0.085 [-1.000, 0.100], mean action: -1.547 [-23.482, 13.077], mean observation: -0.161 [-7.962, 4.000], loss: 0.031739, mean_absolute_error: 0.119408, mean_q: 2.573399\n",
      " 36027/50000: episode: 687, duration: 0.827s, episode steps: 65, steps per second: 79, episode reward: 5.400, mean reward: 0.083 [-1.000, 0.100], mean action: -4.673 [-83.071, 8.094], mean observation: 0.189 [-6.867, 4.000], loss: 0.040647, mean_absolute_error: 0.128350, mean_q: 2.596659\n",
      " 36098/50000: episode: 688, duration: 0.953s, episode steps: 71, steps per second: 75, episode reward: 6.000, mean reward: 0.085 [-1.000, 0.100], mean action: -7.647 [-122.068, 8.577], mean observation: 0.219 [-6.867, 4.000], loss: 0.035551, mean_absolute_error: 0.122181, mean_q: 2.636374\n",
      " 36190/50000: episode: 689, duration: 1.330s, episode steps: 92, steps per second: 69, episode reward: 8.100, mean reward: 0.088 [-1.000, 0.100], mean action: 0.631 [-29.078, 22.668], mean observation: 0.066 [-7.848, 4.000], loss: 0.039325, mean_absolute_error: 0.128143, mean_q: 2.626226\n",
      " 36211/50000: episode: 690, duration: 0.338s, episode steps: 21, steps per second: 62, episode reward: 1.000, mean reward: 0.048 [-1.000, 0.100], mean action: -1.212 [-32.111, 11.885], mean observation: 0.329 [-5.266, 4.000], loss: 0.037494, mean_absolute_error: 0.126660, mean_q: 2.618418\n",
      " 36229/50000: episode: 691, duration: 0.286s, episode steps: 18, steps per second: 63, episode reward: 0.700, mean reward: 0.039 [-1.000, 0.100], mean action: -9.208 [-44.555, 14.738], mean observation: 0.232 [-7.848, 4.000], loss: 0.036429, mean_absolute_error: 0.126582, mean_q: 2.586122\n",
      " 36281/50000: episode: 692, duration: 0.679s, episode steps: 52, steps per second: 77, episode reward: 4.100, mean reward: 0.079 [-1.000, 0.100], mean action: -1.098 [-24.562, 15.595], mean observation: 0.061 [-5.799, 4.000], loss: 0.040101, mean_absolute_error: 0.124570, mean_q: 2.646657\n",
      " 36302/50000: episode: 693, duration: 0.279s, episode steps: 21, steps per second: 75, episode reward: 1.000, mean reward: 0.048 [-1.000, 0.100], mean action: -6.774 [-38.198, 8.351], mean observation: 0.197 [-7.848, 4.000], loss: 0.033905, mean_absolute_error: 0.110136, mean_q: 2.691457\n",
      " 36329/50000: episode: 694, duration: 0.332s, episode steps: 27, steps per second: 81, episode reward: 1.600, mean reward: 0.059 [-1.000, 0.100], mean action: -2.359 [-24.294, 5.003], mean observation: 0.240 [-7.848, 4.000], loss: 0.027212, mean_absolute_error: 0.110947, mean_q: 2.666433\n",
      " 36357/50000: episode: 695, duration: 0.372s, episode steps: 28, steps per second: 75, episode reward: 1.700, mean reward: 0.061 [-1.000, 0.100], mean action: -11.239 [-59.674, 7.276], mean observation: 0.126 [-7.848, 4.000], loss: 0.040294, mean_absolute_error: 0.120888, mean_q: 2.639652\n",
      " 36379/50000: episode: 696, duration: 0.320s, episode steps: 22, steps per second: 69, episode reward: 1.100, mean reward: 0.050 [-1.000, 0.100], mean action: 1.316 [-13.478, 24.384], mean observation: -0.536 [-7.848, 4.000], loss: 0.033740, mean_absolute_error: 0.119517, mean_q: 2.560114\n",
      " 36407/50000: episode: 697, duration: 0.424s, episode steps: 28, steps per second: 66, episode reward: 1.700, mean reward: 0.061 [-1.000, 0.100], mean action: 2.845 [-13.346, 17.640], mean observation: -0.388 [-7.848, 4.000], loss: 0.026436, mean_absolute_error: 0.105318, mean_q: 2.710695\n",
      " 36457/50000: episode: 698, duration: 0.640s, episode steps: 50, steps per second: 78, episode reward: 3.900, mean reward: 0.078 [-1.000, 0.100], mean action: -0.635 [-50.047, 16.947], mean observation: 0.170 [-5.886, 4.000], loss: 0.049245, mean_absolute_error: 0.129144, mean_q: 2.643369\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 36496/50000: episode: 699, duration: 0.527s, episode steps: 39, steps per second: 74, episode reward: 2.800, mean reward: 0.072 [-1.000, 0.100], mean action: -12.154 [-134.172, 10.128], mean observation: 0.431 [-8.724, 4.000], loss: 0.034680, mean_absolute_error: 0.122147, mean_q: 2.631930\n",
      " 36523/50000: episode: 700, duration: 0.415s, episode steps: 27, steps per second: 65, episode reward: 1.600, mean reward: 0.059 [-1.000, 0.100], mean action: -17.717 [-92.413, 4.958], mean observation: 0.276 [-6.867, 4.000], loss: 0.030619, mean_absolute_error: 0.113758, mean_q: 2.692477\n",
      " 36621/50000: episode: 701, duration: 1.323s, episode steps: 98, steps per second: 74, episode reward: 8.700, mean reward: 0.089 [-1.000, 0.100], mean action: -5.296 [-105.164, 16.082], mean observation: 0.235 [-6.867, 4.000], loss: 0.042000, mean_absolute_error: 0.128913, mean_q: 2.641666\n",
      " 36672/50000: episode: 702, duration: 0.642s, episode steps: 51, steps per second: 79, episode reward: 4.000, mean reward: 0.078 [-1.000, 0.100], mean action: -9.599 [-125.714, 15.001], mean observation: 0.090 [-7.800, 4.000], loss: 0.035361, mean_absolute_error: 0.120952, mean_q: 2.679497\n",
      " 36702/50000: episode: 703, duration: 0.432s, episode steps: 30, steps per second: 69, episode reward: 1.900, mean reward: 0.063 [-1.000, 0.100], mean action: -10.972 [-71.773, 8.728], mean observation: 0.312 [-6.867, 4.000], loss: 0.051033, mean_absolute_error: 0.136478, mean_q: 2.627200\n",
      " 36725/50000: episode: 704, duration: 0.285s, episode steps: 23, steps per second: 81, episode reward: 1.200, mean reward: 0.052 [-1.000, 0.100], mean action: -15.672 [-85.723, 11.191], mean observation: 0.233 [-6.867, 4.000], loss: 0.039295, mean_absolute_error: 0.126764, mean_q: 2.662572\n",
      " 36864/50000: episode: 705, duration: 1.822s, episode steps: 139, steps per second: 76, episode reward: 12.800, mean reward: 0.092 [-1.000, 0.100], mean action: -3.822 [-124.714, 15.879], mean observation: 0.162 [-6.867, 4.000], loss: 0.031722, mean_absolute_error: 0.112929, mean_q: 2.683694\n",
      " 36893/50000: episode: 706, duration: 0.409s, episode steps: 29, steps per second: 71, episode reward: 1.800, mean reward: 0.062 [-1.000, 0.100], mean action: 1.727 [-12.322, 21.536], mean observation: -0.368 [-6.867, 4.000], loss: 0.041943, mean_absolute_error: 0.133010, mean_q: 2.646381\n",
      " 36917/50000: episode: 707, duration: 0.292s, episode steps: 24, steps per second: 82, episode reward: 1.300, mean reward: 0.054 [-1.000, 0.100], mean action: -7.794 [-65.925, 10.979], mean observation: 0.371 [-4.905, 4.000], loss: 0.036356, mean_absolute_error: 0.116747, mean_q: 2.710889\n",
      " 36938/50000: episode: 708, duration: 0.277s, episode steps: 21, steps per second: 76, episode reward: 1.000, mean reward: 0.048 [-1.000, 0.100], mean action: -5.450 [-69.982, 17.397], mean observation: 0.397 [-5.886, 4.000], loss: 0.043833, mean_absolute_error: 0.120605, mean_q: 2.686458\n",
      " 36996/50000: episode: 709, duration: 0.686s, episode steps: 58, steps per second: 85, episode reward: 4.700, mean reward: 0.081 [-1.000, 0.100], mean action: -10.302 [-113.676, 12.790], mean observation: 0.279 [-6.867, 4.000], loss: 0.036018, mean_absolute_error: 0.123560, mean_q: 2.657138\n",
      " 37022/50000: episode: 710, duration: 0.307s, episode steps: 26, steps per second: 85, episode reward: 1.500, mean reward: 0.058 [-1.000, 0.100], mean action: 1.933 [-10.193, 21.912], mean observation: -0.403 [-6.867, 4.000], loss: 0.035245, mean_absolute_error: 0.123167, mean_q: 2.696095\n",
      " 37043/50000: episode: 711, duration: 0.260s, episode steps: 21, steps per second: 81, episode reward: 1.000, mean reward: 0.048 [-1.000, 0.100], mean action: -11.952 [-55.768, 3.273], mean observation: -0.336 [-7.848, 4.000], loss: 0.026147, mean_absolute_error: 0.111236, mean_q: 2.691894\n",
      " 37076/50000: episode: 712, duration: 0.418s, episode steps: 33, steps per second: 79, episode reward: 2.200, mean reward: 0.067 [-1.000, 0.100], mean action: -4.264 [-67.098, 15.991], mean observation: 0.373 [-5.886, 4.000], loss: 0.028464, mean_absolute_error: 0.112298, mean_q: 2.685921\n",
      " 37101/50000: episode: 713, duration: 0.322s, episode steps: 25, steps per second: 78, episode reward: 1.400, mean reward: 0.056 [-1.000, 0.100], mean action: -5.894 [-54.262, 10.885], mean observation: 0.435 [-4.905, 4.000], loss: 0.032060, mean_absolute_error: 0.127726, mean_q: 2.673773\n",
      " 37124/50000: episode: 714, duration: 0.297s, episode steps: 23, steps per second: 78, episode reward: 1.200, mean reward: 0.052 [-1.000, 0.100], mean action: -7.364 [-51.172, 5.589], mean observation: 0.460 [-5.886, 4.000], loss: 0.030787, mean_absolute_error: 0.117457, mean_q: 2.700203\n",
      " 37201/50000: episode: 715, duration: 1.006s, episode steps: 77, steps per second: 77, episode reward: 6.600, mean reward: 0.086 [-1.000, 0.100], mean action: -6.764 [-103.590, 7.776], mean observation: 0.101 [-7.848, 4.000], loss: 0.021986, mean_absolute_error: 0.103911, mean_q: 2.688820\n",
      " 37226/50000: episode: 716, duration: 0.313s, episode steps: 25, steps per second: 80, episode reward: 1.400, mean reward: 0.056 [-1.000, 0.100], mean action: -4.245 [-38.536, 16.326], mean observation: 0.381 [-6.867, 4.000], loss: 0.035898, mean_absolute_error: 0.114977, mean_q: 2.744478\n",
      " 37250/50000: episode: 717, duration: 0.333s, episode steps: 24, steps per second: 72, episode reward: 1.300, mean reward: 0.054 [-1.000, 0.100], mean action: -18.831 [-72.980, 10.806], mean observation: 0.188 [-7.848, 4.000], loss: 0.024355, mean_absolute_error: 0.109335, mean_q: 2.686144\n",
      " 37307/50000: episode: 718, duration: 0.680s, episode steps: 57, steps per second: 84, episode reward: 4.600, mean reward: 0.081 [-1.000, 0.100], mean action: -4.666 [-73.154, 7.265], mean observation: 0.297 [-6.867, 4.000], loss: 0.029700, mean_absolute_error: 0.116037, mean_q: 2.681337\n",
      " 37488/50000: episode: 719, duration: 2.261s, episode steps: 181, steps per second: 80, episode reward: 17.000, mean reward: 0.094 [-1.000, 0.100], mean action: -4.161 [-133.454, 10.242], mean observation: 0.143 [-7.848, 4.000], loss: 0.023584, mean_absolute_error: 0.104474, mean_q: 2.710990\n",
      " 37581/50000: episode: 720, duration: 1.135s, episode steps: 93, steps per second: 82, episode reward: 8.200, mean reward: 0.088 [-1.000, 0.100], mean action: -6.566 [-110.193, 18.345], mean observation: 0.210 [-7.848, 4.000], loss: 0.031003, mean_absolute_error: 0.119388, mean_q: 2.722981\n",
      " 38035/50000: episode: 721, duration: 5.655s, episode steps: 454, steps per second: 80, episode reward: 44.300, mean reward: 0.098 [-1.000, 0.100], mean action: -0.623 [-72.695, 20.708], mean observation: 0.112 [-6.867, 4.000], loss: 0.029883, mean_absolute_error: 0.114428, mean_q: 2.753368\n",
      " 38060/50000: episode: 722, duration: 0.340s, episode steps: 25, steps per second: 74, episode reward: 1.400, mean reward: 0.056 [-1.000, 0.100], mean action: 3.176 [-11.173, 22.373], mean observation: -0.602 [-7.848, 4.000], loss: 0.028316, mean_absolute_error: 0.109709, mean_q: 2.714391\n",
      " 38102/50000: episode: 723, duration: 0.528s, episode steps: 42, steps per second: 79, episode reward: 3.100, mean reward: 0.074 [-1.000, 0.100], mean action: -3.171 [-79.886, 13.116], mean observation: 0.156 [-4.905, 4.000], loss: 0.021004, mean_absolute_error: 0.092020, mean_q: 2.803628\n",
      " 38127/50000: episode: 724, duration: 0.411s, episode steps: 25, steps per second: 61, episode reward: 1.400, mean reward: 0.056 [-1.000, 0.100], mean action: -7.503 [-74.188, 14.127], mean observation: 0.448 [-5.886, 4.000], loss: 0.023315, mean_absolute_error: 0.107263, mean_q: 2.800329\n",
      " 38163/50000: episode: 725, duration: 0.476s, episode steps: 36, steps per second: 76, episode reward: 2.500, mean reward: 0.069 [-1.000, 0.100], mean action: 2.573 [-49.749, 24.727], mean observation: -0.546 [-9.014, 4.000], loss: 0.033409, mean_absolute_error: 0.108798, mean_q: 2.795758\n",
      " 38180/50000: episode: 726, duration: 0.290s, episode steps: 17, steps per second: 59, episode reward: 0.600, mean reward: 0.035 [-1.000, 0.100], mean action: -10.122 [-44.708, 9.409], mean observation: -0.683 [-7.848, 4.000], loss: 0.031984, mean_absolute_error: 0.122599, mean_q: 2.805203\n",
      " 38206/50000: episode: 727, duration: 0.428s, episode steps: 26, steps per second: 61, episode reward: 1.500, mean reward: 0.058 [-1.000, 0.100], mean action: -24.516 [-109.338, 15.247], mean observation: -0.256 [-7.848, 4.000], loss: 0.033533, mean_absolute_error: 0.120212, mean_q: 2.725764\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 38260/50000: episode: 728, duration: 0.711s, episode steps: 54, steps per second: 76, episode reward: 4.300, mean reward: 0.080 [-1.000, 0.100], mean action: -8.291 [-89.423, 10.564], mean observation: 0.262 [-6.867, 4.000], loss: 0.028006, mean_absolute_error: 0.111408, mean_q: 2.787579\n",
      " 38287/50000: episode: 729, duration: 0.350s, episode steps: 27, steps per second: 77, episode reward: 1.600, mean reward: 0.059 [-1.000, 0.100], mean action: -0.109 [-35.068, 30.088], mean observation: -0.670 [-8.704, 4.000], loss: 0.033792, mean_absolute_error: 0.123873, mean_q: 2.736282\n",
      " 38308/50000: episode: 730, duration: 0.273s, episode steps: 21, steps per second: 77, episode reward: 1.000, mean reward: 0.048 [-1.000, 0.100], mean action: -7.347 [-55.597, 17.902], mean observation: -0.371 [-7.848, 4.000], loss: 0.024022, mean_absolute_error: 0.104003, mean_q: 2.883602\n",
      " 38332/50000: episode: 731, duration: 0.308s, episode steps: 24, steps per second: 78, episode reward: 1.300, mean reward: 0.054 [-1.000, 0.100], mean action: -14.752 [-133.619, 21.503], mean observation: -0.211 [-6.867, 4.000], loss: 0.035128, mean_absolute_error: 0.120055, mean_q: 2.758467\n",
      " 38358/50000: episode: 732, duration: 0.385s, episode steps: 26, steps per second: 68, episode reward: 1.500, mean reward: 0.058 [-1.000, 0.100], mean action: -6.163 [-93.400, 18.203], mean observation: 0.411 [-6.867, 4.000], loss: 0.028795, mean_absolute_error: 0.107129, mean_q: 2.853981\n",
      " 38387/50000: episode: 733, duration: 0.410s, episode steps: 29, steps per second: 71, episode reward: 1.800, mean reward: 0.062 [-1.000, 0.100], mean action: -6.133 [-86.017, 14.989], mean observation: 0.401 [-6.867, 4.000], loss: 0.042313, mean_absolute_error: 0.125300, mean_q: 2.755218\n",
      " 38420/50000: episode: 734, duration: 0.517s, episode steps: 33, steps per second: 64, episode reward: 2.200, mean reward: 0.067 [-1.000, 0.100], mean action: 3.799 [-16.865, 27.951], mean observation: -0.500 [-8.316, 4.000], loss: 0.022306, mean_absolute_error: 0.103521, mean_q: 2.817077\n",
      " 38444/50000: episode: 735, duration: 0.380s, episode steps: 24, steps per second: 63, episode reward: 1.300, mean reward: 0.054 [-1.000, 0.100], mean action: -6.901 [-53.145, 11.719], mean observation: -0.330 [-7.848, 4.000], loss: 0.030754, mean_absolute_error: 0.114838, mean_q: 2.815009\n",
      " 38613/50000: episode: 736, duration: 2.179s, episode steps: 169, steps per second: 78, episode reward: 15.800, mean reward: 0.093 [-1.000, 0.100], mean action: -2.939 [-145.308, 21.464], mean observation: 0.130 [-7.848, 4.000], loss: 0.033501, mean_absolute_error: 0.118209, mean_q: 2.788817\n",
      " 38658/50000: episode: 737, duration: 0.567s, episode steps: 45, steps per second: 79, episode reward: 3.400, mean reward: 0.076 [-1.000, 0.100], mean action: -9.314 [-141.542, 16.661], mean observation: 0.100 [-6.867, 4.000], loss: 0.039358, mean_absolute_error: 0.128087, mean_q: 2.806291\n",
      " 38703/50000: episode: 738, duration: 0.584s, episode steps: 45, steps per second: 77, episode reward: 3.400, mean reward: 0.076 [-1.000, 0.100], mean action: -8.171 [-122.471, 15.103], mean observation: 0.077 [-6.867, 4.000], loss: 0.026257, mean_absolute_error: 0.103969, mean_q: 2.824977\n",
      " 38746/50000: episode: 739, duration: 0.681s, episode steps: 43, steps per second: 63, episode reward: 3.200, mean reward: 0.074 [-1.000, 0.100], mean action: -5.401 [-85.281, 15.244], mean observation: 0.215 [-6.867, 4.000], loss: 0.032836, mean_absolute_error: 0.111169, mean_q: 2.842706\n",
      " 38771/50000: episode: 740, duration: 0.351s, episode steps: 25, steps per second: 71, episode reward: 1.400, mean reward: 0.056 [-1.000, 0.100], mean action: -13.737 [-147.992, 12.940], mean observation: 0.390 [-6.867, 4.000], loss: 0.033387, mean_absolute_error: 0.120668, mean_q: 2.805833\n",
      " 38890/50000: episode: 741, duration: 1.719s, episode steps: 119, steps per second: 69, episode reward: 10.800, mean reward: 0.091 [-1.000, 0.100], mean action: -1.539 [-77.499, 11.017], mean observation: 0.164 [-5.538, 4.000], loss: 0.033606, mean_absolute_error: 0.118771, mean_q: 2.822505\n",
      " 38942/50000: episode: 742, duration: 0.623s, episode steps: 52, steps per second: 84, episode reward: 4.100, mean reward: 0.079 [-1.000, 0.100], mean action: -9.413 [-159.323, 19.424], mean observation: 0.125 [-6.867, 4.000], loss: 0.030188, mean_absolute_error: 0.114837, mean_q: 2.808873\n",
      " 39016/50000: episode: 743, duration: 0.894s, episode steps: 74, steps per second: 83, episode reward: 6.300, mean reward: 0.085 [-1.000, 0.100], mean action: -4.778 [-102.458, 9.375], mean observation: 0.018 [-6.867, 4.000], loss: 0.034965, mean_absolute_error: 0.121577, mean_q: 2.825510\n",
      " 39041/50000: episode: 744, duration: 0.315s, episode steps: 25, steps per second: 79, episode reward: 1.400, mean reward: 0.056 [-1.000, 0.100], mean action: 2.873 [-11.787, 24.463], mean observation: -0.375 [-4.905, 4.000], loss: 0.036473, mean_absolute_error: 0.124075, mean_q: 2.800678\n",
      " 39158/50000: episode: 745, duration: 1.375s, episode steps: 117, steps per second: 85, episode reward: 10.600, mean reward: 0.091 [-1.000, 0.100], mean action: -1.095 [-97.088, 20.182], mean observation: 0.142 [-8.757, 4.000], loss: 0.039430, mean_absolute_error: 0.124525, mean_q: 2.824940\n",
      " 39191/50000: episode: 746, duration: 0.399s, episode steps: 33, steps per second: 83, episode reward: 2.200, mean reward: 0.067 [-1.000, 0.100], mean action: -11.921 [-112.773, 4.986], mean observation: 0.295 [-7.519, 4.000], loss: 0.039287, mean_absolute_error: 0.125727, mean_q: 2.838484\n",
      " 39243/50000: episode: 747, duration: 0.633s, episode steps: 52, steps per second: 82, episode reward: 4.100, mean reward: 0.079 [-1.000, 0.100], mean action: -8.895 [-158.663, 10.957], mean observation: 0.163 [-7.318, 4.000], loss: 0.038280, mean_absolute_error: 0.123163, mean_q: 2.846442\n",
      " 39291/50000: episode: 748, duration: 0.614s, episode steps: 48, steps per second: 78, episode reward: 3.700, mean reward: 0.077 [-1.000, 0.100], mean action: -12.986 [-96.520, 15.235], mean observation: -0.285 [-7.848, 4.000], loss: 0.032374, mean_absolute_error: 0.114666, mean_q: 2.867100\n",
      " 39346/50000: episode: 749, duration: 0.699s, episode steps: 55, steps per second: 79, episode reward: 4.400, mean reward: 0.080 [-1.000, 0.100], mean action: -3.835 [-36.030, 12.411], mean observation: -0.169 [-7.848, 4.000], loss: 0.028839, mean_absolute_error: 0.114138, mean_q: 2.830834\n",
      " 39375/50000: episode: 750, duration: 0.357s, episode steps: 29, steps per second: 81, episode reward: 1.800, mean reward: 0.062 [-1.000, 0.100], mean action: -19.650 [-113.787, 11.243], mean observation: -0.549 [-5.249, 4.000], loss: 0.033494, mean_absolute_error: 0.121847, mean_q: 2.874989\n",
      " 39450/50000: episode: 751, duration: 1.041s, episode steps: 75, steps per second: 72, episode reward: 6.400, mean reward: 0.085 [-1.000, 0.100], mean action: -6.724 [-121.456, 8.758], mean observation: 0.152 [-8.677, 4.000], loss: 0.031428, mean_absolute_error: 0.122018, mean_q: 2.862477\n",
      " 39493/50000: episode: 752, duration: 0.636s, episode steps: 43, steps per second: 68, episode reward: 3.200, mean reward: 0.074 [-1.000, 0.100], mean action: -8.266 [-100.412, 17.677], mean observation: -0.199 [-6.867, 4.000], loss: 0.034689, mean_absolute_error: 0.121034, mean_q: 2.856625\n",
      " 39519/50000: episode: 753, duration: 0.319s, episode steps: 26, steps per second: 82, episode reward: 1.500, mean reward: 0.058 [-1.000, 0.100], mean action: 2.306 [-23.622, 19.564], mean observation: -0.309 [-6.867, 4.000], loss: 0.024791, mean_absolute_error: 0.107667, mean_q: 2.880744\n",
      " 39550/50000: episode: 754, duration: 0.396s, episode steps: 31, steps per second: 78, episode reward: 2.000, mean reward: 0.065 [-1.000, 0.100], mean action: -6.705 [-104.620, 15.308], mean observation: 0.394 [-5.886, 4.000], loss: 0.036562, mean_absolute_error: 0.109118, mean_q: 2.920324\n",
      " 39617/50000: episode: 755, duration: 0.893s, episode steps: 67, steps per second: 75, episode reward: 5.600, mean reward: 0.084 [-1.000, 0.100], mean action: -0.779 [-58.816, 16.932], mean observation: 0.261 [-5.886, 4.000], loss: 0.032999, mean_absolute_error: 0.120266, mean_q: 2.889458\n",
      " 39652/50000: episode: 756, duration: 0.480s, episode steps: 35, steps per second: 73, episode reward: 2.400, mean reward: 0.069 [-1.000, 0.100], mean action: -5.038 [-73.797, 7.990], mean observation: 0.301 [-6.867, 4.000], loss: 0.030551, mean_absolute_error: 0.114716, mean_q: 2.876283\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 39674/50000: episode: 757, duration: 0.296s, episode steps: 22, steps per second: 74, episode reward: 1.100, mean reward: 0.050 [-1.000, 0.100], mean action: -8.887 [-90.316, 7.253], mean observation: 0.113 [-6.867, 4.000], loss: 0.028356, mean_absolute_error: 0.115115, mean_q: 2.835270\n",
      " 39701/50000: episode: 758, duration: 0.379s, episode steps: 27, steps per second: 71, episode reward: 1.600, mean reward: 0.059 [-1.000, 0.100], mean action: -7.283 [-80.736, 20.031], mean observation: 0.452 [-5.886, 4.000], loss: 0.021650, mean_absolute_error: 0.107723, mean_q: 2.895331\n",
      " 39756/50000: episode: 759, duration: 0.758s, episode steps: 55, steps per second: 73, episode reward: 4.400, mean reward: 0.080 [-1.000, 0.100], mean action: -7.801 [-154.423, 10.536], mean observation: -0.035 [-6.805, 4.000], loss: 0.038875, mean_absolute_error: 0.123527, mean_q: 2.901737\n",
      " 39789/50000: episode: 760, duration: 0.472s, episode steps: 33, steps per second: 70, episode reward: 2.200, mean reward: 0.067 [-1.000, 0.100], mean action: -3.007 [-83.818, 14.949], mean observation: 0.308 [-6.867, 4.000], loss: 0.039868, mean_absolute_error: 0.129302, mean_q: 2.858357\n",
      " 39906/50000: episode: 761, duration: 1.561s, episode steps: 117, steps per second: 75, episode reward: 10.600, mean reward: 0.091 [-1.000, 0.100], mean action: -1.985 [-162.533, 22.617], mean observation: 0.015 [-7.442, 4.000], loss: 0.036655, mean_absolute_error: 0.119792, mean_q: 2.902365\n",
      " 39969/50000: episode: 762, duration: 0.780s, episode steps: 63, steps per second: 81, episode reward: 5.200, mean reward: 0.083 [-1.000, 0.100], mean action: -4.082 [-114.372, 15.799], mean observation: 0.221 [-4.905, 4.000], loss: 0.029316, mean_absolute_error: 0.112588, mean_q: 2.906293\n",
      " 39998/50000: episode: 763, duration: 0.441s, episode steps: 29, steps per second: 66, episode reward: 1.800, mean reward: 0.062 [-1.000, 0.100], mean action: -13.093 [-150.850, 16.329], mean observation: -0.373 [-6.347, 4.000], loss: 0.024355, mean_absolute_error: 0.102539, mean_q: 2.955254\n",
      " 40051/50000: episode: 764, duration: 0.667s, episode steps: 53, steps per second: 79, episode reward: 4.200, mean reward: 0.079 [-1.000, 0.100], mean action: -5.607 [-79.952, 21.057], mean observation: 0.256 [-6.058, 4.000], loss: 0.032845, mean_absolute_error: 0.113022, mean_q: 2.906361\n",
      " 40168/50000: episode: 765, duration: 1.437s, episode steps: 117, steps per second: 81, episode reward: 10.600, mean reward: 0.091 [-1.000, 0.100], mean action: -2.542 [-132.047, 19.253], mean observation: -0.004 [-8.012, 4.000], loss: 0.030310, mean_absolute_error: 0.114926, mean_q: 2.931290\n",
      " 40193/50000: episode: 766, duration: 0.299s, episode steps: 25, steps per second: 84, episode reward: 1.400, mean reward: 0.056 [-1.000, 0.100], mean action: -16.076 [-117.393, 20.485], mean observation: -0.358 [-6.867, 4.000], loss: 0.031327, mean_absolute_error: 0.123081, mean_q: 2.938698\n",
      " 40231/50000: episode: 767, duration: 0.459s, episode steps: 38, steps per second: 83, episode reward: 2.700, mean reward: 0.071 [-1.000, 0.100], mean action: -8.931 [-173.993, 26.243], mean observation: -0.474 [-8.640, 4.000], loss: 0.032763, mean_absolute_error: 0.120080, mean_q: 2.924166\n",
      " 40308/50000: episode: 768, duration: 0.961s, episode steps: 77, steps per second: 80, episode reward: 6.600, mean reward: 0.086 [-1.000, 0.100], mean action: -1.979 [-84.002, 25.746], mean observation: 0.155 [-6.867, 4.000], loss: 0.033315, mean_absolute_error: 0.119216, mean_q: 2.894831\n",
      " 40335/50000: episode: 769, duration: 0.359s, episode steps: 27, steps per second: 75, episode reward: 1.600, mean reward: 0.059 [-1.000, 0.100], mean action: -12.793 [-114.218, 8.957], mean observation: 0.349 [-8.151, 4.000], loss: 0.027323, mean_absolute_error: 0.119128, mean_q: 2.874431\n",
      " 40487/50000: episode: 770, duration: 1.834s, episode steps: 152, steps per second: 83, episode reward: 14.100, mean reward: 0.093 [-1.000, 0.100], mean action: -3.309 [-137.604, 15.229], mean observation: 0.043 [-7.848, 4.000], loss: 0.031475, mean_absolute_error: 0.116104, mean_q: 2.956124\n",
      " 40510/50000: episode: 771, duration: 0.281s, episode steps: 23, steps per second: 82, episode reward: 1.200, mean reward: 0.052 [-1.000, 0.100], mean action: -14.907 [-72.494, 4.768], mean observation: -0.599 [-7.848, 4.000], loss: 0.034380, mean_absolute_error: 0.117849, mean_q: 2.962108\n",
      " 40558/50000: episode: 772, duration: 0.586s, episode steps: 48, steps per second: 82, episode reward: 3.700, mean reward: 0.077 [-1.000, 0.100], mean action: -5.230 [-101.976, 21.313], mean observation: 0.196 [-5.886, 4.000], loss: 0.032347, mean_absolute_error: 0.118442, mean_q: 2.953567\n",
      " 40589/50000: episode: 773, duration: 0.379s, episode steps: 31, steps per second: 82, episode reward: 2.000, mean reward: 0.065 [-1.000, 0.100], mean action: -8.809 [-106.654, 22.339], mean observation: -0.329 [-6.867, 4.000], loss: 0.035018, mean_absolute_error: 0.118383, mean_q: 2.969722\n",
      " 40613/50000: episode: 774, duration: 0.326s, episode steps: 24, steps per second: 74, episode reward: 1.300, mean reward: 0.054 [-1.000, 0.100], mean action: -14.223 [-109.176, 9.628], mean observation: -0.500 [-5.886, 4.000], loss: 0.029914, mean_absolute_error: 0.108640, mean_q: 2.992697\n",
      " 40640/50000: episode: 775, duration: 0.347s, episode steps: 27, steps per second: 78, episode reward: 1.600, mean reward: 0.059 [-1.000, 0.100], mean action: -20.595 [-170.997, 15.375], mean observation: -0.624 [-5.886, 4.000], loss: 0.033046, mean_absolute_error: 0.121942, mean_q: 2.934932\n",
      " 40658/50000: episode: 776, duration: 0.244s, episode steps: 18, steps per second: 74, episode reward: 0.700, mean reward: 0.039 [-1.000, 0.100], mean action: -32.353 [-228.370, 21.441], mean observation: -0.480 [-6.867, 4.000], loss: 0.027495, mean_absolute_error: 0.112944, mean_q: 2.977082\n",
      " 40793/50000: episode: 777, duration: 1.740s, episode steps: 135, steps per second: 78, episode reward: 12.400, mean reward: 0.092 [-1.000, 0.100], mean action: -3.289 [-96.750, 13.391], mean observation: 0.034 [-6.693, 4.000], loss: 0.034719, mean_absolute_error: 0.116307, mean_q: 2.957665\n",
      " 40853/50000: episode: 778, duration: 0.768s, episode steps: 60, steps per second: 78, episode reward: 4.900, mean reward: 0.082 [-1.000, 0.100], mean action: -5.134 [-117.021, 17.748], mean observation: 0.109 [-8.455, 4.000], loss: 0.041560, mean_absolute_error: 0.125770, mean_q: 2.951613\n",
      " 41127/50000: episode: 779, duration: 3.359s, episode steps: 274, steps per second: 82, episode reward: 26.300, mean reward: 0.096 [-1.000, 0.100], mean action: -0.830 [-72.597, 26.568], mean observation: 0.111 [-6.867, 4.000], loss: 0.030511, mean_absolute_error: 0.113646, mean_q: 2.994518\n",
      " 41192/50000: episode: 780, duration: 0.820s, episode steps: 65, steps per second: 79, episode reward: 5.400, mean reward: 0.083 [-1.000, 0.100], mean action: -1.913 [-53.723, 21.725], mean observation: -0.033 [-5.431, 4.000], loss: 0.023879, mean_absolute_error: 0.104493, mean_q: 3.032590\n",
      " 41255/50000: episode: 781, duration: 0.816s, episode steps: 63, steps per second: 77, episode reward: 5.200, mean reward: 0.083 [-1.000, 0.100], mean action: -2.007 [-46.060, 23.202], mean observation: -0.031 [-5.886, 4.000], loss: 0.040076, mean_absolute_error: 0.120438, mean_q: 3.019505\n",
      " 41283/50000: episode: 782, duration: 0.359s, episode steps: 28, steps per second: 78, episode reward: 1.700, mean reward: 0.061 [-1.000, 0.100], mean action: -16.037 [-123.671, 18.898], mean observation: -0.260 [-7.969, 4.000], loss: 0.028611, mean_absolute_error: 0.110620, mean_q: 3.021073\n",
      " 41465/50000: episode: 783, duration: 2.238s, episode steps: 182, steps per second: 81, episode reward: 17.100, mean reward: 0.094 [-1.000, 0.100], mean action: -1.045 [-73.763, 19.121], mean observation: 0.048 [-6.867, 4.000], loss: 0.030853, mean_absolute_error: 0.114405, mean_q: 3.026903\n",
      " 41788/50000: episode: 784, duration: 3.983s, episode steps: 323, steps per second: 81, episode reward: 31.200, mean reward: 0.097 [-1.000, 0.100], mean action: -0.751 [-20.628, 15.514], mean observation: 0.061 [-7.848, 4.000], loss: 0.031384, mean_absolute_error: 0.113630, mean_q: 3.036268\n",
      " 41907/50000: episode: 785, duration: 1.624s, episode steps: 119, steps per second: 73, episode reward: 10.800, mean reward: 0.091 [-1.000, 0.100], mean action: -0.684 [-16.478, 14.769], mean observation: 0.001 [-5.886, 4.000], loss: 0.033073, mean_absolute_error: 0.114520, mean_q: 3.058172\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 42627/50000: episode: 786, duration: 8.851s, episode steps: 720, steps per second: 81, episode reward: 70.900, mean reward: 0.098 [-1.000, 0.100], mean action: -0.637 [-90.445, 18.721], mean observation: 0.130 [-6.200, 4.000], loss: 0.027259, mean_absolute_error: 0.106750, mean_q: 3.088026\n",
      " 42673/50000: episode: 787, duration: 0.544s, episode steps: 46, steps per second: 85, episode reward: 3.500, mean reward: 0.076 [-1.000, 0.100], mean action: -6.647 [-112.164, 24.557], mean observation: 0.040 [-5.886, 4.000], loss: 0.026549, mean_absolute_error: 0.107299, mean_q: 3.061661\n",
      " 43043/50000: episode: 788, duration: 4.366s, episode steps: 370, steps per second: 85, episode reward: 35.900, mean reward: 0.097 [-1.000, 0.100], mean action: -0.744 [-98.096, 19.333], mean observation: 0.150 [-7.993, 4.000], loss: 0.023993, mean_absolute_error: 0.102078, mean_q: 3.111101\n",
      " 43072/50000: episode: 789, duration: 0.344s, episode steps: 29, steps per second: 84, episode reward: 1.800, mean reward: 0.062 [-1.000, 0.100], mean action: -5.406 [-86.074, 24.196], mean observation: -0.180 [-7.848, 4.000], loss: 0.042636, mean_absolute_error: 0.126081, mean_q: 3.078635\n",
      " 43100/50000: episode: 790, duration: 0.381s, episode steps: 28, steps per second: 73, episode reward: 1.700, mean reward: 0.061 [-1.000, 0.100], mean action: -1.319 [-25.928, 24.709], mean observation: 0.336 [-5.260, 4.000], loss: 0.022009, mean_absolute_error: 0.103289, mean_q: 3.145005\n",
      " 43656/50000: episode: 791, duration: 7.277s, episode steps: 556, steps per second: 76, episode reward: 54.500, mean reward: 0.098 [-1.000, 0.100], mean action: -0.122 [-41.929, 25.811], mean observation: 0.131 [-7.848, 4.000], loss: 0.023962, mean_absolute_error: 0.103671, mean_q: 3.140506\n",
      " 43734/50000: episode: 792, duration: 1.031s, episode steps: 78, steps per second: 76, episode reward: 6.700, mean reward: 0.086 [-1.000, 0.100], mean action: -0.267 [-29.754, 26.068], mean observation: -0.029 [-5.886, 4.000], loss: 0.021185, mean_absolute_error: 0.099909, mean_q: 3.144218\n",
      " 43768/50000: episode: 793, duration: 0.444s, episode steps: 34, steps per second: 77, episode reward: 2.300, mean reward: 0.068 [-1.000, 0.100], mean action: -3.558 [-67.370, 19.599], mean observation: -0.214 [-6.867, 4.000], loss: 0.024140, mean_absolute_error: 0.106391, mean_q: 3.108261\n",
      " 44769/50000: episode: 794, duration: 12.265s, episode steps: 1001, steps per second: 82, episode reward: 100.100, mean reward: 0.100 [0.100, 0.100], mean action: 0.083 [-16.163, 18.848], mean observation: 0.062 [-6.867, 4.000], loss: 0.022176, mean_absolute_error: 0.101417, mean_q: 3.197383\n",
      " 45276/50000: episode: 795, duration: 6.010s, episode steps: 507, steps per second: 84, episode reward: 49.600, mean reward: 0.098 [-1.000, 0.100], mean action: -0.319 [-30.281, 20.268], mean observation: 0.049 [-6.867, 4.000], loss: 0.023625, mean_absolute_error: 0.103001, mean_q: 3.233929\n",
      " 46277/50000: episode: 796, duration: 12.085s, episode steps: 1001, steps per second: 83, episode reward: 100.100, mean reward: 0.100 [0.100, 0.100], mean action: 0.011 [-14.088, 14.199], mean observation: 0.014 [-5.886, 4.000], loss: 0.019327, mean_absolute_error: 0.094917, mean_q: 3.295069\n",
      " 47278/50000: episode: 797, duration: 12.056s, episode steps: 1001, steps per second: 83, episode reward: 100.100, mean reward: 0.100 [0.100, 0.100], mean action: 0.143 [-21.174, 19.258], mean observation: -0.069 [-5.886, 4.000], loss: 0.018294, mean_absolute_error: 0.095928, mean_q: 3.351488\n",
      " 48279/50000: episode: 798, duration: 11.922s, episode steps: 1001, steps per second: 84, episode reward: 100.100, mean reward: 0.100 [0.100, 0.100], mean action: 0.344 [-15.001, 18.203], mean observation: -0.144 [-5.886, 4.000], loss: 0.020945, mean_absolute_error: 0.101928, mean_q: 3.434754\n",
      " 49280/50000: episode: 799, duration: 11.780s, episode steps: 1001, steps per second: 85, episode reward: 100.100, mean reward: 0.100 [0.100, 0.100], mean action: 0.171 [-26.258, 16.313], mean observation: -0.140 [-6.867, 4.000], loss: 0.021219, mean_absolute_error: 0.102701, mean_q: 3.508156\n",
      "done, took 650.495 seconds\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'ENV_NAME' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-65828be18e50>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# After training is done, we save the final weights.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ddpg_{}_weights.h5f'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mENV_NAME\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Finally, evaluate our algorithm for 5 episodes.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ENV_NAME' is not defined"
     ]
    }
   ],
   "source": [
    "agent.fit(env, nb_steps=50000, visualize=True, verbose=2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 5 episodes ...\n",
      "Episode 1: reward: 20.000, steps: 200\n",
      "Episode 2: reward: 20.000, steps: 200\n",
      "Episode 3: reward: 20.000, steps: 200\n",
      "Episode 4: reward: 20.000, steps: 200\n",
      "Episode 5: reward: 20.000, steps: 200\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x11de4d470>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Finally, evaluate our algorithm for 5 episodes.\n",
    "agent.test(env, nb_episodes=1000, visualize=True, nb_max_episode_steps=1001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After training is done, we save the final weights.\n",
    "ENV_NAME = '3DBall_128'\n",
    "agent.save_weights('ddpg_{}_vec_weights.h5f'.format(ENV_NAME), overwrite=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
